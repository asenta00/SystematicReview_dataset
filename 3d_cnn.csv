Publication Type,Authors,Article Title,Source Title,Publication Year,Author Keywords,Abstract,WoS Categories,Research Areas,Open Access,Satellite,Application,Domain,Title_lower,ML Technique,Complex,Accuracy (%),F1-score (%),Precision (%),Recall (%),Producer accuracy (%),User accuracy (%),Pixel accuracy,Kappa (%),IoU (%),MIoU (%),Dice (%),R-squared,RMSE,MAE,Parameter
J,"Jiang, H; Lu, N",Multi-Scale Residual Convolutional Neural Network for Haze Removal of Remote Sensing Images,REMOTE SENSING,2018,haze removal; multi-scale context aggregation; residual learning; convolutional neural network; Landsat 8 OLI,"Haze removal is a pre-processing step that operates on at-sensor radiance data prior to the physically based image correction step to enhance hazy imagery visually. Most current haze removal methods focus on point-to-point operations and utilize information in the spectral domain, without taking consideration of the multi-scale spatial information of haze. In this paper, we propose a multi-scale residual convolutional neural network (MRCNN) for haze removal of remote sensing images. MRCNN utilizes 3D convolutional kernels to extract spatial-spectral correlation information and abstract features from surrounding neighborhoods for haze transmission estimation. It takes advantage of dilated convolution to aggregate multi-scale contextual information for the purpose of improving its prediction accuracy. Meanwhile, residual learning is utilized to avoid the loss of weak information while deepening the network. Our experiments indicate that MRCNN performs accurately, achieving an extremely low validation error and testing error. The haze removal results of several scenes of Landsat 8 Operational Land Imager (OLI) data show that the visibility of the dehazed images is significantly improved, and the color of recovered surface is consistent with the actual scene. Quantitative analysis proves that the dehazed results of MRCNN are superior to the traditional methods and other networks. Additionally, a comparison to haze-free data illustrates the spectral consistency after haze removal and reveals the changes in the vegetation index.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Landsat-8,Haze Removal,Others,multi-scale residual convolutional neural network for haze removal of remote sensing images,Image enhancement,yes,,,,,,,,,,,,,,,
J,"Xu, ZW; Guan, KY; Casler, N; Peng, B; Wang, SW",A 3D convolutional neural network method for land cover classification using LiDAR and multi-temporal Landsat imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2018,Big data analysis; Convolutional neural network; Land cover classification; LiDAR; Multi-temporal Landsat imagery,"Terrestrial landscape has complex three-dimensional (3D) features that are difficult to extract using traditional methods based on 2D representations. These methods often relegate such features to raster or metric-based (two-dimensional) representations based on Digital Surface Models (DSM) or Digital Elevation Models (DEM), and thus are not suitable for resolving morphological and intensity features for fine-scale land cover mapping. Small footprint LiDAR provides an ideal way for capturing these 3D features. This research develops a novel method of integrating airborne LiDAR derived features and multi-temporal Landsat images to classify land cover types. We tested our approach in Williamson County, Illinois, which has diverse and mixed landscape features. Specifically, our method applied a 3D convolutional neural network (CNN) approach to extract features from LiDAR point clouds by (1) creating an occupancy grid, an intensity grid at 1-meter resolution, and then (2) normalizing and incorporating data into the 3D CNN. The extracted features (e.g., morphological and intensity features) from the 3D CNN were finally combined with multi-temporal spectral data to enhance the performance of land cover classification based on a Support Vector Machine classifier. Visual interpretation from both hyper-resolution photos and point clouds was used for training and preparation of testing data. The classification results show that our method outperforms a traditional method by 2.65% (from 81.52% to 84.17%) when solely using LiDAR and 2.19% (from 90.20% to 92.57%) when combining all available imageries. We demonstrate that our method can effectively extract LiDAR features and improve fine-scale land cover mapping through fusion of complementary types of remote sensing data.","Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,Landsat,Land cover classification,Urban,a 3d convolutional neural network method for land cover classification using lidar and multi-temporal landsat imagery,Classification,yes,,,,,,,,,,,,,,,
J,"Ji, SP; Zhang, C; Xu, AJ; Shi, Y; Duan, YL",3D Convolutional Neural Networks for Crop Classification with Multi-Temporal Remote Sensing Images,REMOTE SENSING,2018,3D convolution; convolutional neural networks; crop classification; multi-temporal remote sensing images; active learning,"This study describes a novel three-dimensional (3D) convolutional neural networks (CNN) based method that automatically classifies crops from spatio-temporal remote sensing images. First, 3D kernel is designed according to the structure of multi-spectral multi-temporal remote sensing data. Secondly, the 3D CNN framework with fine-tuned parameters is designed for training 3D crop samples and learning spatio-temporal discriminative representations, with the full crop growth cycles being preserved. In addition, we introduce an active learning strategy to the CNN model to improve labelling accuracy up to a required threshold with the most efficiency. Finally, experiments are carried out to test the advantage of the 3D CNN, in comparison to the two-dimensional (2D) CNN and other conventional methods. Our experiments show that the 3D CNN is especially suitable in characterizing the dynamics of crop growth and outperformed the other mainstream methods.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Gaofen-2,Crop classification,Agriculture,3d convolutional neural networks for crop classification with multi-temporal remote sensing images,Classification,no,94.9,,,,,,,91.3,,,,,,,
C,Li N.; Wang R.; Zhao H.; Wei W.,AN IMPROVED FEATURE EXTRACTION METHOD BASED on CONTEXT FEATURES for MULTI-SPECTRAL REMOTE SENSING IMAGERY,"ICSIDP 2019 - IEEE International Conference on Signal, Information and Data Processing 2019",2019,3D convolution network; context information; feature extraction; multi-spectral remote sensing images; small objects,"Feature extraction methods of multi-spectral remote sensing images is of great significance for remote sensing image analysis, but it still faces some challenges. The ability of traditional feature extraction methods based on artificial features or shallow machine learning have some shortcomings and limitations. Recently, a series of proposed R-CNN networks, especially Faster R-CNN, have achieved excellent results in the field of target recognition. However, Faster R-CNN for multi-spectral imagery object detection has several drawbacks: (1) the object spectral information cannot be fully utilized in Faster R-CNN used to process RGB images; (2) the spatial semantic relationship information which could not be mined by Faster R-CNN among remote sensing image features can improve the feature extraction ability of the network; (3) objects occupy relatively few pixels because of the low resolution of multi-spectral images, and Faster R-CNN has poor detection performance for small objects. To address the above problems, we propose an effective and novel object detection method for multi-spectral images with small objects. First, we design a feature extractor by adopting a 3D convolution neural network which can simultaneously extract spectral information and spatial information. Secondly, an object relation module for mining context information is introduced into the network. Finally, in order to solve the problem of small targets, a multi-scale object proposal network for generating regions of objects from several intermediate layers is used. We conducted a set of controlled trials on the satellite imagery feature detection dataset released by Dstl on the Kaggle website and the results showed that our approach was very effective. Â© 2019 IEEE.",Conference paper,,yes,WorldView-3,Feature extraction,Others,an improved feature extraction method based on context features for multi-spectral remote sensing imagery,Classification,yes,,,,,,,,,,,,,,,
C,A. B. Molini; D. Valsesia; G. Fracastoro; E. Magli,Deep Learning For Super-Resolution Of Unregistered Multi-Temporal Satellite Images,2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS),2019,Multi-image superresolution;convolutional neural networks;multi-temporal images,"Recently, convolutional neural networks (CNN) have been successfully applied to many remote sensing tasks. However, deep learning for multi-image superresolution from multitemporal imagery has received little attention so far. We propose a residual CNN that exploits both spatial and temporal correlations in the low-resolution image set by using 3D convolutional layers to combine multiple images from the same scene. The experiments have been carried out using a dataset of PROBA-V satellite ground images, composed of several low-resolution and high-resolution images taken at different times from instruments on the same platform, in the context of a challenge issued by the European Space Agency.",IEEE Conferences,,yes,PROBA-V,Improvement of the satellite image quality,Others,deep learning for super-resolution of unregistered multi-temporal satellite images,Regression,yes,,,,,,,,,,,,,,,
J,"Sun, ZC; Zhao, XW; Wu, MF; Wang, CZ",Extracting Urban Impervious Surface from WorldView-2 and Airborne LiDAR Data Using 3D Convolutional Neural Networks,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,2019,WorldView-2; Airborne light detection and ranging (LiDAR); Impervious surface; Convolutional neural networks (CNNs); Support vector machine (SVM),"The urban impervious surface has been recognized as a key quantifiable indicator in assessing urbanization and its environmental impacts. Adopting deep learning technologies, this study proposes an approach of three-dimensional convolutional neural networks (3D CNNs) to extract impervious surfaces from the WorldView-2 and airborne LiDAR datasets. The influences of different 3D CNN parameters on impervious surface extraction are evaluated. In an effort to reduce the limitations from single sensor data, this study also explores the synergistic use of multi-source remote sensing datasets for delineating urban impervious surfaces. Results indicate that our proposed 3D CNN approach has a great potential and better performance on impervious surface extraction, with an overall accuracy higher than 93.00% and the overall kappa value above 0.89. Compared with the commonly applied pixel-based support vector machine classifier, our proposed 3D CNN approach takes advantage not only of the pixel-level spatial and spectral information, but also of texture and feature maps through multi-scale convolutional processes, which enhance the extraction of impervious surfaces. While image analysis is facing large challenges in a rapidly developing big data era, our proposed 3D CNNs will become an effective approach for improved urban impervious surface extraction.",Environmental Sciences; Remote Sensing,Environmental Sciences & Ecology; Remote Sensing,yes,WorldView-2,Extracting Urban Impervious Surface,Urban,extracting urban impervious surface from worldview-2 and airborne lidar data using 3d convolutional neural networks,Classification,no,93.2,,,,93.47,93.75,,89.36,,,,,,,
C,A. S. Terliksiz; D. T. AltÃ½lar,"Use Of Deep Neural Networks For Crop Yield Prediction: A Case Study Of Soybean Yield in Lauderdale County, Alabama, USA",2019 8th International Conference on Agro-Geoinformatics (Agro-Geoinformatics),2019,crop yield prediction;deep neural networks;convolutional neural networks,"World population is constantly increasing and it is necessary to have sufficient crop production. Monitoring crop growth and yield estimation are very important for the economic development of a nation. The prediction of crop yield has direct impact on national and international economies and play important role in the food management and food security. Deep learning gains importance on crop monitoring, crop type classification and crop yield estimation applications with the recent advances in image classification using deep Convolutional Neural Networks. Traditional crop yield prediction approaches based on remote sensing consist of classical Machine Learning methods such as Support Vector Machines and Decision Trees. Convolutional Neural Network (CNN] and Long-Short Term Memory Network (LSTM] are deep neural network models that are proposed for crop yield prediction recently. This study focused on soybean yield prediction of Lauderdale County, Alabama, USA using 3D CNN model that leverages the spatiotemporal features. The yield is provided from USDA NASS Quick Stat tool for years 2003-2016. The satellite data used is collected from NASA's MODIS land products surface reflectance, land surface temperature and land surface temperature via Google Earth Engine. The root mean squared error (RMSE] is used as the evaluation metric in order to be able to compare the results with other methods that generally uses RMSE as the evaluation metric.",IEEE Conferences,,yes,MODIS,Crop yield prediction,Agriculture,"use of deep neural networks for crop yield prediction: a case study of soybean yield in lauderdale county, alabama, usa",Regression,no,,,,,,,,,,,,,0.81,,Soybean yield (bsh/ac)
J,"Lee, J; Im, J; Cha, DH; Park, H; Sim, S",Tropical Cyclone Intensity Estimation Using Multi-Dimensional Convolutional Neural Networks from Geostationary Satellite Data,REMOTE SENSING,2020,tropical cyclones; multispectral imaging; 2D; 3D convolutional neural networks,"For a long time, researchers have tried to find a way to analyze tropical cyclone (TC) intensity in real-time. Since there is no standardized method for estimating TC intensity and the most widely used method is a manual algorithm using satellite-based cloud images, there is a bias that varies depending on the TC center and shape. In this study, we adopted convolutional neural networks (CNNs) which are part of a state-of-art approach that analyzes image patterns to estimate TC intensity by mimicking human cloud pattern recognition. Both two dimensional-CNN (2D-CNN) and three-dimensional-CNN (3D-CNN) were used to analyze the relationship between multi-spectral geostationary satellite images and TC intensity. Our best-optimized model produced a root mean squared error (RMSE) of 8.32 kts, resulting in better performance (35%) than the existing model using the CNN-based approach with a single channel image. Moreover, we analyzed the characteristics of multi-spectral satellite-based TC images according to intensity using a heat map, which is one of the visualization means of CNNs. It shows that the stronger the intensity of the TC, the greater the influence of the TC center in the lower atmosphere. This is consistent with the results from the existing TC initialization method with numerical simulations based on dynamical TC models. Our study suggests the possibility that a deep learning approach can be used to interpret the behavior characteristics of TCs.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,COMS,Tropical Cyclone Intensity,Geohazards,tropical cyclone intensity estimation using multi-dimensional convolutional neural networks from geostationary satellite data,Regression,no,,,,,,,,,,,,0.8856,11.34,8.65,Maximum sustained wind speed (kts)
J,Chen Y.; Tang L.; Kan Z.; Latif A.; Yang X.; Bilal M.; Li Q.,Cloud and cloud shadow detection based on multiscale 3D-CNN for high resolution multispectral imagery,IEEE Access,2020,Cloud detection; cloud shadow; convolution neural networks; multiscale 3D-CNN,"Cloud and cloud shadow detection is one of the most important tasks for optical remote sensing image preprocessing. It is not an easy task due to the variety and complexity of underlying surfaces, such as the low-albedo objects (water and mountain shadow) and the high-albedo objects (snow and ice). In this study, an end-to-end multiscale 3D-CNN method is proposed for cloud and cloud shadow detection in high resolution multispectral imagery. Specifically, a multiscale learning module is designed to extract cloud and cloud shadow contextual information of different levels. In order to make full use of band information, four band-combination images are inputted into the multiscale 3D-CNN. A joint spectral-spatial information of 3D-convolution layer is developed to fully explore the joint spatial-spectral correlations feature in the input data. Overall, in the experiments undertaken in this paper, the proposed method achieved a mean overall accuracy of 97.27 & x0025; for cloud detection, with a mean precision of 96.02 & x0025; and a mean recall of 95.86 & x0025;. For cloud shadow detection, the proposed method achieved a mean precision of 95.92 & x0025; and a mean recall of 92.86 & x0025;. Experimental results on two validation datasets (GF-1 WFV validation data and ZY-3 validation data) show that the proposed multiscale-3D-CNN method achieved good performance with limited spectral ranges.","Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications",Science Citation Index Expanded (SCI-EXPANDED),yes,"Gaofen-1, ZY-3",Cloud and Cloud Shadow Detection,Others,cloud and cloud shadow detection based on multiscale 3d-cnn for high resolution multispectral imagery,Classification,no,98.87,,95.59,93.73,,,,,,,,,,,
J,"Salvetti, F; Mazzia, V; Khaliq, A; Chiaberge, M",Multi-Image Super Resolution of Remotely Sensed Images Using Residual Attention Deep Neural Networks,REMOTE SENSING,2020,deep learning; multi-image super-resolution; attention networks; 3D convolutional neural networks,"Convolutional Neural Networks (CNNs) consistently proved state-of-the-art results in image Super-resolution (SR), representing an exceptional opportunity for the remote sensing field to extract further information and knowledge from captured data. However, most of the works published in the literature focused on the Single-image Super-resolution problem so far. At present, satellite-based remote sensing platforms offer huge data availability with high temporal resolution and low spatial resolution. In this context, the presented research proposes a novel residual attention model (RAMS) that efficiently tackles the Multi-image Super-resolution task, simultaneously exploiting spatial and temporal correlations to combine multiple images. We introduce the mechanism of visual feature attention with 3D convolutions in order to obtain an aware data fusion and information extraction of the multiple low-resolution images, transcending limitations of the local region of convolutional operations. Moreover, having multiple inputs with the same scene, our representation learning network makes extensive use of nestled residual connections to let flow redundant low-frequency signals and focus the computation on more important high-frequency components. Extensive experimentation and evaluations against other available solutions, either for Single or Multi-image Super-resolution, demonstrated that the proposed deep learning-based solution can be considered state-of-the-art for Multi-image Super-resolution for remote sensing applications.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,PROBA-V,Improvement of the satellite image quality,Others,multi-image super resolution of remotely sensed images using residual attention deep neural networks,Image enhancement,yes,,,,,,,,,,,,,,,
J,Chen G.; Pei Q.; Kamruzzaman M.M.,Remote sensing image quality evaluation based on deep support value learning networks,Signal Processing: Image Communication,2020,3D convolutional neural networks; Deep support value learning networks; Feature extraction; Remote sensing image,"Aiming at the problem that the remote sensing image quality evaluation models with manually extracted features lack robustness and generality, this paper proposes a 3D CNN-based architecture and nuclear power plant for accurate remote sensing image quality assessment. The model incorporates two sub-networks. The DSVL-based sub-network is employed to extract multi-scale, multi-direction and high-level features by layer-wise training. Afterwards, the extracted feature maps are fused as flowed as input data of the second sub-network, which is designed with 3D CNN architecture and nuclear power plant for remote sensing image quality assessment. Experimental results on remote sensing image quality database from the GeoEye-1 and WorldView-2 satellites show that the proposed model can optimally discover the essential features of the image and effectively extract the high-frequency information of each level of image, and has better overall quality assessment performance than the other state-of-the-art methods. Â© 2020 Elsevier B.V.",Article,,no,"GeoEye-1, WorldView-2",Image quality evaluation,Others,remote sensing image quality evaluation based on deep support value learning networks,Regression,yes,,,,,,,,,,,,,,,
J,"Dorr, F",Satellite Image Multi-Frame Super Resolution Using 3D Wide-Activation Neural Networks,REMOTE SENSING,2020,multi-frame super resolution; wide activation super resolution; 3D convolutional neural network; deep learning,"The small satellite market continues to grow year after year. A compound annual growth rate of 17% is estimated during the period between 2020 and 2025. Low-cost satellites can send a vast amount of images to be post-processed at the ground to improve the quality and extract detailed information. In this domain lies the resolution enhancement task, where a low-resolution image is converted to a higher resolution automatically. Deep learning approaches to Super Resolution (SR) reached the state-of-the-art in multiple benchmarks; however, most of them were studied in a single-frame fashion. With satellite imagery, multi-frame images can be obtained at different conditions giving the possibility to add more information per image and improve the final analysis. In this context, we developed and applied to the PROBA-V dataset of multi-frame satellite images a model that recently topped the European Space Agency's Multi-frame Super Resolution (MFSR) competition. The model is based on proven methods that worked on 2D images tweaked to work on 3D: the Wide Activation Super Resolution (WDSR) family. We show that with a simple 3D CNN residual architecture with WDSR blocks and a frame permutation technique as the data augmentation, better scores can be achieved than with more complex models. Moreover, the model requires few hardware resources, both for training and evaluation, so it can be applied directly on a personal laptop.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,PROBA-V,Improvement of the satellite image quality,Others,satellite image multi-frame super resolution using 3d wide-activation neural networks,Image enhancement,yes,,,,,,,,,,,,,,,
J,"Song, Ahram; Choi, Jaewan",Fully Convolutional Networks with Multiscale 3D Filters and Transfer Learning for Change Detection in High Spatial Resolution Satellite Images,REMOTE SENSING,2020,multiscale three-dimensional filters; transfer learning; change detection; high spatial resolution satellite image; fully convolutional network; convolutional long short-term memory,"Remote sensing images having high spatial resolution are acquired, and large amounts of data are extracted from their region of interest. For processing these images, objects of various sizes, from very small neighborhoods to large regions composed of thousands of pixels, should be considered. To this end, this study proposes change detection method using transfer learning and recurrent fully convolutional networks with multiscale three-dimensional (3D) filters. The initial convolutional layer of the change detection network with multiscale 3D filters was designed to extract spatial and spectral features of materials having different sizes; the layer exploits pre-trained weights and biases of semantic segmentation network trained on an open benchmark dataset. The 3D filter sizes were defined in a specialized way to extract spatial and spectral information, and the optimal size of the filter was determined using highly accurate semantic segmentation results. To demonstrate the effectiveness of the proposed method, binary change detection was performed on images obtained from multi-temporal Korea multipurpose satellite-3A. Results revealed that the proposed method outperformed the traditional deep learning-based change detection methods and the change detection accuracy improved using multiscale 3D filters and transfer learning.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Kompsat-3A,Change detection,Urban,fully convolutional networks with multiscale 3d filters and transfer learning for change detection in high spatial resolution satellite images,Classification,yes,97.95,94.12,,,,,,92.88,,,,,,,
C,"Bergamasco, L; Bovolo, F; Bruzzone, L",A Novel Deep-Learning Data Structure for Multispectral Remote Sensing Images,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXVI,2020,Spatial-spectral analysis; Deep-learning classification; Data-cube analysis; Convolutional Neural Network; Remote Sensing,"Standard deep-learning (DL) architectures do not optimize the use of the spatial and spectral information in the multi-spectral images but often consider only one of the two components. Two-stream DL architectures split and process them separately. However, the fusing of the output of the two streams is a challenging task. 3D-CNN processes spatial and spectral information together at the cost of a large number of parameters. To overcome these limitations, we propose a novel DL data structure that re-organizes the spectral and spatial information in remote-sensing (RS) images and process them together. Representing a RS image I as a data cube, we handle the spatial and spectral information by reducing the spectral bands from N to M, where M can drop out to one. The spectral information is projected in the spatial dimensions and re-organized in 2-dimensional B blocks. The proposed approach analyzes the spectral information of each block by using 2-dimensional convolutional kernels of appropriate size and stride. The output represents the relationship between the spectral bands of the input image and preserves the spatial relationship between its neighboring pixels. The spatial relationships are analyzed by processing the output of the previous layer with standard 2D-CNNs. Experiments by using images acquired by Sentinel-2 and Landsat-8 data and the labels of the LUCAS database released in 2018 provide promising results.",Remote Sensing; Optics; Imaging Science & Photographic Technology,Remote Sensing; Optics; Imaging Science & Photographic Technology,no,"Sentinel-2, Landsat-8",Land-use and land-cover,Vegetation,a novel deep-learning data structure for multispectral remote sensing images,Classification,yes,,,,,,,,,,,,,,,
C,M. S. Aydemir; A. N. Keyik; F. Kahraman; E. Aptoula,Land Cover Map Production of the Sakarya Basin from Multi-Temporal Satellite Images,2020 28th Signal Processing and Communications Applications Conference (SIU),2020,Remote sensing;convolutional neural networks;land cover;land use;CORINE,"The proliferation of multi-temporal remote sensing imagery, especially through Sentinel 2 satellites, has reinforced efforts towards the processing of multi-spectral and multi-temporal images. In this paper, we present the results of our study on the production of land cover/land use of the Sakarya basin, employing CORINE ground truths. The main contribution of our study is the exploitation of the temporal dimension through 3 dimensional convolutional neural networks, motivated by their capacity to process data across the temporal dimension of the input patch cube. Our experiments spanning 26 classes at a region-wide scale, show that 3D convolutional neural networks possess a strong potential in this regard.",IEEE Conferences,,yes,Sentinel-2,Land Cover Map,Vegetation,land cover map production of the sakarya basin from multi-temporal satellite images,Classification,yes,86.32,,,,,,,75.54,,,,,,,
C,Ä°. YÄ±lmaz; M. Ä°mamoÄŸlu; G. Ã–zbulak; F. Kahraman; E. Aptoula,Large Scale Crop Classification from Multi-temporal and Multi-spectral Satellite Images,2020 28th Signal Processing and Communications Applications Conference (SIU),2020,crop classification;remote sensing;deep learning;random forest,"Crop classification is one of the foremost and most challenging applications of remote sensing. Crops exhibit both high intra-class variance across geographical locations, as well as low inter-class variance especially across seasons. As such, they require both spectral and temporal input, both of which are provided by the Sentinel 2 satellites. In this paper, we present the preliminary results of our multispectral and multitemporal crop classification analysis, on a region-wide scale, encompassing multiple climatological conditions and a high number of crop types. We have experimented using the ground-truth provided by the Farmer Registration System, with both well-known spectral and spatial shallow features and classifiers, at both pixel and field level, as well as with state of the art 3D convolutional neural networks. Our results show that Sentinel 2 imagery exhibit a strong potential as input for a systematic crop classification infrastructure.",IEEE Conferences,,yes,Sentinel-2,Crop classification,Agriculture,large scale crop classification from multi-temporal and multi-spectral satellite images,Classification,yes,72.7,,,,,,,68.3,,,,,,,
J,"Ji, SP; Zhang, ZL; Zhang, C; Wei, SQ; Lu, M; Duan, YL",Learning discriminative spatiotemporal features for precise crop classification from multi-temporal satellite images,INTERNATIONAL JOURNAL OF REMOTE SENSING,2020,,"Precise crop classification from multi-temporal remote sensing images has important applications such as yield estimation and food transportation planning. However, the mainstream convolutional neural networks based on 2D convolution collapse the time series information. In this study, a 3D fully convolutional neural network (FCN) embedded with a global pooling module and channel attention modules is proposed to extract discriminative spatiotemporal presentations of different types of crops from multi-temporal high-resolution satellite images. Firstly, a novel 3D FCN structure is introduced to replace 2D FCNs as well as to improve current 3D convolutional neural networks (CNNs) by providing a mean to learn distinctive spatiotemporal representations of each crop type from the reshaped multi-temporal images. Secondly, to strengthen the learning significance of the spatiotemporal representations, our approach includes 3D channel attention modules, which regulate the between-channel consistency of the features from the encoder and the decoder, and a 3D global pooling module, which selects the most distinctive features at the top of the encoder. Experiments were conducted using two data sets with different types of crops and time spans. Our results show that our method outperformed in both accuracy and efficiency, several mainstream 2D FCNs as well as a recent 3D CNN designed for crop classification. The experimental data and source code are made openly available at http://study.rsgis.whu.edu.cn/pages/download/..",Remote Sensing; Imaging Science & Photographic Technology,Remote Sensing; Imaging Science & Photographic Technology,no,Gaofen-2,Crop classification,Agriculture,learning discriminative spatiotemporal features for precise crop classification from multi-temporal satellite images,Classification,yes,,,,,,,,,,,,,,,
J,"Zhang, WC; Liu, HB; Wu, W; Zhan, LQ; Wei, J",Mapping Rice Paddy Based on Machine Learning with Sentinel-2 Multi-Temporal Data: Model Comparison and Transferability,REMOTE SENSING,2020,rice; convolutional neural network; F1 score; sentinel-2; transfer,"Rice is an important agricultural crop in the Southwest Hilly Area, China, but there has been a lack of efficient and accurate monitoring methods in the region. Recently, convolutional neural networks (CNNs) have obtained considerable achievements in the remote sensing community. However, it has not been widely used in mapping a rice paddy, and most studies lack the comparison of classification effectiveness and efficiency between CNNs and other classic machine learning models and their transferability. This study aims to develop various machine learning classification models with remote sensing data for comparing the local accuracy of classifiers and evaluating the transferability of pretrained classifiers. Therefore, two types of experiments were designed: local classification experiments and model transferability experiments. These experiments were conducted using cloud-free Sentinel-2 multi-temporal data in Banan District and Zhongxian County, typical hilly areas of Southwestern China. A pure pixel extraction algorithm was designed based on land-use vector data and a Google Earth Online image. Four convolutional neural network (CNN) algorithms (one-dimensional (Conv-1D), two-dimensional (Conv-2D) and three-dimensional (Conv-3D_1 and Conv-3D_2) convolutional neural networks) were developed and compared with four widely used classifiers (random forest (RF), extreme gradient boosting (XGBoost), support vector machine (SVM) and multilayer perceptron (MLP)). Recall, precision, overall accuracy (OA) and F1 score were applied to evaluate classification accuracy. The results showed that Conv-2D performed best in local classification experiments with OA of 93.14% and F1 score of 0.8552 in Banan District, OA of 92.53% and F1 score of 0.8399 in Zhongxian County. CNN-based models except Conv-1D provided more desirable performance than non-CNN classifiers. Besides, among the non-CNN classifiers, XGBoost received the best result with OA of 89.73% and F1 score of 0.7742 in Banan District, SVM received the best result with OA of 88.57% and F1 score of 0.7538 in Zhongxian County. In model transferability experiments, almost all CNN classifiers had low transferability. RF and XGBoost models have achieved acceptable F1 scores for transfer (RF = 0.6673 and 0.6469, XGBoost = 0.7171 and 0.6709, respectively).","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Mapping rice paddy,Vegetation,mapping rice paddy based on machine learning with sentinel-2 multi-temporal data: model comparison and transferability,Classification,no,91.57,83.21,81.75,84.72,,,,,,,,,,,
J,"Kalinicheva, E; Sublime, J; Trocan, M",Unsupervised Satellite Image Time Series Clustering Using Object-Based Approaches and 3D Convolutional Autoencoder,REMOTE SENSING,2020,satellite image time series; unsupervised learning; clustering; segmentation; 3D convolutional network; autoencoder,"Nowadays, satellite image time series (SITS) analysis has become an indispensable part of many research projects as the quantity of freely available remote sensed data increases every day. However, with the growing image resolution, pixel-level SITS analysis approaches have been replaced by more efficient ones leveraging object-based data representations. Unfortunately, the segmentation of a full time series may be a complicated task as some objects undergo important variations from one image to another and can also appear and disappear. In this paper, we propose an algorithm that performs both segmentation and clustering of SITS. It is achieved by using a compressed SITS representation obtained with a multi-view 3D convolutional autoencoder. First, a unique segmentation map is computed for the whole SITS. Then, the extracted spatio-temporal objects are clustered using their encoded descriptors. The proposed approach was evaluated on two real-life datasets and outperformed the state-of-the-art methods.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"Sentinel-2, SPOT-5",Land cover classification,Vegetation,unsupervised satellite image time series clustering using object-based approaches and 3d convolutional autoencoder,Segmentation,yes,,,,,,,,,,,,,,,
J,"Jiang, ZC; Ma, Y",Accurate extraction of offshore raft aquaculture areas based on a 3D-CNN model,INTERNATIONAL JOURNAL OF REMOTE SENSING,2020,,"Offshore aquaculture plays an important role in China's marine fishery economy. The research on the extraction of offshore aquaculture areas based on remote sensing technology is of great significance for the regulation of offshore fishery resources and the protection of the marine ecological environment. This paper uses the Gaofen-2 series multispectral remote sensing image to extract the offshore aquaculture areas of Lianyungang City. We use the optimum index factor to extract the spectral features of the aquaculture areas and the grey-level co-occurrence matrix to extract their texture features. We use the Bhattacharyya distance to select the spatial and spectrum features and construct the characteristic data set of the aquaculture areas. In this paper, we propose a method to construct a uniform distributed disturbance term to optimize the cross entropy loss function. We employ it in the three-dimensional convolutional neural network (3D-CNN) model, extract the extended feature data set of aquaculture areas, and input it into the radial basis function support vector machine (RBF-SVM) classifier for classification. Within the study area of 150 km(2), the experimental results show that the extraction model has high extraction accuracy and strong spatial migration despite complex water backgrounds. The F-1-score values in the training area and the four random test areas were 0.939 or above for 2017 data. In addition, the extraction model also has stable time migration. We used the extraction model on remote sensing data for the same study area in 2018 and 2019. The F-1-scores for all test areas are 0.866 or higher. Therefore, the model proposed in this paper is suitable for the extraction of large-scale and multi-temporal offshore raft aquaculture areas from remote sensing images.",Remote Sensing; Imaging Science & Photographic Technology,Remote Sensing; Imaging Science & Photographic Technology,no,Gaofen-2,Extraction of offshore raft aquaculture areas based,Water,accurate extraction of offshore raft aquaculture areas based on a 3d-cnn model,Classification,yes,,86.6,,,,,,,,,,,,,
J,"Han, YL; Wei, C; Zhou, RY; Hong, ZH; Zhang, Y; Yang, SH",Combining 3D-CNN and Squeeze-and-Excitation Networks for Remote Sensing Sea Ice Image Classification,MATHEMATICAL PROBLEMS IN ENGINEERING,2020,,"Sea ice is one of the most prominent marine disasters in high latitudes. Remote sensing technology provides an effective means for sea ice detection. Remote sensing sea ice images contain rich spectral and spatial information. However, most traditional methods only focus on spectral information or spatial information, and do not excavate the feature of spectral and spatial simultaneously in remote sensing sea ice images classification. At the same time, the complex correlation characteristics among spectra and small sample problem in sea ice classification also limit the improvement of sea ice classification accuracy. For this issue, this paper proposes a new remote sensing sea ice image classification method based on squeeze-and-excitation (SE) network, convolutional neural network (CNN), and support vector machines (SVMs). The proposed method designs 3D-CNN deep network so as to fully exploit the spatial-spectrum features of remote sensing sea ice images and integrates SE-Block into 3D-CNN in-depth network in order to distinguish the contributions of different spectra to sea ice classification. According to the different contributions of spectral features, the weight of each spectral feature is optimized by fusing SE-Block in order to further enhance the sample quality. Finally, information-rich and representative samples are chosen by combining the idea of active learning and input into SVM classifier, and this achieves superior classification accuracy of remote sensing sea ice images with small samples. In order to verify the effectiveness of the proposed method, we conducted experiments on three different data from Baffin Bay, Bohai Bay, and Liaodong Bay. The experimental results show that compared with other classical classification methods, the proposed method comprehensively considers the correlation among spectral features and the small samples problems and deeply excavates the spatial-spectrum characteristics of sea ice and achieves better classification performance, which can be effectively applied to remote sensing sea ice image classification.","Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications",Engineering; Mathematics,yes,Landsat-8,Sea Ice image classification,Water,combining 3d-cnn and squeeze-and-excitation networks for remote sensing sea ice image classification,Classification,yes,91.69,,,,,,,,,,,,,,
J,"Wang, L; Wu, YX; Xu, JP; Zhang, HY; Wang, XY; Yu, JB; Sun, Q; Zhao, ZY",STATUS PREDICTION BY 3D FRACTAL NET CNN BASED ON REMOTE SENSING IMAGES,FRACTALS-COMPLEX GEOMETRY PATTERNS AND SCALING IN NATURE AND SOCIETY,2020,Fractal Net; 3D CNN; Status Prediction; Remote Sensing Images; Eutrophication,"The contradiction between the supply and demand of water resources is becoming increasingly prominent, whose main reason is the eutrophication of rivers and lakes. However, limited and inaccurate data makes it impossible to establish a precise model to successfully predict eutrophication levels. Moreover, it is incompetent to distinguish the degree of eutrophication status of lakes by manual calculation and processing. Focusing on these inconveniences, this study proposes 3D fractal net CNN to extract features in remote sensing images automatically, aiming at achieving scientific forecasting on eutrophication status of lakes. In order to certificate the effectiveness of the proposed method, we predict the state of the water body based on remote sensing images of natural lake. The images in natural lake were accessed by MODIS satellite, cloud-free chlorophyll inversion picture of 2009 was resized into 273 x 273 patches, which were collected as training and testing samples. In the total of 162 pictures, our study makes three consecutive pictures as a set of data so as to attain 120 group of training and 40 testing data. Taking one set of data as input of the neural network and the next day's eutrophication level as labels, CNNs act considerable efficiency. Through the experimental results of 2D CNN, 3D CNN and 3D fractal net CNN, 3D fractal net CNN has more outstanding performance than the other two, with the prediction accuracy of 67.5% better than 47.5% and 62.5%, respectively.","Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences",Mathematics; Science & Technology - Other Topics,yes,MODIS,Eutrophication status of lakes,Water,status prediction by 3d fractal net cnn based on remote sensing images,Classification,no,62.5,,,,,,,,,,,,,,
J,Zhang Q.; Yuan Q.; Li Z.; Sun F.; Zhang L.,Combined deep prior with low-rank tensor SVD for thick cloud removal in multitemporal images,ISPRS Journal of Photogrammetry and Remote Sensing,2021,Deep prior; Low-rank tensor SVD; Multitemporal images; Spatio-temporal; Thick cloud removal,"The thick cloud coverage phenomenon severely disturbs optical satellite observation missions (covering approximately 40â€“60% areas in the global scale). Therefore, the manner by which to eliminate thick cloud in remote sensing imagery is greatly significant and indispensable. In this study, we combine the deep spatio-temporal prior with low-rank tensor singular value decomposition (DP-LRTSVD) for thick cloud removal in multitemporal images. On the one hand, DP-LRTSVD utilizes the low-rank characteristic of multitemporal images via the third-order tensor SVD and completion. On the other hand, DP-LRTSVD employs the deep spatio-temporal feature expression ability by 3D convolutional neural network. The proposed framework can effectively eliminate thick cloud in multitemporal images through combining the model-driven and data-driven strategies. Moreover, DP-LRTSVD outperforms on thick cloud removal in the simulated and real multitemporal Sentinel-2/GF-1 experiments compared with model-driven or data-driven methods. In contrast with most methods that can only use a single reference image for thick cloud removal, the proposed method can simultaneously eliminate thick cloud in time-series images. Â© 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)",Article,,no,Sentinel-2 and Gaofen-1,Thick cloud removal,Others,combined deep prior with low-rank tensor svd for thick cloud removal in multitemporal images,Regression,yes,,,,,,,,,,,,,,,
J,"He, S; Zhou, RQ; Li, SH; Jiang, S; Jiang, WS",Disparity Estimation of High-Resolution Remote Sensing Images with Dual-Scale Matching Network,REMOTE SENSING,2021,high-resolution remote sensing images; disparity estimation; stereo matching; convolutional neural network; dual-scale matching,"As an essential task in remote sensing, disparity estimation of high-resolution stereo images is still confronted with intractable problems due to extremely complex scenes and dynamically changing disparities. Especially in areas containing texture-less regions, repetitive patterns, disparity discontinuities, and occlusions, stereo matching is difficult. Recently, convolutional neural networks have provided a new paradigm for disparity estimation, but it is difficult for current models to consider both accuracy and speed. This paper proposes a novel end-to-end network to overcome the aforementioned obstacles. The proposed network learns stereo matching at dual scales, in which the low one captures coarse-grained information while the high one captures fine-grained information, helpful for matching structures of different scales. Moreiver, we construct cost volumes from negative to positive values to make the network work well for both negative and nonnegative disparities since the disparity varies dramatically in remote sensing stereo images. A 3D encoder-decoder module formed by factorized 3D convolutions is introduced to adaptively learn cost aggregation, which is of high efficiency and able to alleviate the edge-fattening issue at disparity discontinuities and approximate the matching of occlusions. Besides, we use a refinement module that brings in shallow features as guidance to attain high-quality full-resolution disparity maps. The proposed network is compared with several typical models. Experimental results on a challenging dataset demonstrate that our network shows powerful learning and generalization abilities. It achieves convincing performance on both accuracy and efficiency, and improvements of stereo matching in these challenging areas are noteworthy.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,WorldView-3,Disparity estimation,Others,disparity estimation of high-resolution remote sensing images with dual-scale matching network,Regression,yes,,,,,,,,,,,,,,,
C,Wang D.; Bai Y.; Bai B.; Wu C.; Li Y.,Heterogeneous two-stream network with hierarchical feature Prefusion for multispectral pan-sharpening,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2021,Heterogeneous network; Hierarchical feature prefusion; Pan-sharpening; Two-stream network,"Multispectral (MS) pan-sharpening aims at producing a high spatial resolution (HR) MS image by fusing a single-band HR panchromatic (PAN) image and a corresponding MS image with low spatial resolution. In this paper, we propose a heterogeneous two-stream network (HTSNet) with hierarchical feature prefusion for MS pan-sharpening. The HTSNet employs a heterogeneous group of spatial and spectral streams for spatial and spectral information extraction, respectively. The spatial stream utilizes a 2D CNN for spatial information extraction from the PAN images, and the spectral stream obtains spectral feature cubes from the MS images by a 3D CNN. At the same time, a prefusion module is introduced to prefuse the spatial details with spectral information and transfer information between different streams, which can enhance later processing. In the experiment, the Gaofen-2 satellite dataset is utilized to compare the proposed method with the state-of-the-art MS pan-sharpening methods. Experimental results demonstrate the superiority of our HTSNet in terms of visual effect and quantitative qualities.  Â©2021 IEEE.",Conference paper,,yes,Gaofen-2,Pan-sharpening,Others,heterogeneous two-stream network with hierarchical feature prefusion for multispectral pan-sharpening,Image enhancement,yes,,,,,,,,,,,,,,,
J,"Zhang, L; Liu, P; Wang, LZ; Liu, JB; Song, BZ; Zhang, YW; He, GJ; Zhang, H",Improved 1-km-Resolution Hourly Estimates of Aerosol Optical Depth Using Conditional Generative Adversarial Networks,REMOTE SENSING,2021,aerosol; conditional generative adversarial network; spatio-temporal estimation,"Aerosol Optical Depth (AOD) is a crucial parameter for various environmental and climate studies. Merging multi-sensor AOD products is an effective way to produce AOD products with more spatiotemporal integrity and accuracy. This study proposed a conditional generative adversarial network architecture (AeroCGAN) to improve the estimation of AOD. It first adopted MODIS Multiple Angle Implication of Atmospheric Correction (MAIAC) AOD data to training the initial model, and then transferred the trained model to Himawari data and obtained the estimation of 1-km-resolution, hourly Himawari AOD products. Specifically, the generator adopted an encoder-decoder network for preliminary resolution enhancement. In addition, a three-dimensional convolutional neural network (3D-CNN) was used for environment features extraction and connected to a residual network for improving accuracy. Meanwhile, the sampled data and environment data were designed as conditions of the generator. The spatial distribution feature comparison and quantitative evaluation over an area of the North China Plain during the year 2017 have shown that this approach can better model the distribution of spatial features of AOD data and improve the accuracy of estimation with the help of local environment patterns.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Himawari-7,Aerosol Optical Depth,Others,improved 1-km-resolution hourly estimates of aerosol optical depth using conditional generative adversarial networks,Regression,yes,,,,,,,,,,,,0.865,0.314,,Aerosol optical depth
J,"Kong, Fanqiang; Hu, Kedi; Li, Yunsong; Li, Dan; Zhao, Shunmin",Spectral-Spatial Feature Partitioned Extraction Based on CNN for Multispectral Image Compression,REMOTE SENSING,2021,spectral-spatial feature; multispectral image compression; partitioned extraction; group convolution; rate-distortion,"Recently, the rapid development of multispectral imaging technology has received great attention from many fields, which inevitably involves the image transmission and storage problem. To solve this issue, a novel end-to-end multispectral image compression method based on spectral-spatial feature partitioned extraction is proposed. The whole multispectral image compression framework is based on a convolutional neural network (CNN), whose innovation lies in the feature extraction module that is divided into two parallel parts, one is for spectral and the other is for spatial. Firstly, the spectral feature extraction module is used to extract spectral features independently, and the spatial feature extraction module is operated to obtain the separated spatial features. After feature extraction, the spectral and spatial features are fused element-by-element, followed by downsampling, which can reduce the size of the feature maps. Then, the data are converted to bit-stream through quantization and lossless entropy encoding. To make the data more compact, a rate-distortion optimizer is added to the network. The decoder is a relatively inverse process of the encoder. For comparison, the proposed method is tested along with JPEG2000, 3D-SPIHT and ResConv, another CNN-based algorithm on datasets from Landsat-8 and WorldView-3 satellites. The result shows the proposed algorithm outperforms other methods at the same bit rate.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"Landsat-8, WorldView-3",Multispectral image compression,Others,spectral-spatial feature partitioned extraction based on cnn for multispectral image compression,Compression,yes,,,,,,,,,,,,,,,
C,Mohammadi S.; Belgiu M.; Stein A.,3D FULLY CONVOLUTIONAL NEURAL NETWORKS WITH INTERSECTION OVER UNION LOSS FOR CROP MAPPING FROM MULTI-TEMPORAL SATELLITE IMAGES,International Geoscience and Remote Sensing Symposium (IGARSS),2021,Crop mapping; deep learning; fully convolutional neural networks; time series,"Information on cultivated crops is relevant for a large number of food security studies. Different scientific efforts are dedicated to generate this information from remote sensing images by means of machine learning methods. Unfortunately, these methods do not take account of the spatial-temporal relationships inherent in remote sensing images. In our paper, we explore the capability of a 3D Fully Convolutional Neural Network (FCN) to map crop types from multi-temporal images. In addition, we propose the Intersection Over Union (IOU) loss function for increasing the overlap between the predicted classes and ground reference data. The proposed method was applied to identify soybean and corn from a study area situated in the US corn belt using multi-temporal Landsat images. The study shows that our method outperforms related methods, obtaining a Kappa coefficient of 91.8%. We conclude that using the IOU loss function provides a superior choice to learn individual crop types. Â© 2021 IEEE.",Conference paper,,yes,Landsat ARD,Crop mapping,Agriculture,3d fully convolutional neural networks with intersection over union loss for crop mapping from multi-temporal satellite images,Segmentation,yes,,,,,93.7,93.6,,91.8,,,,,,,
C,Meshkini K.; Bovolo F.; Bruzzone L.,AN UNSUPERVISED CHANGE DETECTION APPROACH FOR DENSE SATELLITE IMAGE TIME SERIES USING 3D CNN,International Geoscience and Remote Sensing Symposium (IGARSS),2021,3D convolutional neural network; Deep learning; High resolution image; Land cover change map; Satellite image time series,"Recent satellite missions have initiated a new era in the area of Satellite Image Time Series (SITS) analysis by providing a huge number of High Resolution (HR) spectral-temporal images. The availability of HR images opens a door to an unprecedented wide range of possibilities to produce and develop high resolution Land Cover (LC) and Land Cover Change (LCC) maps. The goal of this paper is to effectively use high spatio-temporal resolution images to generate LCC maps by defining a novel automatic and unsupervised deep learning method based on three-dimensional (3D) Convolutional Neural Network (CNN). The method extracts spatio-temporal information from long SITS by using a pre-trained 3D CNN, detects changes and locates them in space and time. Experiments have provided promising results over both Amazonia and Saudi Arabia in the period 2013-2017, and has been compared to the other well-known LCC detection method. Â© 2021 IEEE.",Conference paper,,yes,"Landsat-5, Landsat-7, Landsat-8",Land Cover Change Maps,Vegetation,an unsupervised change detection approach for dense satellite image time series using 3d cnn,Segmentation,no,,,,,,,,,,,,,,,
J,"Qiao, MJ; He, XH; Cheng, XJ; Li, PL; Luo, HT; Zhang, LH; Tian, ZH","Crop yield prediction from multi-spectral, multi-temporal remotely sensed imagery using recurrent 3D convolutional neural networks*",INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,2021,Crop yield prediction; 3D convolutional neural network; Long short-term memory; Multi-temporal images,"Crop yield prediction has played a vital role in maintaining food security and has been extensively investigated in recent decades. Most research has focused on excavating fixed spectral information from remote sensing images. However, the growth of crops is a highly complex trait determined by diverse features. To maximally explore these heterogeneous features, we aim to simultaneously exploit spatial, spectral, and temporal information from multi-spectral and multi-temporal remotely sensed imagery. Therefore, in this paper, we propose a novel deep learning architecture for crop yield prediction, namely, SSTNN (Spatial-Spectral-Temporal Neural Network), which combines 3D convolutional and recurrent neural networks to exploit their complementarity. Specifically, the SSTNN incorporates a spatial-spectral learning module and a temporal dependency capturing module into a unified convolutional network to recognize the joint spatial-spectral-temporal representation. The novel spatialspectral feature learning module first exploits sufficient spatial-spectral features from the multi-spectral images. Then, the temporal dependency capturing module is concatenated on top of the spatial-spectral feature learning module to mine the temporal relationship from the long time-series images. Furthermore, we introduce a new loss function that eliminates the influence of an imbalanced distribution of crop yield labels. Finally, the proposed SSTNN is validated on winter wheat and corn yield predictions from China. The results are compared with widely used machine learning methods as well as state-of-art deep learning methods. The experimental results demonstrate that the proposed method can provide better prediction performance than the competitive methods.",Remote Sensing,Remote Sensing,yes,MODIS,Crop yield prediction,Agriculture,"crop yield prediction from multi-spectral, multi-temporal remotely sensed imagery using recurrent 3d convolutional neural networks*",Regression,yes,,,,,,,,,,,,0.755,0.755,,Crop yield prediction (MT/ha)
J,"Sagan, V; Maimaitijiang, M; Bhadra, S; Maimaitiyiming, M; Brown, DR; Sidike, P; Fritschi, FB",Field-scale crop yield prediction using multi-temporal WorldView-3 and PlanetScope satellite data and deep learning,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2021,PlanetScope; WorldView-3; Deep learning; Convolutionneural network; ResNet; Artificial intelligence; Food security,"Agricultural management at field-scale is critical for improving yield to address global food security, as providing enough food for the world's growing population has become a wicked problem for both scientists and policy-makers. County- or regional-scale data do not provide meaningful information to farmers who are interested in field-scale yield forecasting for effective and timely field management. No studies directly utilized raw satellite imagery for field-scale yield prediction using deep learning. The objectives of this paper were twofold: (1) to develop a raw imagery-based deep learning approach for field-scale yield prediction, (2) investigate the contribution of in-season multitemporal imagery for grain yield prediction with hand-crafted features and WorldView-3 (WV) and PlanetScope (PS) imagery as the direct input, respectively. Four WV-3 and 25 PS imagery collected during the growing season of soybean were utilized. Both 2-dimensional (2D) and 3-dimensional (3D) convolution neural network (CNN) architectures were developed that integrated spectral, spatial, temporal information contained in the satellite data. For comparison, hundreds of carefully selected spectral, spatial, textural, and temporal features that are optimal for crop growth monitoring were extracted and fed into the same deep learning model. Our results demonstrated that (1) deep learning was able to predict yield directly using raw satellite imagery to the extent that was comparable to feature-fed deep learning approaches; (2) both 2D and 3D CNN models were able to explain nearly 90% variance in field-scale yield; (3) limited number of WV-3 outperformed multi-temporal PS data collected during entire growing season mainly attributed to RedEdge and SWIR bands available with WV-3; and (4) 3D CNN increased the prediction power of PS data compared to 2D CNN due to its ability to digest temporal features extracted from PS data.","Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,"WorldView-3, PlanetScope",Crop yield prediction,Agriculture,field-scale crop yield prediction using multi-temporal worldview-3 and planetscope satellite data and deep learning,Regression,no,,,,,,,,,,,,,,,
J,"Fernandez-Beltran, R; Baidar, T; Kang, J; Pla, F",Rice-Yield Prediction with Multi-Temporal Sentinel-2 Data and 3D CNN: A Case Study in Nepal,REMOTE SENSING,2021,Sentinel-2; rice-yield estimation; regression; deep learning; Nepal,"Crop yield estimation is a major issue of crop monitoring which remains particularly challenging in developing countries due to the problem of timely and adequate data availability. Whereas traditional agricultural systems mainly rely on scarce ground-survey data, freely available multi-temporal and multi-spectral remote sensing images are excellent tools to support these vulnerable systems by accurately monitoring and estimating crop yields before harvest. In this context, we introduce the use of Sentinel-2 (S2) imagery, with a medium spatial, spectral and temporal resolutions, to estimate rice crop yields in Nepal as a case study. Firstly, we build a new large-scale rice crop database (RicePAL) composed by multi-temporal S2 and climate/soil data from the Terai districts of Nepal. Secondly, we propose a novel 3D Convolutional Neural Network (CNN) adapted to these intrinsic data constraints for the accurate rice crop yield estimation. Thirdly, we study the effect of considering different temporal, climate and soil data configurations in terms of the performance achieved by the proposed approach and several state-of-the-art regression and CNN-based yield estimation methods. The extensive experiments conducted in this work demonstrate the suitability of the proposed CNN-based framework for rice crop yield estimation in the developing country of Nepal using S2 data.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Rice-yield prediction,Vegetation,rice-yield prediction with multi-temporal sentinel-2 data and 3d cnn: a case study in nepal,Regression,no,,,,,,,,,,,,0.9526,107.26,,Cropy yield (kg/ha)
J,"Adrian, J; Sagan, V; Maimaitijiang, M",Sentinel SAR-optical fusion for crop type mapping using deep learning and Google Earth Engine,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2021,3D U-Net; Denoising neural networks; Sentinel-1; Sentinel-2; Data fusion,"Accurate crop type mapping provides numerous benefits for a deeper understanding of food systems and yield prediction. Ever-increasing big data, easy access to high-resolution imagery, and cloud-based analytics platforms like Google Earth Engine have drastically improved the ability for scientists to advance data-driven agriculture with improved algorithms for crop type mapping using remote sensing, computer vision, and machine learning. Crop type mapping techniques mainly relied on standalone SAR and optical imagery, few studies investigated the potential of SAR-optical data fusion, coupled with virtual constellation, and 3-dimensional (3D) deep learning networks. To this extent, we use a deep learning approach that utilizes the denoised backscatter and texture information from multi-temporal Sentinel-1 SAR data and the spectral information from multi-temporal optical Sentinel-2 data for mapping ten different crop types, as well as water, soil and urban area. Multi-temporal Sentinel-1 data was fused with multi-temporal optical Sentinel-2 data in an effort to improve classification accuracies for crop types. We compared the results of the 3D U-Net to the state-of-the-art deep learning networks, including SegNet and 2D U-Net, as well as commonly used machine learning method such as Random Forest. The results showed (1) fusing multi-temporal SAR and optical data yields higher training overall accuracies (OA) (3D U-Net 0.992, 2D U-Net 0.943, SegNet 0.871) and testing OA (3D U-Net 0.941, 2D U-Net 0.847, SegNet 0.643) for crop type mapping compared to standalone multi-temporal SAR or optical data (2) optical data fused with denoised SAR data via a denoising convolution neural network (OA 0.912) performed better for crop type mapping compared to optical data fused with boxcar (OA 0.880), Lee (OA 0.881), and median (OA 0.887) filtered SAR data and (3) 3D convolutional neural networks perform better than 2D convolutional neural networks for crop type mapping (SAR OA 0.912, optical OA 0.937, fused OA 0.992).","Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,Sentinel-2,Crop type mapping,Vegetation,sentinel sar-optical fusion for crop type mapping using deep learning and google earth engine,Classification,yes,94.1,,,,,,,,,,,,,,
C,Qadeer M.U.; Saeed S.; Taj M.; Muhammad A.,SPATIO-TEMPORAL CROP CLASSIFICATION ON VOLUMETRIC DATA,"Proceedings - International Conference on Image Processing, ICIP",2021,CNN; Crop Classification; Satellite data,"Large-area crop classification using multi-spectral imagery is a widely studied problem for several decades and is generally addressed using classical Random Forest classifier. Recently, deep convolutional neural networks (DCNN) have been proposed. However, these methods only achieved results comparable with Random Forest. In this work, we present a novel CNN based architecture for large-area crop classification. Our methodology combines both spatio-temporal analysis via 3D CNN as well as temporal analysis via 1D CNN. We evaluated the efficacy of our approach on Yolo and Imperial county benchmark datasets. Our combined strategy outperforms both classical as well as recent DCNN based methods in terms of classification accuracy by 2% while maintaining a minimum number of parameters and the lowest inference time. Â© 2021 IEEE",Conference paper,,yes,"Sentinel-2, Landsat-8",Crop classification,Agriculture,spatio-temporal crop classification on volumetric data,Classification,yes,90.23,90.08,,,,,,,,,,,,,
J,"Aldabbagh, YAN; Shafri, HZM; Mansor, S; Ismail, MH","Desertification prediction with an integrated 3D convolutional neural network and cellular automata in Al-Muthanna, Iraq",ENVIRONMENTAL MONITORING AND ASSESSMENT,2022,Desertification prediction; Convolutional neural networks; Cellular automata; Al-Muthanna,"Desertification is a major environmental issue all over the world, and Al-Khidhir district, Al-Muthanna, in the south of Iraq is no exception. In mapping, assessing, and predicting desertification, remote sensing and geospatial solutions (spatial analysis, machine learning) are crucial. During 1998-2018, this study employed satellite images from Landsat TM, ETM +, and OLI to map and predict desertification in the Al-Khidhir district. The year 2028 was chosen as the target date. Prediction models were constructed using a 3D convolutional neural network (3D CNN) and cellular automata (CA) techniques. In addition to the historical land cover maps, the model incorporated desertification indicators identified as important in the study, including geology, soil type, distance from waterways, elevation, population density, and Normalized Difference Vegetation Index (NDVI). Several accuracy metrics were used to evaluate the models, including overall accuracy (OA), average accuracy (AA), and the Kappa index (K). The simulated and actual land cover maps from 1998 and 2008 were used to evaluate the desertification prediction models. The 3D CNN model outperforms the typical 2D CNN for both the 2008 and 2018 images, according to the results. For the 2008 image, the 3D CNN model achieved 89.675 OA, 69.946 AA, and 0.781 K, while the 2018 image achieved 91.494 OA, 75.138 AA, and 0.770 K. The 2D CNN model performed a little worse than the 3D CNN model. The results of the change assessment showed that between 1998 and 2008, agricultural land was the dominant class (39%, 47.4%, respectively). The bare land, however, was the most dominant class in 2018, accounting for 46.6% of the total, compared to 26.2% for agricultural land. The spatial distribution characteristics of desertification in the Al-Khidhir, in the year 1998, were prevalent in the area's south (25.9%). In the following 10 years, desertification has spread to the surrounding territories. In the year 2008, desertification increased in the north of the study area (50.8%). Unless the local administration of Al-Khidhir district establishes desertification control strategies, this study suggests that the extent of bare land could expand in 2028 (54.1%).",Environmental Sciences,Environmental Sciences & Ecology,no,Landsat-7,Desertification prediction,Geohazards,"desertification prediction with an integrated 3d convolutional neural network and cellular automata in al-muthanna, iraq",Classification,no,89.68,,,,,,,78.1,,,,,,,
J,"Aldabbagh, YAN; Shafri, HZM; Mansor, S; Ismail, MH","Desertification prediction with an integrated 3D convolutional neural network and cellular automata in Al-Muthanna, Iraq",ENVIRONMENTAL MONITORING AND ASSESSMENT,2022,Desertification prediction; Convolutional neural networks; Cellular automata; Al-Muthanna,"Desertification is a major environmental issue all over the world, and Al-Khidhir district, Al-Muthanna, in the south of Iraq is no exception. In mapping, assessing, and predicting desertification, remote sensing and geospatial solutions (spatial analysis, machine learning) are crucial. During 1998-2018, this study employed satellite images from Landsat TM, ETM +, and OLI to map and predict desertification in the Al-Khidhir district. The year 2028 was chosen as the target date. Prediction models were constructed using a 3D convolutional neural network (3D CNN) and cellular automata (CA) techniques. In addition to the historical land cover maps, the model incorporated desertification indicators identified as important in the study, including geology, soil type, distance from waterways, elevation, population density, and Normalized Difference Vegetation Index (NDVI). Several accuracy metrics were used to evaluate the models, including overall accuracy (OA), average accuracy (AA), and the Kappa index (K). The simulated and actual land cover maps from 1998 and 2008 were used to evaluate the desertification prediction models. The 3D CNN model outperforms the typical 2D CNN for both the 2008 and 2018 images, according to the results. For the 2008 image, the 3D CNN model achieved 89.675 OA, 69.946 AA, and 0.781 K, while the 2018 image achieved 91.494 OA, 75.138 AA, and 0.770 K. The 2D CNN model performed a little worse than the 3D CNN model. The results of the change assessment showed that between 1998 and 2008, agricultural land was the dominant class (39%, 47.4%, respectively). The bare land, however, was the most dominant class in 2018, accounting for 46.6% of the total, compared to 26.2% for agricultural land. The spatial distribution characteristics of desertification in the Al-Khidhir, in the year 1998, were prevalent in the area's south (25.9%). In the following 10 years, desertification has spread to the surrounding territories. In the year 2008, desertification increased in the north of the study area (50.8%). Unless the local administration of Al-Khidhir district establishes desertification control strategies, this study suggests that the extent of bare land could expand in 2028 (54.1%).",Environmental Sciences,Environmental Sciences & Ecology,no,Landsat-8,Desertification prediction,Geohazards,"desertification prediction with an integrated 3d convolutional neural network and cellular automata in al-muthanna, iraq",Classification,no,91.49,,,,,,,77,,,,,,,
C,"Ibrahim, MR; Benavente, R; Lumbreras, F; Ponsa, D",3DRRDB: Super Resolution of Multiple Remote Sensing Images using 3D Residual in Residual Dense Blocks,2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW 2022),2022,,"The rapid advancement of Deep Convolutional Neural Networks helped in solving many remote sensing problems, especially the problems of super-resolution. However, most state-of-the-art methods focus more on Single Image Super-Resolution neglecting Multi-Image Super-Resolution. In this work, a new proposed 3D Residual in Residual Dense Blocks model (3DRRDB) focuses on remote sensing Multi-Image Super-Resolution for two different single spectral bands. The proposed 3DRRDB model explores the idea of 3D convolution layers in deeply connected Dense Blocks and the effect of local and global residual connections with residual scaling in Multi-Image Super-Resolution. The model tested on the Proba-V challenge dataset shows a significant improvement above the current state-of-the-art models scoring a Corrected Peak Signal to Noise Ratio (cPSNR) of 48.79 dB and 50.83 dB for Near Infrared (NIR) and RED Bands respectively. Moreover, the proposed 3DRRDB model scores a Corrected Structural Similarity Index Measure (cSSIM) of 0.9865 and 0.9909 for NIR and RED bands respectively.","Computer Science, Artificial Intelligence; Computer Science, Theory & Methods",Computer Science,yes,PROBA-V,Improvement of the satellite image quality,Others,3drrdb: super resolution of multiple remote sensing images using 3d residual in residual dense blocks,Regression,yes,,,,,,,,,,,,,,,
C,Igeta T.; Iwasaki A.,An Unsupervised Network for Stereo Matching of Very High Resolution Satellite Imagery,International Geoscience and Remote Sensing Symposium (IGARSS),2022,disparity estimation; high-resolution remote sensing imagery; stereo matching; unsupervised learning,"Convolutional Neural Networks (CNN) were recently applied to stereo matching, which is one of the important steps in 3D reconstruction. In remote sensing using Very High Resolution (VHR) satellite imagery, CNN-based stereo matching is employed in Digital Surface Models (DSM) generation, achieving higher accuracy than conventional methods through supervised learning. Since ground-truth disparity maps calculated from reliable elevation data are hard to collect, unsupervised stereo matching methods are as significant as supervised ones. However, there are few studies on stereo matching of VHR satellite imagery with unsupervised learning. In this work, we propose an unsupervised stereo matching network for DSM generation. We also introduce a criterion to select the best epoch without using ground-truth data for validation in a training strategy of gradually increasing the weight of a smoothness loss. Experimental results show that our network performs better in the average endpoint error and the fraction of erroneous pixels than the baseline method of the used dataset without ground-truth data. Â© 2022 IEEE.",Conference paper,,yes,WorldView-3,Stereo matching of VHR satellite imagery,Others,an unsupervised network for stereo matching of very high resolution satellite imagery,Regression,yes,,,,,,,,,,,,,,,
J,Z. Zhu; Y. Tao; X. Luo,HCNNet: A Hybrid Convolutional Neural Network for Spatiotemporal Image Fusion,IEEE Transactions on Geoscience and Remote Sensing,2022,Feature fusion;hybrid convolution (Conv);spatiotemporal fusion (STF);spectral correlation,"In recent years, leaps and bounds have developed spatiotemporal fusion (STF) methods for remote sensing (RS) images based on deep learning. However, most existing methods use 2-D convolution (Conv) to explore features. 3-D Conv can explore time-dimensional features, but it requires more memory footprint and is rarely used. In addition, the current STF methods based on convolutional neural networks (CNNs) are mainly the following two: 1) use 2-D Conv to extract features from multiple bands of the input image together and fuse the features to predict the multiband image directly and 2) use 2-D Conv to extract features from individual bands of the image, predict the reflectance data of individual bands, and finally stack the predicted individual bands directly to synthesize the multiband image. The former method does not sufficiently consider the spectral and reflectance differences between different bands, and the latter does not consider the similarity of spatial structures between adjacent bands and the spectral correlation. To solve these problems, we propose a 2-D/3-D hybrid CNN called HCNNet, in which the 2D-CNN branch extracts the spatial information features of single-band image, and the 3D-CNN branch extracts spatiotemporal features of single-band images. After fusing the features of the dual branches, we introduce neighboring band features to share spatial information so that the information is complementary to obtain single-band features and images, and finally stack each single-band image to generate multiband images. Visual assessment and metric evaluation of the three publicly available datasets showed that our method predicted better images compared with the five methods.",IEEE Journals,,yes,"Landsat-8, MODIS",Spatiotemporal Image Fusion,Others,hcnnet: a hybrid convolutional neural network for spatiotemporal image fusion,Regression,yes,,,,,,,,,,,,0.9906,0.0258,,Reflectance
J,"Zhang, E; Fu, YH; Wang, J; Liu, L; Yu, K; Peng, JY",MSAC-Net: 3D Multi-Scale Attention Convolutional Network for Multi-Spectral Imagery Pansharpening,REMOTE SENSING,2022,deep learning; multi-spectral image; 3D convolutional; multi-scale cost,"Pansharpening fuses spectral information from the multi-spectral image and spatial information from the panchromatic image, generating super-resolution multi-spectral images with high spatial resolution. In this paper, we proposed a novel 3D multi-scale attention convolutional network (MSAC-Net) based on the typical U-Net framework for multi-spectral imagery pansharpening. MSAC-Net is designed via 3D convolution, and the attention mechanism replaces the skip connection between the contraction and expansion pathways. Multiple pansharpening layers at the expansion pathway are designed to calculate the reconstruction results for preserving multi-scale spatial information. The MSAC-Net performance is verified on the IKONOS and QuickBird satellites' datasets, proving that MSAC-Net achieves comparable or superior performance to the state-of-the-art methods. Additionally, 2D and 3D convolution are compared, and the influences of the number of convolutions in the convolution block, the weight of multi-scale information, and the network's depth on the network performance are analyzed.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,QuickBird,MS imagery pansharpening,Others,msac-net: 3d multi-scale attention convolutional network for multi-spectral imagery pansharpening,Regression,no,,,,,,,,,,,,0.7868,0.0975,,Similarity of spectral features
J,"Azeez, OS; Shafri, HZM; Alias, AH; Haron, NA",A Joint Bayesian Optimization for the Classification of Fine Spatial Resolution Remotely Sensed Imagery Using Object-Based Convolutional Neural Networks,LAND,2022,object-based convolution neural networks; deep learning; Bayesian optimization; decision-level fusion; Worldview-3,"In recent years, deep learning-based image classification has become widespread, especially in remote sensing applications, due to its automatic and strong feature extraction capability. However, as deep learning methods operate on rectangular-shaped image patches, they cannot accurately extract objects' boundaries, especially in complex urban settings. As a result, combining deep learning and object-based image analysis (OBIA) has become a new avenue in remote sensing studies. This paper presents a novel approach for combining convolutional neural networks (CNN) with OBIA based on joint optimization of segmentation parameters and deep feature extraction. A Bayesian technique was used to find the best parameters for the multiresolution segmentation (MRS) algorithm while the CNN model learns the image features at different layers, achieving joint optimization. The proposed classification model achieved the best accuracy, with 0.96 OA, 0.95 Kappa, and 0.96 mIoU in the training area and 0.97 OA, 0.96 Kappa, and 0.97 mIoU in the test area, outperforming several benchmark methods including Patch CNN, Center OCNN, Random OCNN, and Decision Fusion. The analysis of CNN variants within the proposed classification workflow showed that the HybridSN model achieved the best results compared to 2D and 3D CNNs. The 3D CNN layers and combining 3D and 2D CNN layers (HybridSN) yielded slightly better accuracies than the 2D CNN layers regarding geometric fidelity, object boundary extraction, and separation of adjacent objects. The Bayesian optimization could find comparable optimal MRS parameters for the training and test areas, with excellent quality measured by AFI (0.046, -0.037) and QR (0.945, 0.932). In the proposed model, higher accuracies could be obtained with larger patch sizes (e.g., 9 x 9 compared to 3 x 3). Moreover, the proposed model is computationally efficient, with the longest training being fewer than 25 s considering all the subprocesses and a single training epoch. As a result, the proposed model can be used for urban and environmental applications that rely on VHR satellite images and require information about land use.",Environmental Studies,Environmental Sciences & Ecology,yes,Worldview-3,Urban land-use and land-cover,Urban,a joint bayesian optimization for the classification of fine spatial resolution remotely sensed imagery using object-based convolutional neural networks,Classification,no,95,,,,,,,94,,95,,,,,
J,"Yang, ZW; Zhang, HB; Lyu, XX; Du, WB","Improving Typical Urban Land-Use Classification with Active-Passive Remote Sensing and Multi-Attention Modules Hybrid Network: A Case Study of Qibin District, Henan, China",SUSTAINABILITY,2022,land-use classification; convolutional neural networks (CNN); active and passive remote sensing; data fusion,"The study of high-precision land-use classification is essential for the sustainable development of land resources. This study addresses the problem of classification errors in optical remote-sensing images under high surface humidity, cloud cover, and hazy weather. The synthetic aperture radar (SAR) images are sensitive to soil moisture, and the microwave can penetrate clouds, haze, and smoke. By using both the active and passive remote-sensing data, the Sentinel-1A SAR and Sentinel-2B multispectral (MS) images are combined synergistically. The full-band data combining the SAR + MS + spectral indexes is thus constructed. Based on the high dimensionality and heterogeneity of this data set, a new framework (MAM-HybridNet) based on two-dimensional (2D) and three-dimensional (3D) hybrid convolutional neural networks combined with multi-attention modules (MAMs) is proposed for improving the accuracy of land-use classification in cities with high surface humidity. In addition, the same training samples supported by All bands data (SAR + MS + spectral index) are selected and compared with k-Nearest Neighbors (KNN), support vector machine (SVM), 2D convolutional neural networks, 3D convolutional neural networks, and hybridSN classification models to verify the accuracy of the proposed classification model. The results show that (1) fusion classification based on Sentinel-2B MSI and Sentinel-1A SAR data produce an overall accuracy (OA) of 95.10%, a kappa coefficient (KC) of 0.93, and an average accuracy (AA) of 92.86%, which is better than the classification results using Sentinel-2B MSI and Sentinel-1A SAR images separately. (2) The classification accuracy improves upon adding the spectral index, and the OA, KC, and AA improve by 3.77%, 0.05, and 5.5%, respectively. (3) With the support of full-band data, the algorithm proposed herein produces better results than other classification algorithms, with an OA of 98.87%, a KC of 0.98, and an AA of 98.36%. These results indicate that the synergistic effect of active-passive remote-sensing data improves land-use classification. Additionally, the results verify the effectiveness of the proposed deep-learning classification model for land-use classification.",Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies,Science & Technology - Other Topics; Environmental Sciences & Ecology,yes,Sentinel-2,Urban Land-Use Classification,Urban,"improving typical urban land-use classification with active-passive remote sensing and multi-attention modules hybrid network: a case study of qibin district, henan, china",Classification,no,96.76,,,,,,,95.71,,,,,,,
C,"Voelsen, M; Teimouri, M; Rottensteiner, F; Heipke, C",INVESTIGATING 2D AND 3D CONVOLUTIONS FOR MULTITEMPORAL LAND COVER CLASSIFICATION USING REMOTE SENSING IMAGES,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",2022,land cover classification; remote sensing; FCN; multi-temporal images; 3D-CNN,"With the availability of large amounts of satellite image time series (SITS), the identification of different materials of the Earth's surface is possible with a high temporal resolution. One of the basic tasks is the pixel-wise classification of land cover, i.e. the task of identifying the physical material of the Earth's surface in an image. Fully convolutional neural networks (FCN) are successfully used for this task. In this paper, we investigate different FCN variants, using different methods for the computation of spatial, spectral, and temporal features. We investigate the impact of 3D convolutions in the spatial-temporal as well as in the spatial-spectral dimensions in comparison to 2D convolutions in the spatial dimensions only. Additionally, we introduce a new method to generate multitemporal input patches by using time intervals instead of fixed acquisition dates. We then choose the image that is closest in time to the middle of the corresponding time interval, which makes our approach more flexible with respect to the requirements for the acquisition of new data. Using these multi-temporal input patches, generated from Sentinel-2 images, we improve the classification of land cover by 4% in the mean F1-score and 1.3% in the overall accuracy compared to a classification using mono-temporal input patches. Furthermore, the usage of 3D convolutions instead of 2D convolutions improves the classification performance by a small amount of 0.4% in the mean F1-score and 1.2% in the overall accuracy.","Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology",Physical Geography; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Land cover classification,Urban,investigating 2d and 3d convolutions for multitemporal land cover classification using remote sensing images,Classification,yes,88.1,79.7,,,,,,,,,,,,,
J,"Li, R; Zheng, SY; Duan, CX; Wang, LB; Zhang, C",Land cover classification from remote sensing images based on multi-scale fully convolutional network,GEO-SPATIAL INFORMATION SCIENCE,2022,Spatio-temporal remote sensing images; Multi-Scale Fully Convolutional Network; land cover classification,"Although the Convolutional Neural Network (CNN) has shown great potential for land cover classification, the frequently used single-scale convolution kernel limits the scope of information extraction. Therefore, we propose a Multi-Scale Fully Convolutional Network (MSFCN) with a multi-scale convolutional kernel as well as a Channel Attention Block (CAB) and a Global Pooling Module (GPM) in this paper to exploit discriminative representations from two-dimensional (2D) satellite images. Meanwhile, to explore the ability of the proposed MSFCN for spatio-temporal images, we expand our MSFCN to three-dimension using three-dimensional (3D) CNN, capable of harnessing each land cover category's time series interaction from the reshaped spatio-temporal remote sensing images. To verify the effectiveness of the proposed MSFCN, we conduct experiments on two spatial datasets and two spatiotemporal datasets. The proposed MSFCN achieves 60.366% on the WHDLD dataset and 75.127% on the GID dataset in terms of mIoU index while the figures for two spatiotemporal datasets are 87.753% and 77.156%. Extensive comparative experiments and ablation studies demonstrate the effectiveness of the proposed MSFCN. Code will be available at https://github.com/lironui/MSFCN.",Remote Sensing,Remote Sensing,yes,Gaofen-2,Land cover classification,Vegetation,land cover classification from remote sensing images based on multi-scale fully convolutional network,Segmentation,yes,97.458,89.4945,,,,,,94.689,,82.4545,,,,,
C,Meshkini K.; Bovolo F.; Bruzzone L.,A 3D CNN APPROACH FOR CHANGE DETECTION IN HR SATELLITE IMAGE TIME SERIES BASED ON A PRETRAINED 2D CNN,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",2022,3D Convolutional Neural Network (CNN); Change Detection (CD); Change Vector Analysis (CVA); Deep Learning; High Resolution (HR) Image; Transfer Learning,"Over recent decades, Change Detection (CD) has been intensively investigated due to the availability of High Resolution (HR) multi-spectral multi-temporal remote sensing images. Deep Learning (DL) based methods such as Convolutional Neural Network (CNN) have recently received increasing attention in CD problems demonstrating high potential. However, most of the CNN-based CD methods are designed for bi-temporal image analysis. Here, we propose a Three-Dimensional (3D) CNN-based CD approach that can effectively deal with HR image time series and process spatial-spectral-temporal features. The method is unsupervised and thus does not require the complex task of collecting labelled multi-temporal data. Since there are only a few pretrained 3D CNNs available that are not suitable for remote sensing CD analysis, the proposed approach starts with a pretrained 2D CNN architecture trained on remote sensing images for semantic segmentation and develops a 3D CNN architecture using a transfer learning technique to jointly deal with spatial, spectral and temporal information. A layerwise feature reduction strategy is performed to select the most informative features and a pixelwise year-based Change Vector Analysis (CVA) is employed to identify changed pixels. Experimental results on a long time series of Landsat 8 images for an area located in Saudi Arabia confirm the effectiveness of the proposed approach.  Â© Authors 2022",Conference paper,,yes,Landsat-8,Change detection,Vegetation,a 3d cnn approach for change detection in hr satellite image time series based on a pretrained 2d cnn,Segmentation,yes,,,,,,,,,,,,,,,
J,"Seydi, ST; Amani, M; Ghorbanian, A",A Dual Attention Convolutional Neural Network for Crop Classification Using Time-Series Sentinel-2 Imagery,REMOTE SENSING,2022,crop mapping; deep learning; convolutional neural networks (CNN); attention modules (AM); dual attention CNN; Sentinel-2; multi-temporal,"Accurate and timely mapping of crop types and having reliable information about the cultivation pattern/area play a key role in various applications, including food security and sustainable agriculture management. Remote sensing (RS) has extensively been employed for crop type classification. However, accurate mapping of crop types and extents is still a challenge, especially using traditional machine learning methods. Therefore, in this study, a novel framework based on a deep convolutional neural network (CNN) and a dual attention module (DAM) and using Sentinel-2 time-series datasets was proposed to classify crops. A new DAM was implemented to extract informative deep features by taking advantage of both spectral and spatial characteristics of Sentinel-2 datasets. The spectral and spatial attention modules (AMs) were respectively applied to investigate the behavior of crops during the growing season and their neighborhood properties (e.g., textural characteristics and spatial relation to surrounding crops). The proposed network contained two streams: (1) convolution blocks for deep feature extraction and (2) several DAMs, which were employed after each convolution block. The first stream included three multi-scale residual convolution blocks, where the spectral attention blocks were mainly applied to extract deep spectral features. The second stream was built using four multi-scale convolution blocks with a spatial AM. In this study, over 200,000 samples from six different crop types (i.e., alfalfa, broad bean, wheat, barley, canola, and garden) and three non-crop classes (i.e., built-up, barren, and water) were collected to train and validate the proposed framework. The results demonstrated that the proposed method achieved high overall accuracy and a Kappa coefficient of 98.54% and 0.981, respectively. It also outperformed other state-of-the-art classification methods, including RF, XGBOOST, R-CNN, 2D-CNN, 3D-CNN, and CBAM, indicating its high potential to discriminate different crop types.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Crop classification,Agriculture,a dual attention convolutional neural network for crop classification using time-series sentinel-2 imagery,Classification,no,97.45,,,,,,,96.8,,,,,,,
C,"Phan, A; Takejima, K; Hirakawa, T; Fukui, H","FOREST-RELATED SDG ISSUES MONITORING FOR DATA-SCARCE REGIONS EMPLOYING MACHINE LEARNING AND REMOTE SENSING - A CASE STUDY FOR ENA CITY, JAPAN",2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,Forest; Tree species; Tree age; SDG; CNN; Sentinel 1/2; 3D Atrous Convolution; Ena City,"We proposed a combined machine learning approach with a deep convolutional neural network (CNN) to monitor forest utilization toward Sustainable Development Goals (SDGs) for data-scarce regions. First, we employed the Random Forest (RF) classifier using Google Earth Engine (GEE) for forest mapping. Then, we designed a deep CNN architecture that works for tree species/age mapping from coarse and polygonal ground-truth data. The proposed network has U-shape and comprises 3D Atrous Convolutions. The model was optimized by a weighted cross-entropy loss function. We trained the model with times-series Sentinel 1, 2, and Digital Elevation Model (DEM) data with sparse annotations. Our proposed models achieved 94.5% overall accuracy (OA) for forest mapping, 77.80% (OA) for tree species, and 81.74% (OA) for tree age classification, respectively in Ena city, Japan. The outcome of our study indicates the potential of remote sensing and machine learning in monitoring forest development, conservation, and utilization toward SDGs from coarse ground-truth data. Our source code for the implementation is available at: https://github.com/anhp95/forest_attr_segment","Geosciences, Multidisciplinary; Remote Sensing",Geology; Remote Sensing,yes,Sentinel-2,Land cover map,Vegetation,"forest-related sdg issues monitoring for data-scarce regions employing machine learning and remote sensing - a case study for ena city, japan",Classification,yes,84.68,,,,,,,,,,,,,,
J,S. Yang; L. Gu; X. Li; F. Gao; T. Jiang,Fully Automated Classification Method for Crops Based on Spatiotemporal Deep-Learning Fusion Technology,IEEE Transactions on Geoscience and Remote Sensing,2022,Active learning;Crops;Classification algorithms;Data fusion;Deep learning;Training samples,"Accurate and timely crop mapping is essential for agricultural applications, and deep-learning methods have been applied on a range of remotely sensed data sources to classify crops. In this article, we develop a novel crop classification method based on spatiotemporal deep-learning fusion technology. However, for crop mapping, the selection and labeling of training samples is expensive and time consuming. Therefore, we propose a fully automated training-sample-selection method. First, we design the method according to image processing algorithms and the concept of a sliding window. Second, we develop the Geo-3D convolutional neural network (CNN) and Geo-Conv1D for crop classification using time-series Sentinel-2 imagery. Specifically, we integrate geographic information of crops into the structure of deep-learning networks. Finally, we apply an active learning strategy to integrate the classification advantages of Geo-3D CNN and Geo-Conv1D. Experiments conducted in Northeast China show that the proposed sampling method can reliably provide and label a large number of samples and achieve satisfactory results for different deep-learning networks. Based on the automatic selection and labeling of training samples, the crop classification method based on spatiotemporal deep-learning fusion technology can achieve the highest overall accuracy (OA) with approximately 92.50% as compared with Geo-Conv1D (91.89%) and Geo-3D CNN (91.27%) in the three study areas, indicating that the proposed method is effective and efficient in multi-temporal crop classification.",IEEE Journals,,yes,Sentinel-2,Crop classification,Agriculture,fully automated classification method for crops based on spatiotemporal deep-learning fusion technology,Classification,no,91.27,,,,,,,,,,,,,,
J,"Teimouri, M; Mokhtarzade, M; Baghdadi, N; Heipke, C",Fusion of time-series optical and SAR images using 3D convolutional neural networks for crop classification,GEOCARTO INTERNATIONAL,2022,Crop classification; fusion; time-series radar images; time-series optical images; 3D-CNN,"Remote sensing is a most promising technique for providing crop maps, thanks to the development of satellite images at various temporal and spatial resolutions. Three-dimensional (3D) convolutional neural networks (CNNs) have the potential to provide rich features that represent the spatial and temporal patterns of crops when applied to time series. This study presents a novel 3D-CNN framework for classifying crops that is based on the fusion of radar and optical time series and also fully exploits 3D spatial-temporal information. To extract deep convolutional maps, the proposed technique uses one separate sequence for each time series dataset. To determine the label of each pixel, the extracted feature maps are passed to the concatenating layer and subsequent transmitted to the sequential fully connected layers. The proposed approach not only takes advantage of CNNs, i.e. automatic feature extraction, but also discovers discriminative feature maps in both, spatial and temporal dimensions and preserves the growth dynamics of crop cycles. An overall accuracy of 91.3% and a kappa coefficient of 89.9% confirm the proposed method's potential. It is also shown that the suggested approach outperforms other methods.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,Sentinel-2,Crop classification,Agriculture,fusion of time-series optical and sar images using 3d convolutional neural networks for crop classification,Classification,no,91.3,,,,,,,89.9,,,,,,,
J,"Saralioglu, E; Gungor, O",Semantic segmentation of land cover from high resolution multispectral satellite images by spectral-spatial convolutional neural network,GEOCARTO INTERNATIONAL,2022,Remote sensing; classification; deep learning; semantic segmentation,"Research to improve the accuracy of very high-resolution satellite image classification algorithms is still one of the hot topics in the field of remote sensing. Successful results of deep learning methods in areas such as image classification and object detection have led to the application of these methods to remote sensing problems. Recently, Convolutional Neural Networks (CNNs) are among the most common deep learning methods used in image classification, however, the use of CNN's in satellite image classification is relatively new. Due to the high computational complexity of 3D CNNs, which aim to extract both spatial and spectral information, 2D CNNs focussing on the extraction of spatial information are often preferred. High-resolution satellite images, however, contain crucial spectral information as well as spatial information. In this study, a 3D-2D CNN model using both spectral and spatial information was applied to extract more accurate land cover information from very high-resolution satellite images. The model was applied on a Worldview-2 satellite image including agricultural product areas such as tea, hazelnut groves and land use classes such as buildings and roads. The results of the CNN based model were also compared against those of the Support Vector Machine (SVM) and Random Forest (RF) algorithms. The post-classification accuracies were obtained using 800 control points generated by a web interface created for crowdsourcing purposes. The classification accuracy was 95.6% for the 3D-2D CNN model, 89.2% for the RF and 86.4% for the SVM.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,WorldView-2,Semantic segmentation of land cover,Vegetation,semantic segmentation of land cover from high resolution multispectral satellite images by spectral-spatial convolutional neural network,Segmentation,yes,95.6,,,,,,,,,,,,,,
J,"Ball, JGC; Petrova, K; Coomes, DA; Flaxman, S",Using deep convolutional neural networks to forecast spatial patterns of Amazonian deforestation,METHODS IN ECOLOGY AND EVOLUTION,2022,Amazon; artificial intelligence; convolutional neural networks; deep learning; deforestation forecasting; machine learning; spatial forecasting; tropical forests,"1.Tropical forests are subject to diverse deforestation pressures while their conservation is essential to achieve global climate goals. Predicting the location of deforestation is challenging due to the complexity of the natural and human systems involved but accurate and timely forecasts could enable effective planning and on-the-ground enforcement practices to curb deforestation rates. New computer vision technologies based on deep learning can be applied to the increasing volume of Earth observation data to generate novel insights and make predictions with unprecedented accuracy. 2. Here, we demonstrate the ability of deep convolutional neural networks (CNNs) to learn spatiotemporal patterns of deforestation from a limited set of freely available global data layers, including multispectral satellite imagery, the Hansen maps of annual forest change (2001-2020) and the ALOS PALSAR digital surface model, to forecast deforestation (2021). We designed four model architectures, based on 2D CNNs, 3D CNNs, and Convolutional Long Short-Term Memory (ConvLSTM) Recurrent Neural Networks (RNNs), to produce spatial maps that indicate the risk to each forested pixel (similar to 30 m) in the landscape of becoming deforested within the next year. They were trained and tested on data from two similar to 80,000 km(2) tropical forest regions in the Southern Peruvian Amazon. 3. The networks could predict the location of future forest loss to a high degree of accuracy (F-1 = 0.58-0.71). Our best performing model (3D CNN) had the highest pixel-wise accuracy (F-1 = 0.71) when validated on 2020 forest loss (2014-2019 training). Visual interpretation of the mapped forecasts indicated that the network could automatically discern the drivers of forest loss from the input data. For example, pixels around new access routes (e.g. roads) were assigned high risk, whereas this was not the case for recent, concentrated natural loss events (e.g. remote landslides). 4. Convolutional neural networks can harness limited time-series data to predict near-future deforestation patterns, an important step in harnessing the growing volume of satellite remote sensing data to curb global deforestation. The modelling framework can be readily applied to any tropical forest location and used by governments and conservation organisations to prevent deforestation and plan protected areas.",Ecology,Environmental Sciences & Ecology,yes,Landsat-7,Deforestation,Geohazards,using deep convolutional neural networks to forecast spatial patterns of amazonian deforestation,Classification,no,,74.81,74.8,74.81,,,,,,,,,,,
J,"Zhou, XY; Zhou, WZ; Li, F; Shao, ZL; Fu, XL",Vegetation Type Classification Based on 3D Convolutional Neural Network Model: A Case Study of Baishuijiang National Nature Reserve,FORESTS,2022,vegetation types; classification; 3D convolutional neural network; Baishuijiang National Nature Reserve,"Efficient and accurate vegetation type extraction from remote sensing images can provide decision makers with basic forest cover and land use information, and provides a reliable basis for long-term monitoring. With the development of deep learning, the convolutional neural network (CNN) has been used successfully to classify tree species in many studies, but CNN models have rarely been applied in the classification of vegetation types on larger scales. To evaluate the performance of CNN models in the classification of vegetation types, this paper compared the classification accuracy of nine dominant land cover types in Baishuijiang National Nature Reserve with four models: 3D-CNN, 2D-CNN, JSSAN (joint spatial-spectral attention network) and Resnet18, using sentinel-2A data. Comparing the difference in classification accuracy between the direct use of raw sentinel images and fused feature indices sentinel images, the results showed that adding feature indices can improve the overall accuracy of the model. After fusing the characteristic bands, the accuracy of the four models was improved significantly, by 5.46-19.33%. The best performing 3D-CNN model achieved the highest classification accuracy with an overall accuracy of 95.82% and a kappa coefficient of 95.07%. In comparison, 2D-CNN achieved an overall accuracy of 79.07% and a kappa coefficient of 75.44%, JSSAN achieved an overall accuracy of 81.67% and a kappa coefficient of 78.56%, and Resnet18 achieved an overall accuracy of 93.61% and a kappa coefficient of 92.45%. The results showed that the 3D-CNN model can effectively capture vegetation type cover changes from broad-leaved forests at lower elevation, to shrublands and grasslands at higher elevation, across a range spanning 542-4007 m. In experiments using a small amount of sample data, 3D-CNN can better incorporate spatial-spectral information and is more effective in distinguishing the performance of spectrally similar vegetation types, providing an efficient and novel approach to classifying vegetation types in nature reserves with complex conditions.",Forestry,Forestry,yes,Sentinel-2,Vegetation type classification,Vegetation,vegetation type classification based on 3d convolutional neural network model: a case study of baishuijiang national nature reserve,Classification,no,95.82,,,,,,,95.07,,,,,,,
J,Jamali A.; Mahdianpari M.; Brisco B.; Mao D.; Salehi B.; Mohammadimanesh F.,3DUNetGSFormer: A deep learning pipeline for complex wetland mapping using generative adversarial networks and Swin transformer,Ecological Informatics,2022,Convolutional neural networks; Deep learning; Generative adversarial network; Swin transformer; Vision transformers; Wetland mapping,"Many ecosystems, particularly wetlands, are significantly degraded or lost as a result of climate change and anthropogenic activities. Simultaneously, developments in machine learning, particularly deep learning methods, have greatly improved wetland mapping, which is a critical step in ecosystem monitoring. Yet, present deep and very deep models necessitate a greater number of training data, which are costly, logistically challenging, and time-consuming to acquire. Thus, we explore and address the potential and possible limitations caused by the availability of limited ground-truth data for large-scale wetland mapping. To overcome this persistent problem for remote sensing data classification using deep learning models, we propose 3D UNet Generative Adversarial Network Swin Transformer (3DUNetGSFormer) to adaptively synthesize wetland training data based on each class's data availability. Both real and synthesized training data are then imported to a novel deep learning architecture consisting of cutting-edge Convolutional Neural Networks and vision transformers for wetland mapping. Results demonstrated that the developed wetland classifier obtained a high level of kappa coefficient, average accuracy, and overall accuracy of 96.99%, 97.13%, and 97.39%, respectively, for the data in three pilot sites in and around Grand Falls-Windsor, Avalon, and Gros Morne National Park located in Canada. The results show that the proposed methodology opens a new window for future high-quality wetland data generation and classification. The developed codes are available at https://github.com/aj1365/3DUNetGSFormer. Â© 2022 The Authors",Article,,yes,Sentinel-2,Wetland mapping,Water,3dunetgsformer: a deep learning pipeline for complex wetland mapping using generative adversarial networks and swin transformer,Classification,yes,97.39,,,,,,,96.99,,,,,,,
J,"Jamali, A; Mahdianpari, M; Mohammadimanesh, F; Homayouni, S",A deep learning framework based on generative adversarial networks and vision transformer for complex wetland classification using limited training samples,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,2022,Generative adversarial network; Convolutional neural network; Wetland classification; New Brunswick; Vision Transformer (ViT); Deep learning,"Wetlands have long been recognized among the most critical ecosystems globally, yet their numbers quickly diminish due to human activities and climate change. Thus, large-scale wetland monitoring is essential to provide efficient spatial and temporal insights for resource management and conservation plans. However, the main challenge is the lack of enough reference data for accurate large-scale wetland mapping. As such, the main objective of this study was to investigate the efficient deep-learning models for generating high-resolution and temporally rich training datasets for wetland mapping. The Sentinel-1 and Sentinel-2 satellites from the Euro-pean Copernicus program deliver radar and optical data at a high temporal and spatial resolution. These Earth observations provide a unique source of information for more precise wetland mapping from space. The second objective was to investigate the efficiency of vision transformers for complex landscape mapping. As such, we proposed a 3D Generative Adversarial Network (3D GAN) to best achieve these two objectives of synthesizing training data and a Vision Transformer model for large-scale wetland classification. The proposed approach was tested in three different study areas of Saint John, Sussex, and Fredericton, New Brunswick, Canada. The results showed the ability of the 3D GAN to stimulate and increase the number of training data and, as a result, increase the accuracy of wetland classification. The quantitative results also demonstrated the capability of jointly using data augmentation, 3D GAN, and Vision Transformer models with overall accuracy, average accuracy, and Kappa index of 75.61%, 73.4%, and 71.87%, respectively, using a disjoint data sampling strategy. Therefore, the proposed deep learning method opens a new window for large-scale remote sensing wetland classification.",Remote Sensing,Remote Sensing,yes,Sentinel-2,Wetland classification,Water,a deep learning framework based on generative adversarial networks and vision transformer for complex wetland classification using limited training samples,Classification,yes,75.61,,,,,,,71.81,,,,,,,
J,"Fei, TH; Huang, BH; Wang, X; Zhu, JX; Chen, Y; Wang, HZ; Zhang, WM",A Hybrid Deep Learning Model for the Bias Correction of SST Numerical Forecast Products Using Satellite Data,REMOTE SENSING,2022,SST; bias correction; deep learning; ConvLSTM; 3D-C BAM,"Sea surface temperature (SST) has important practical value in ocean related fields. Numerical prediction is a common method for forecasting SST at present. However, the forecast results produced by the numerical forecast models often deviate from the actual observation data, so it is necessary to correct the bias of the numerical forecast products. In this paper, an SST correction approach based on the Convolutional Long Short-Term Memory (ConvLSTM) network with multiple attention mechanisms is proposed, which considers the spatio-temporal relations in SST data. The proposed model is appropriate for correcting SST numerical forecast products by using satellite remote sensing data. The approach is tested in the region of the South China Sea and reduces the root mean squared error (RMSE) to 0.35 degrees C. Experimental results reveal that the proposed approach is significantly better than existing models, including traditional statistical methods, machine learning based methods, and deep learning methods.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,AVHRR,Bias Correction of SST (sea surface temperature),Water,a hybrid deep learning model for the bias correction of sst numerical forecast products using satellite data,Regression,yes,,,,,,,,,,,,,0.352,0.2641,SST (degrees)
J,"Seydi, ST; Saeidi, V; Kalantar, B; Ueda, N; van Genderen, JL; Maskouni, FH; Aria, FA",Fusion of the Multisource Datasets for Flood Extent Mapping Based on Ensemble Convolutional Neural Network (CNN) Model,JOURNAL OF SENSORS,2022,,"Floods, as one of the natural hazards, can affect the environment, damage the infrastructures, and threaten human lives. Due to climate change and anthropogenic activities, floods occur in high frequency all over the world. Therefore, mapping of the flood areas is of prime importance in disaster management. This research presents a novel framework for flood area mapping based on heterogeneous remote sensing (RS) datasets. The proposed framework fuses the synthetic aperture radar (SAR), optical, and altimetry datasets for mapping flood areas, and it is applied in three main steps: (1) preprocessing, (2) deep feature extraction based on multiscale residual kernel convolution and convolution neural network's (CNN) parameter optimization by fusing the datasets, and (3) flood detection based on the trained model. This research exploits two large-scale area datasets for mapping the flooded areas in Golestan and Khuzestan provinces, Iran. The results show that the proposed methodology has a high performance in flood area detection. The visual and numerical analyses verify the effectiveness and ability of the proposed method to detect the flood areas with an overall accuracy (OA) higher than 98% in both study areas. Finally, the efficiency of the designed architecture was verified by hybrid-CNN and 3D-CNN methods.","Engineering, Electrical & Electronic; Instruments & Instrumentation",Engineering; Instruments & Instrumentation,yes,Sentinel-2,Flood extent mapping,Water,fusion of the multisource datasets for flood extent mapping based on ensemble convolutional neural network (cnn) model,Classification,no,87.68,59.02,46.46,87.8,,,,,,,,,,,
J,"Wang, L; Wang, XY; Zhao, ZY; Wu, YX; Xu, JP; Zhang, HY; Yu, JB; Sun, Q; Bai, YT",MULTI-FACTOR STATUS PREDICTION BY 4D FRACTAL CNN BASED ON REMOTE SENSING IMAGES,FRACTALS-COMPLEX GEOMETRY PATTERNS AND SCALING IN NATURE AND SOCIETY,2022,Fractal Net; 4D CNN; Status Prediction; Remote Sensing Images; Eutrophication,"With the acceleration of industrialization and urbanization, most lakes and reservoirs have been in eutrophication state. Eutrophication of water body will produce a series of environmental problems, among which cyanobacteria bloom is one of the most studied and seriously polluted problems. It is of great significance to effectively control the occurrence of cyanobacteria blooms by predicting and simulating the outbreak process of cyanobacteria blooms and accurately forecasting the relevant governance departments. However, there are two problems in the existing analysis of algal blooms: on the one hand, it is difficult to consider the impact of other factors on cyanobacteria blooms by taking chlorophyll concentration as the main influencing factor, and it is also unable to determine the relationship between various factors. On the other hand, only based on the field monitoring data research, lack of comprehensive analysis of the whole water area. The remote sensing image can reflect the change of the whole water area, but the traditional analysis method is difficult to deal with the massive remote sensing data effectively. In this study, eutrophication level was used as characterization index of cyanobacteria bloom, and the remote sensing image and its inversion map were taken as the main research data, and a new method of cyanobacteria bloom prediction based on four-dimensional (4D) fractal CNN was proposed. The prediction model uses 4D fractal CNN to extract the features of multi factor remote sensing images, capture the temporal and spatial characteristics and the interaction between multiple factors, and predict the eutrophication level of water body. In this study, a total of 216 remote sensing images of Taihu Lake Basin were selected from 29 groups with fine weather from 2009 to 2010 obtained by MODIS satellite. The simulation results show that the method proposed in this paper has excellent prediction performance, and the accuracy rate of 85.71% is better than that of common 3D CNN and 4D CNN models.","Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences",Mathematics; Science & Technology - Other Topics,yes,MODIS ,Multi-factor status prediction,Water,multi-factor status prediction by 4d fractal cnn based on remote sensing images,Segmentation,no,64.29,,,,,,,,,,,,,,
J,"Jamali, A; Mahdianpari, M","Swin Transformer and Deep Convolutional Neural Networks for Coastal Wetland Classification Using Sentinel-1, Sentinel-2, and LiDAR Data",REMOTE SENSING,2022,Swin transformer; 3D convolutional neural network; coastal wetlands; New Brunswick; random forest; support vector machine; deep learning,"The use of machine learning algorithms to classify complex landscapes has been revolutionized by the introduction of deep learning techniques, particularly in remote sensing. Convolutional neural networks (CNNs) have shown great success in the classification of complex high-dimensional remote sensing imagery, specifically in wetland classification. On the other hand, the state-of-the-art natural language processing (NLP) algorithms are transformers. Although the transformers have been studied for a few remote sensing applications, the integration of deep CNNs and transformers has not been studied, particularly in wetland mapping. As such, in this study, we explore the potential and possible limitations to be overcome regarding the use of a multi-model deep learning network with the integration of a modified version of the well-known deep CNN network of VGG-16, a 3D CNN network, and Swin transformer for complex coastal wetland classification. Moreover, we discuss the potential and limitation of the proposed multi-model technique over several solo models, including a random forest (RF), support vector machine (SVM), VGG-16, 3D CNN, and Swin transformer in the pilot site of Saint John city located in New Brunswick, Canada. In terms of F-1 score, the multi-model network obtained values of 0.87, 0.88, 0.89, 0.91, 0.93, 0.93, and 0.93 for the recognition of shrub wetland, fen, bog, aquatic bed, coastal marsh, forested wetland, and freshwater marsh, respectively. The results suggest that the multi-model network is superior to other solo classifiers from 3.36% to 33.35% in terms of average accuracy. Results achieved in this study suggest the high potential for integrating and using CNN networks with the cutting-edge transformers for the classification of complex landscapes in remote sensing.","Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Wetland classification,Water,"swin transformer and deep convolutional neural networks for coastal wetland classification using sentinel-1, sentinel-2, and lidar data",Classification,no,85.38,85.64,88.55,83.82,,,,82.13,,,,,,,
J,"Liu, Mengmeng; Liu, Jiping; Xu, Shenghua; Chen, Cai; Bao, Shuai; Wang, Zhuolu; Du, Jun",3DCNN landslide susceptibility considering spatial-factor features,FRONTIERS IN ENVIRONMENTAL SCIENCE,2023,3DCNN; spatial-factor features; landslide susceptibility; trigger factos; scale comparison,"Effective landslide disaster risk management contributes to sustainable development. A useful method for emergency management and landslide avoidance is Landslide Susceptibility Mapping (LSM). The statistical landslide susceptibility prediction model based on slope unit ignores the re-lationship between landslide triggering factors and spatial characteristics. It disregards the influence of adjacent image elements around the slope-unit element. Therefore, this paper proposes a hardwired kernels-3DCNN approach to LSMs considering spatial-factor features. This method effectively solved the problem of low dimensionality of 3D convolution in the hazard factor layer by combining Prewitt operators to enhance the generation of multi-level 3D cube input data sets. The susceptibility value of the target area was then calculated using a 3D convolution to extract spatial and multi-factor features between them. A geospatial dataset of 402 landslides in Xiangxi Tujia and Miao Autonomous Prefecture, Hunan Province, China, was created for this study. Nine landslide trigger factors, including topography and geomorphology, stratigraphic lithology, rainfall, and human influences, were employed in the LSM. The research area's pixel points' landslide probabilities were then estimated by the training model, yielding the sensitivity maps. According to the results of this study, the 3DCNN model performs better when spatial information are included and trigger variables are taken into account, as shown by the high values of the area under the receiver operating characteristic curve (AUC) and other quantitative metrics. The proposed model outperforms CNN and SVM in AUC by 4.3% and 5.9%, respectively. Thus, the 3DCNN model, with the addition of spatial attributes, effectively improves the prediction accuracy of LSM. At the same time, this paper found that the model performance of the proposed method is related to the actual space size of the landslide body by comparing the impact of input data of different scales on the proposed method.",Environmental Sciences,Environmental Sciences & Ecology,yes,Landsat-8,Landslide suspectibility,Geohazards,3dcnn landslide susceptibility considering spatial-factor features,Classification,no,,,,,,,,,,,,,,,
J,"Zhao, Xin; Ma, Yi; Xiao, Yanfang; Liu, Jianqiang; Ding, Jing; Ye, Xiaomin; Liu, Rongjie",Atmospheric correction algorithm based on deep learning with spatial-spectral feature constraints for broadband optical satellites: Examples from the HY-1C Coastal Zone Imager,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2023,Atmospheric correction; Broadband optical satellite; HY-1C coastal zone imager (CZI); Convolution neural network (CNN); Spatial-spectral feature constraint,"Broadband optical satellites have been widely used for fine monitoring of coastal waters and inland lakes for their high spatial resolution. Atmospheric correction (AC) is one of the essential data processing steps for remote sensing of water environments with high spatial resolution satellites. Most broadband optical satellites, such as HY-1C Coastal Zone Imager (CZI) with a spatial resolution of 50 m, lack of near-infrared and shortwave infrared bands needed for atmospheric correction, making accurate AC difficult. Auxiliary aerosol data is often needed for accurate AC of broadband optical satellite data, which suffers from the differences in spatial resolution and imaging time between auxiliary data and satellite data. Furthermore, existing AC algorithms always perform AC pixel by pixel, which ignores the spatial relationship between adjacent pixels. Taking HY-1C CZI as an example, this paper proposes a novel atmospheric correction algorithm based on deep learning (SSACNet). Considering the inherent spatial-spectral features of satellite images, the SSACNet combines 2D and 3D convolution. The 3D convolution was used to mine the spatial and spectral features of the image and 2D convolution was explored to perform spatial information compensation. The SSACNet was trained and evaluated using the spatio-temporally synchronized dataset of HY-1C CZI and Landsat8 Operational Land Imager (OLI). The evaluation results by in-situ data show that the SSACNet has good performance, with the average correlation coefficient of 0.89, and the absolute percentage deviation (APD) of four bands ranging from 21.53 % to 35.41 %. Compared with the quasisynchronous Landsat8 OLI remote sensing reflectance (Rrs), the APD is less than 10 %. Compared with traditional atmospheric correction algorithms, SSACNet has significantly improved the spatial and spectral information fidelity. In addition, SSACNet also shows good applicability, as it can be used in both clear ocean waters and turbid coastal waters. This study lays a foundation for the quantitative remote sensing of water environment by broadband optical satellites.","Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,"Landsat-8, HY-1C",Atmospheric correction,Others,atmospheric correction algorithm based on deep learning with spatial-spectral feature constraints for broadband optical satellites: examples from the hy-1c coastal zone imager,Regression,yes,,,,,,,,,,,,,,,
J,Gallo I.; Ranghetti L.; Landro N.; La Grassa R.; Boschetti M.,In-season and dynamic crop mapping using 3D convolution neural networks and sentinel-2 time series,ISPRS Journal of Photogrammetry and Remote Sensing,2023,3D fully convolutive CNN; Crop mapping; Sentinel-2 time series; Short and long-term crop mapping,"An accurate, frequently updated, automatic and reproducible mapping procedure to identify seasonal cultivated crops is a prerequisite for many crop monitoring activities. Deep learning was demonstrated to be an effective mapping approach already successfully applied to decametric resolution satellite images (like Sentinel-2 data) to produce yearly crop maps. In this framework, algorithm training is performed with ground truth typically consisting of spatially explicit information available after the end of the season (e.g. yearly crop maps and/or farmer declaration for subsidies at parcel level); however, such data (i) does not allow performing in-season prediction, and (ii) does not provide temporal details fundamental to describe a dynamic crop succession and/or to understand crop management (i.e. planting and harvesting). In this paper we present a Deep Neural Network-based approach capable of generating (i) a crop map of the current season at a specific point in time (â€œIn season mappingâ€ conventionally at the end of the current year), along with (ii) all intermediate maps during the season able to describe in near real-time the evolution of crop presence (â€œDynamic-mappingâ€ at the temporal granularity of satellite imagery revisiting, e.g., 5 days for Sentinel-2 data). This approach adopts a smart training procedure of a Deep Neural model by exploiting historical satellite data and ground truth. We introduce a method to automatically generate â€œshort-termâ€ ground truth maps (i.e. 5 days reference) starting from the â€œlong-termâ€ ones (i.e. available yearly static reference) and characterizing temporally the different crop presence by performing a phenological analysis of historical time series. The model was trained and validated in Lombardy (North of Italy) exploiting multi-annual authoritative crop maps from 2016 to 2019. Validation was performed both in time (same areas used for training in a different year) and space (different location) for the year 2019. The quantitative error metrics calculation and Spatio-temporal analysis clearly demonstrate that the model can predict in-season crop presence with a generalization capacity over the long-term (yearly maps: OA > 70% and Kappa > 0.64%) and that the short-term predictions (5 days maps) are coherent with the reference information from expert knowledge (local crop calendars). The model can produce dynamically along the season short-term maps with a medium-high crop-specific User Accuracy at the maximum green-up phase (UA > 53% up to 95%). These products are of extreme interest for final users providing information at the peak of plant development that dynamically changes according to the considered crop, the specific location and the investigated season. These results demonstrate that it is possible to produce a crop map early in the season and extract useful additional information such as crop intensity (e.g. double crops presence) and crop dynamics related to different sowing dates. Â© 2022 The Authors",Article,,yes,Sentinel-2,Crop mapping,Agriculture,in-season and dynamic crop mapping using 3d convolution neural networks and sentinel-2 time series,Segmentation,yes,70.33,70.34,70.35,70.33,,,,64,,53.02,,,,,
J,S. M. M. Nejad; D. Abbasi-Moghadam; A. Sharifi; N. Farmonov; K. Amankulova; M. LÃ¡szlÅº,Multispectral Crop Yield Prediction Using 3D-Convolutional Neural Networks and Attention Convolutional LSTM Approaches,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2023,3D-CNN;ConvLSTM;forecasting;LSTM attention;skip connection,"In recent years, national economies are highly affected by crop yield predictions. By early prediction, the market price can be predicted, importing, and exporting plan can be provided, social, and economic effects of waste products can be minimized, and a program can be presented for humanitarian food aid. In addition, agricultural fields are constantly growing to generate products required. The use of machine learning (ML) methods in this sector can lead to the efficient production and high-quality agricultural products. Traditional predictive machine models were unable to find nonlinear relationships between data. Recently, there has been a revolution in prediction systems via the advancement of ML, which can be used to achieve highly accurate decision-making networks. Thus far, many strategies have been used to evaluate agricultural products, such as DeepYield, CNN-LSTM, and ConvLSTM. However, preferable prediction accuracy is required. In this study, two architectures have been proposed. The first model includes 2D-CNN, skip connections, and LSTM-Attentions. The second model comprises 3D-CNN, skip connections, and ConvLSTM Attention. The Input data given from MODIS products such as Land-Cover, Surface-Temperature, and MODIS-Land-surface from 2003 to 2018 on the county level over 1800 counties, where soybean is mainly cultivated in the USA. The proposed methods have been compared with the most recent models. Then, the results showed that the second proposed method notably outperformed the other techniques. In case of MAE, the second proposed method, DeepYield, ConvLSTM, 3DCNN, and CNN-LSTM obtained 4.3, 6.003, 6.05, 6.3, and 7.002, respectively.",IEEE Journals,,yes,MODIS,Crop yield prediction,Agriculture,multispectral crop yield prediction using 3d-convolutional neural networks and attention convolutional lstm approaches,Regression,yes,,,,,,,,,,,,0.78,5.934,4.3939,Prediction performances of crop yield forecasting at the county level
J,"Mohammadi, Sina; Belgiu, Mariana; Stein, Alfred",Improvement in crop mapping from satellite image time series by effectively supervising deep neural networks,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2023,Crop mapping; Deep learning; Fully convolutional neural networks; Supervised contrastive learning; Loss function; Time series,"Deep learning methods have achieved promising results in crop mapping using satellite image time series. A challenge still remains on how to better learn discriminative feature representations to detect crop types when the model is applied to unseen data. To address this challenge and reveal the importance of proper supervision of deep neural networks in improving performance, we propose to supervise intermediate layers of a designed 3D Fully Convolutional Neural Network (FCN) by employing two middle supervision methods: Cross-entropy loss Middle Supervision (CE-MidS) and a novel middle supervision method, namely Supervised Contrastive loss Middle Supervision (SupCon-MidS). This method pulls together features belonging to the same class in embedding space, while pushing apart features from different classes. We demonstrate that SupCon-MidS enhances feature discrimination and clustering throughout the network, thereby improving the network performance. In addition, we employ two output supervision methods, namely F1 loss and Intersection Over Union (IOU) loss. Our experiments on identifying corn, soybean, and the class Other from Landsat image time series in the U.S. corn belt show that the best set-up of our method, namely IOU+SupCon-MidS, is able to outperform the state-of-the-art methods by mIOU scores of 3.5% and 0.5% on average when testing its accuracy across a different year (local test) and different regions (spatial test), respectively. Further, adding SupCon-MidS to the output supervision methods improves mIOU scores by 1.2% and 7.6% on average in local and spatial tests, respectively. We conclude that proper supervision of deep neural networks plays a significant role in improving crop mapping performance. The code and data are available at: https://github.com/Sina-Mohammadi/CropSupervision.","Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"Landsat-7, Landsat-8",Crop mapping,Agriculture,improvement in crop mapping from satellite image time series by effectively supervising deep neural networks,Segmentation,yes,,,,,89.4,90.8,,,,81.8,,,,,
C,G. S. Phartiyal; L. S. Khangarot; D. Singh,Impact of Permuted Spectral Neighborhood of High-Dimensional Msts Rs Data on Crop Classification Performance with DNN Models,IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium,2023,localized spectral information;CNNs;multi-sensor;crop classification;time-series,"It is still a challenge for existing DNN based models to synergistically exploit the spatial, temporal, and especially spectral information of a crop present in multi-sensor time series (MSTS) remote sensing (RS) images and provide accurate crop classification while keeping the generalization ability of DNN models high. This imbalance requires investigation and demands novel CNN and RNN model-based approaches that can address the issue. The novel models proposed in this study involve the concepts of permuted localized spectral convolutions, localized spatial convolutions, and bi-directional recurrent units. The permuted spectral band stacking strategy is explored in this study to strengthen the influence of the spectral information. Overall, 6 models are proposed namely; Perm-1D-CNN, Perm-3D-CNN, Perm-RNN, Perm-1D-CRNN, Perm-2D-CRNN, and Perm-3D-CRNN. The qualitative and quantitative assessments reflect the higher generalization ability of the Perm-3D-CRNN along with its high classification accuracy. Also, the impact of spectral band permutations and localized spectral convolutions on the performance of DNN models is significant toward improved generalization.",Conference paper,Time series analysis;Stacking;Crops;Bidirectional control;Data models;Remote sensing,yes,Sentinel-2,Crop classification,Agriculture,impact of permuted spectral neighborhood of high-dimensional msts rs data on crop classification performance with dnn models,Classification,no,96.45,,,,,,,,,,,,,,
C,N. Wang; Z. Ma; P. Huo; X. Liu,Predicting Crop Yield Using 3D Convolutional Neural Network with Dimension Reduction and Metric Learning,2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),2023,crop yield prediction;metric learning;3D convolutional neural network;feature constraint;multitask learning,"Crop yield prediction using remote sensing data during the growing season is helpful to farm planning and management, which has received more and more attention. Information mining from multi-channel geo-spatiotemporal data brings many benefits to crop yield prediction. However, most of the existing methods have not fully utilized the dimension reduction technology and the spatiotemporal feature of the data. In this paper, a new approach is proposed to predict the yield from multi-spatial images by using the dimensionality reduction method and the 3D convolutional neural network. In addition, counties with similar crop yields should have similar features learned by the network. Thus, metric learning and multitask learning are used to learn more discriminating features. We evaluate the proposed method on county-level soybean yield prediction in the United States and the experimental results show the effectiveness of the proposed method.",Conference paper,Dimensionality reduction;Deep learning;Three-dimensional displays;Learning (artificial intelligence);Spatiotemporal phenomena;Planning;Convolutional neural networks,yes,MODIS,Crop yield prediction,Agriculture,predicting crop yield using 3d convolutional neural network with dimension reduction and metric learning,Regression,yes,,,,,,,,,,,,,5.468,,Cropy yield (kg/ha)
J,Wang N.; Ma Z.; Huo P.; Liu X.,3D Convolutional Neural Network with Dimension Reduction and Metric Learning for Crop Yield Prediction Based on Remote Sensing Data,,2023,crop yield prediction; metric learning; 3D convolutional neural network; feature constraint; multitask learning,"Crop yield prediction is essential for tasks like determining the optimal profile of crops to be planted, allocating government resources, effectively planning and preparing for aid distribution, making decisions about imports, and so on. Crop yield prediction using remote sensing data during the growing season is helpful to farm planning and management, which has received increasing attention. Information mining from multichannel geo-spatiotemporal data brings many benefits to crop yield prediction. However, most of the existing methods have not fully utilized the dimension reduction technology and the spatiotemporal feature of the data. In this paper, a new approach is proposed to predict the yield from multispatial images by using the dimension reduction method and a 3D convolutional neural network. In addition, regions with similar crop yields should have similar features learned by the network. Thus, metric learning and multitask learning are used to learn more discriminative features. We evaluated the proposed method on county-level soybean yield prediction in the United States, and the experimental results show the effectiveness of the proposed method. The proposed method provides new ideas for crop yield estimation and effectively improves the accuracy of crop yield estimation.","Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied",Chemistry; Engineering; Materials Science; Physics,yes,MODIS,Crop yield prediction,Agriculture,3d convolutional neural network with dimension reduction and metric learning for crop yield prediction based on remote sensing data,Regression,no,,,,,,,,,,,,,5.33,,Cropy yield (kg/ha)
J,"Wang, Li; Li, Wenhao; Wang, Xiaoyi; Xu, Jiping",Remote sensing image analysis and prediction based on improved Pix2Pix model for water environment protection of smart cities,PEERJ COMPUTER SCIENCE,2023,Prediction; Remote sensing; Image analysis; Pix2Pix model; Water environment; Smart cities; Artificial intelligence; Deep learning; Spatial-temporal data; Neural network,"Background. As an important part of smart cities, smart water environmental protection has become an important way to solve water environmental pollution problems. It is proposed in this article to develop a water quality remote sensing image analysis and prediction method based on the improved Pix2Pix (3D-GAN) model to overcome the problems associated with water environment prediction of smart cities based on remote sensing image data having low accuracy in predicting image information, as well as being difficult to train.Methods. Firstly, due to inversion differences and weather conditions, water quality remote sensing images are not perfect, which leads to the creation of time series data that cannot be used directly in prediction modeling. Therefore, a method for preprocessing time series of remote sensing images has been proposed in this article. The original remote sensing image was unified by pixel substitution, the image was repaired by spatial weight matrix, and the time series data was supplemented by linear interpolation. Secondly, in order to enhance the ability of the prediction model to process spatial-temporal data and improve the prediction accuracy of remote sensing images, the convolutional gated recurrent unit network is concatenated with the Unet network as the generator of the improved Pix2Pix model. At the same time, the channel attention mechanism is introduced into the convolutional gated recurrent unit network to enhance the ability of extracting image time series information, and the residual structure is introduced into the downsampling of the U-net network to avoid gradient explosion or disappearance. After that, the remote sensing images of historical moments are superimposed on the channels as labels and sent to the discriminator for adversarial training. The improved Pix2Pix model no longer translates images, but can predict two dimensions of space and one dimension of time, so it is actually a 3D-GAN model. Third, remote sensing image inversion data of chlorophyll-a concentrations in the Taihu Lake basin are used to verify and predict the water environment at future moments.Results. The results show that the mean value of structural similarity, peak signal-tonoise ratio, cosine similarity, and mutual information between the predicted value of the proposed method and the real remote sensing image is higher than that of existing methods, which indicates that the proposed method is effective in predicting water environment of smart cities.","Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods",Computer Science,yes,MODIS,Chlorophyll-a prediction,Water,remote sensing image analysis and prediction based on improved pix2pix model for water environment protection of smart cities,Regression,yes,,,,,,,,,,,,,,,
