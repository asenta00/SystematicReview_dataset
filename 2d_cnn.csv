Publication Type,Authors,Article Title,Source Title,Publication Year,Author Keywords,Abstract,Research Areas,Open Access,Satellite,Application,Domain,ML Technique,Algorithm Complexity,Accuracy (%),F1-score (%),Precision (%),Recall (%),Producer accuracy (%),User accuracy (%),Pixel accuracy,Kappa (%),IoU (%),MIoU (%),Dice (%),R-squared,RMSE,MAE,Parameter Description
J,"Ji, SP; Zhang, C; Xu, AJ; Shi, Y; Duan, YL",3D Convolutional Neural Networks for Crop Classification with Multi-Temporal Remote Sensing Images,REMOTE SENSING,2018,3D convolution; convolutional neural networks; crop classification; multi-temporal remote sensing images; active learning,"This study describes a novel three-dimensional (3D) convolutional neural networks (CNN) based method that automatically classifies crops from spatio-temporal remote sensing images. First, 3D kernel is designed according to the structure of multi-spectral multi-temporal remote sensing data. Secondly, the 3D CNN framework with fine-tuned parameters is designed for training 3D crop samples and learning spatio-temporal discriminative representations, with the full crop growth cycles being preserved. In addition, we introduce an active learning strategy to the CNN model to improve labelling accuracy up to a required threshold with the most efficiency. Finally, experiments are carried out to test the advantage of the 3D CNN, in comparison to the two-dimensional (2D) CNN and other conventional methods. Our experiments show that the 3D CNN is especially suitable in characterizing the dynamics of crop growth and outperformed the other mainstream methods.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Gaofen-2,Crop classification,Agriculture,Classification,no,91.5,,,,,,,85.8,,,,,,,
J,"Fathi, Mahdiyeh; Shah-Hosseini, Reza; Moghimi, Armin","3D-ResNet-BiLSTM Model: A Deep Learning Model for County-Level Soybean Yield Prediction with Time-Series Sentinel-1, Sentinel-2 Imagery, and Daymet Data",REMOTE SENSING,2023,soybean; yield prediction; Conv3D; ResNet; BiLSTM; Sentinel 1-2; Daymet; Google Earth Engine (GEE),"Ensuring food security in precision agriculture requires early prediction of soybean yield at various scales within the United States (U.S.), ranging from international to local levels. Accurate yield estimation is essential in preventing famine by providing insights into food availability during the growth season. Numerous deep learning (DL) algorithms have been developed to estimate soybean yield effectively using time-series remote sensing (RS) data to achieve this goal. However, the training data with short time spans can limit their ability to adapt to the dynamic and nuanced temporal changes in crop conditions. To address this challenge, we designed a 3D-ResNet-BiLSTM model to efficiently predict soybean yield at the county level across the U.S., even when using training data with shorter periods. We leveraged detailed Sentinel-2 imagery and Sentinel-1 SAR images to extract spectral bands, key vegetation indices (VIs), and VV and VH polarizations. Additionally, Daymet data was incorporated via Google Earth Engine (GEE) to enhance the model's input features. To process these inputs effectively, a dedicated 3D-ResNet architecture was designed to extract high-level features. These enriched features were then fed into a BiLSTM layer, enabling accurate prediction of soybean yield. To evaluate the efficacy of our model, its performance was compared with that of well-known models, including the Linear Regression (LR), Random Forest (RF), and 1D/2D/3D-ResNet models, as well as a 2D-CNN-LSTM model. The data from a short period (2019 to 2020) were used to train all models, while their accuracy was assessed using data from the year 2021. The experimental results showed that the proposed 3D-Resnet-BiLSTM model had a superior performance compared to the other models, achieving remarkable metrics (R2 = 0.791, RMSE = 5.56 Bu Ac-1, MAE = 4.35 Bu Ac-1, MAPE = 9%, and RRMSE = 10.49%). Furthermore, the 3D-ResNet-BiLSTM model showed a 7% higher R2 than the ResNet and RF models and an enhancement of 27% and 17% against the LR and 2D-CNN-LSTM models, respectively. The results highlighted our model's potential for accurate soybean yield predictions, supporting sustainable agriculture and food security.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Predicting soybean yield,Vegetation,Regression,yes,,,,,,,,,,,,0.758,,,Soybean yield (Bu Ac−1)
J,"Seydi, ST; Amani, M; Ghorbanian, A",A Dual Attention Convolutional Neural Network for Crop Classification Using Time-Series Sentinel-2 Imagery,REMOTE SENSING,2022,crop mapping; deep learning; convolutional neural networks (CNN); attention modules (AM); dual attention CNN; Sentinel-2; multi-temporal,"Accurate and timely mapping of crop types and having reliable information about the cultivation pattern/area play a key role in various applications, including food security and sustainable agriculture management. Remote sensing (RS) has extensively been employed for crop type classification. However, accurate mapping of crop types and extents is still a challenge, especially using traditional machine learning methods. Therefore, in this study, a novel framework based on a deep convolutional neural network (CNN) and a dual attention module (DAM) and using Sentinel-2 time-series datasets was proposed to classify crops. A new DAM was implemented to extract informative deep features by taking advantage of both spectral and spatial characteristics of Sentinel-2 datasets. The spectral and spatial attention modules (AMs) were respectively applied to investigate the behavior of crops during the growing season and their neighborhood properties (e.g., textural characteristics and spatial relation to surrounding crops). The proposed network contained two streams: (1) convolution blocks for deep feature extraction and (2) several DAMs, which were employed after each convolution block. The first stream included three multi-scale residual convolution blocks, where the spectral attention blocks were mainly applied to extract deep spectral features. The second stream was built using four multi-scale convolution blocks with a spatial AM. In this study, over 200,000 samples from six different crop types (i.e., alfalfa, broad bean, wheat, barley, canola, and garden) and three non-crop classes (i.e., built-up, barren, and water) were collected to train and validate the proposed framework. The results demonstrated that the proposed method achieved high overall accuracy and a Kappa coefficient of 98.54% and 0.981, respectively. It also outperformed other state-of-the-art classification methods, including RF, XGBOOST, R-CNN, 2D-CNN, 3D-CNN, and CBAM, indicating its high potential to discriminate different crop types.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Crop classification,Agriculture,Classification,no,95.73,,,,,,,94.7,,,,,,,
J,"Azeez, OS; Shafri, HZM; Alias, AH; Haron, NA",A Joint Bayesian Optimization for the Classification of Fine Spatial Resolution Remotely Sensed Imagery Using Object-Based Convolutional Neural Networks,LAND,2022,object-based convolution neural networks; deep learning; Bayesian optimization; decision-level fusion; Worldview-3,"In recent years, deep learning-based image classification has become widespread, especially in remote sensing applications, due to its automatic and strong feature extraction capability. However, as deep learning methods operate on rectangular-shaped image patches, they cannot accurately extract objects' boundaries, especially in complex urban settings. As a result, combining deep learning and object-based image analysis (OBIA) has become a new avenue in remote sensing studies. This paper presents a novel approach for combining convolutional neural networks (CNN) with OBIA based on joint optimization of segmentation parameters and deep feature extraction. A Bayesian technique was used to find the best parameters for the multiresolution segmentation (MRS) algorithm while the CNN model learns the image features at different layers, achieving joint optimization. The proposed classification model achieved the best accuracy, with 0.96 OA, 0.95 Kappa, and 0.96 mIoU in the training area and 0.97 OA, 0.96 Kappa, and 0.97 mIoU in the test area, outperforming several benchmark methods including Patch CNN, Center OCNN, Random OCNN, and Decision Fusion. The analysis of CNN variants within the proposed classification workflow showed that the HybridSN model achieved the best results compared to 2D and 3D CNNs. The 3D CNN layers and combining 3D and 2D CNN layers (HybridSN) yielded slightly better accuracies than the 2D CNN layers regarding geometric fidelity, object boundary extraction, and separation of adjacent objects. The Bayesian optimization could find comparable optimal MRS parameters for the training and test areas, with excellent quality measured by AFI (0.046, -0.037) and QR (0.945, 0.932). In the proposed model, higher accuracies could be obtained with larger patch sizes (e.g., 9 x 9 compared to 3 x 3). Moreover, the proposed model is computationally efficient, with the longest training being fewer than 25 s considering all the subprocesses and a single training epoch. As a result, the proposed model can be used for urban and environmental applications that rely on VHR satellite images and require information about land use.",Environmental Sciences & Ecology,yes,Worldview-3,Urban land-use and land-cover,Urban,Classification,no,94,,,,,,,93,,94,,,,,
C,Bergamasco L.; Bovolo F.; Bruzzone L.,A novel deep learning data structure for multispectral remote sensing images,Proceedings of SPIE - The International Society for Optical Engineering,2020,Convolutional neural network; Data-cube analysis; Deep-learning classification; Remote sensing; Spatial-spectral analysis,"Standard deep-learning (DL) architectures do not optimize the use of the spatial and spectral information in the multi-spectral images but often consider only one of the two components. Two-stream DL architectures split and process them separately. However, the fusing of the output of the two streams is a challenging task. 3D-CNN processes spatial and spectral information together at the cost of a large number of parameters. To overcome these limitations, we propose a novel DL data structure that re-organizes the spectral and spatial information in remote-sensing (RS) images and process them together. Representing a RS image I as a data cube, we handle the spatial and spectral information by reducing the spectral bands from N to M, where M can drop out to one. The spectral information is projected in the spatial dimensions and re-organized in 2-dimensional B blocks. The proposed approach analyzes the spectral information of each block by using 2-dimensional convolutional kernels of appropriate size and stride. The output represents the relationship between the spectral bands of the input image and preserves the spatial relationship between its neighboring pixels. The spatial relationships are analyzed by processing the output of the previous layer with standard 2D-CNNs. Experiments by using images acquired by Sentinel-2 and Landsat-8 data and the labels of the LUCAS database released in 2018 provide promising results. © SPIE. Downloading of the abstract is permitted for personal use only.",,no,"Sentinel-2, Landsat-8",Land-use and land-cover,Vegetation,Classification,yes,,,,,,,,,,,,,,,
J,"Papadopoulou, Eleni; Mallinis, Giorgos; Siachalou, Sofia; Koutsias, Nikos; Thanopoulos, Athanasios C.; Tsaklidis, Georgios",Agricultural Land Cover Mapping through Two Deep Learning Models in the Framework of EU's CAP Activities Using Sentinel-2 Multitemporal Imagery,REMOTE SENSING,2023,crop classification; entropy; land cover mapping; neural networks; random forest; remote sensing; Sentinel-2 images; uncertainty,"The images of the Sentinel-2 constellation can help the verification process of farmers' declarations, providing, among other things, accurate spatial explicit maps of the agricultural land cover. The aim of the study is to design, develop, and evaluate two deep learning (DL) architectures tailored for agricultural land cover and crop type mapping. The focus is on a detailed class scheme encompassing fifteen distinct classes, utilizing Sentinel-2 imagery acquired on a monthly basis throughout the year. The study's geographical scope covers a diverse rural area in North Greece, situated within southeast Europe. These architectures are a Temporal Convolutional Neural Network (CNN) and a combination of a Recurrent and a 2D Convolutional Neural Network (R-CNN), and their accuracy is compared to the well-established Random Forest (RF) machine learning algorithm. The comparative approach is not restricted to simply presenting the results given by classification metrics, but it also assesses the uncertainty of the classification results using an entropy measure and the spatial distribution of the classification errors. Furthermore, the issue of sampling strategy for the extraction of the training set is highlighted, targeting the efficient handling of both the imbalance of the dataset and the spectral variability of instances among classes. The two developed deep learning architectures performed equally well, presenting an overall accuracy of 90.13% (Temporal CNN) and 90.18% (R-CNN), higher than the 86.31% overall accuracy of the RF approach. Finally, the Temporal CNN method presented a lower entropy value (6.63%), compared both to R-CNN (7.76%) and RF (28.94%) methods, indicating that both DL approaches should be considered for developing operational EO processing workflows.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Crop classification,Agriculture,Classification,yes,90.18,86.03,,87.44,,,,,,,,,,,
J,"Zhao, Xin; Ma, Yi; Xiao, Yanfang; Liu, Jianqiang; Ding, Jing; Ye, Xiaomin; Liu, Rongjie",Atmospheric correction algorithm based on deep learning with spatial-spectral feature constraints for broadband optical satellites: Examples from the HY-1C Coastal Zone Imager,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2023,Atmospheric correction; Broadband optical satellite; HY-1C coastal zone imager (CZI); Convolution neural network (CNN); Spatial-spectral feature constraint,"Broadband optical satellites have been widely used for fine monitoring of coastal waters and inland lakes for their high spatial resolution. Atmospheric correction (AC) is one of the essential data processing steps for remote sensing of water environments with high spatial resolution satellites. Most broadband optical satellites, such as HY-1C Coastal Zone Imager (CZI) with a spatial resolution of 50 m, lack of near-infrared and shortwave infrared bands needed for atmospheric correction, making accurate AC difficult. Auxiliary aerosol data is often needed for accurate AC of broadband optical satellite data, which suffers from the differences in spatial resolution and imaging time between auxiliary data and satellite data. Furthermore, existing AC algorithms always perform AC pixel by pixel, which ignores the spatial relationship between adjacent pixels. Taking HY-1C CZI as an example, this paper proposes a novel atmospheric correction algorithm based on deep learning (SSACNet). Considering the inherent spatial-spectral features of satellite images, the SSACNet combines 2D and 3D convolution. The 3D convolution was used to mine the spatial and spectral features of the image and 2D convolution was explored to perform spatial information compensation. The SSACNet was trained and evaluated using the spatio-temporally synchronized dataset of HY-1C CZI and Landsat8 Operational Land Imager (OLI). The evaluation results by in-situ data show that the SSACNet has good performance, with the average correlation coefficient of 0.89, and the absolute percentage deviation (APD) of four bands ranging from 21.53 % to 35.41 %. Compared with the quasisynchronous Landsat8 OLI remote sensing reflectance (Rrs), the APD is less than 10 %. Compared with traditional atmospheric correction algorithms, SSACNet has significantly improved the spatial and spectral information fidelity. In addition, SSACNet also shows good applicability, as it can be used in both clear ocean waters and turbid coastal waters. This study lays a foundation for the quantitative remote sensing of water environment by broadband optical satellites.",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,"Landsat-8, HY-1C",Atmospheric correction,Others,Regression,yes,,,,,,,,,,,,0.7921,,,
C,Kang J.; Demir B.,Band-Wise Multi-Scale CNN Architecture for Remote Sensing Image Scene Classification,International Geoscience and Remote Sensing Symposium (IGARSS),2020,convolutional neural networks; Deep learning; feature extraction; remote sensing; scene classification,"Most of the existing convolutional neural network (CNN) architectures in the framework of image scene classification problems are designed for modeling RGB image bands. Direct application of these architectures to the high-dimensional remote sensing (RS) scene classification can be insufficient to accurately describe the spectral content. To address this issue, we propose a novel CNN architecture for the feature embedding of high-dimensional RS images. The proposed architecture aims at: 1) decoupling the spectral and spatial feature extraction for sufficiently describing the complex information content of images; and 2) taking advantage of multi-scale representations of different land-use and land-cover classes present in the images. To this end, the proposed architecture is mainly composed of: 1) a convolutional layer for band-wise extraction of multi-scale spatial features; 2) a convolutional layer for pixel-wise extraction of spectral features; and 3) standard 2D convolution and residual blocks for further feature learning. Experiments on BigEarthNet validate the effectiveness of the proposed method, when compared to the state-of-the-art CNN architectures. © 2020 IEEE.",,yes,Sentinel-2,Land-use and land-cover,Vegetation,Classification,yes,73.07,81.84,,,,,,,,,,,,,
J,Virnodkar S.S.; Pachghare V.K.; Patil V.C.; Jha S.K.,CaneSat dataset to leverage convolutional neural networks for sugarcane classification from Sentinel-2,Journal of King Saud University - Computer and Information Sciences,2022,Convolutional neural network; Deep network; Multi-temporal remote sensing images; Sugarcane classification,"The ubiquitous deep learning (DL) in remote sensing (RS) motivates the most challenging problem of crop classification. To perpetrate such an exigent task, an attempt is made to prepare a novel dataset, the CaneSat dataset, in two formats: RGB color space and geo-tiff images, covering the region of four talukas in Karnataka, India. This research aims to build a model for sugarcane classification using two-dimensional convolutional neural network (CNN or ConvNet) applying RS time series data. Further, the study intents to evaluate competency of four state-of-the-art deep CNNs namely AlexNet, GoogLeNet, ResNet50 and DenseNet201 using fine tuning and deep CNNs as feature extractors to classify sugarcane and non-sugarcane areas from Sentinel-2 data. The results of the research are expressive on CaneSat dataset. It shows that the CNN model performs significantly good producing 88.46% accuracy, whereas all deep networks exhibit more than 73.00% overall accuracy. When used as feature extractors, ResNet50 and DenseNet201 outperform all other models with precision of 85.65% and 87.70%, respectively. Noticeably, the results indicate that 2D CNN model and features extracted using CNNs with SVM classifier are efficient methods for sugarcane classification from Sentinel-2 time series data in peninsular zone of India. © 2020 The Authors",,yes,Sentinel-2,Sugarcane classification,Vegetation,Classification,no,88.46,,,,,,,,,,,,,,
C,M. Chelali; C. Kurtz; A. Puissant; N. Vincent,Classification of spatially enriched pixel time series with convolutional neural networks,2020 25th International Conference on Pattern Recognition (ICPR),2021,,"Satellite Image Time Series (SITS), MRI sequences, and more generally image time series, constitute 2D+t data providing spatial and temporal information about an observed scene. Given a pattern recognition task such as image classification, considering jointly such rich information is crucial during the decision process. Nevertheless, due to the complex representation of the data-cube, spatio-temporal features extraction from 2D+t data remains difficult to handle. We present in this article an approach to learn such features from this data, and then to proceed to their classification. Our strategy consists in enriching pixel time series with spatial information. It is based on Random Walk to build a novel segment-based representation of the data, passing from a 2D+t dimension to a 2D one, without loosing too much spatial information. Such new representation is then involved in an end-to-end learning process with a classical 2D Convolutional Neural Network (CNN) in order to learn spatiotemporal features for the classification of image time series. Our approach is evaluated on a remote sensing application for the mapping of agricultural crops. Thanks to a visual attention mechanism, the proposed 2D spatio-temporal representation makes also easier the interpretation of a SITS to understand spatiotemporal phenomenons related to soil management practices.",,yes,Sentinel-2,Agricultural crops classification,Agriculture,Classification,yes,94.64,,,,,,,,,,,,,,
C,A. Shakya; M. Biswas; M. Pal,CNN-Based Fusion and Classification of Multi-Temporal Sentinel-1 & -2 Satellite Data,2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS),2021,Fusion;Classification;Convolutional Neural Network (CNN);Bayesian Optimization,SAR and optical data are widely used in image fusion to provide the complimentary information of each other and obtain the spatial and spectral features for improved classifications. This paper proposes to use multi-temporal data form Sentinel-1 (VV & VH polarization) and Sentinel-2 sensors for the fusion and classification over an agricultural area. Convolutional Neural Network (CNN)- based Pyramid method for fusion and Bayesian Optimized 2-D CNN for classification of fused multi-temporal data was used to extract spatial-spectral information. Results in terms of classification accuracy suggests slightly better performance by VV polarized fused images than the VH and also suggests an improved performance by multi-temporal data in comparison to the single date data over the study area.,,yes,Sentinel-2,Agricultural classification,Vegetation,Classification,yes,99.91,,,,,,,99,,,,,,,
J,"Shakya, A; Biswas, M; Pal, M",CNN-based fusion and classification of SAR and Optical data,INTERNATIONAL JOURNAL OF REMOTE SENSING,2020,,"Image fusion combines the images of different spectral, spatial, multi-date, as well as radiometric data to achieve a better quality image for improved classification results. Recently, Convolution Neural Network (CNN)-based classification algorithms are extensively used for remote sensing applications. Keeping this in view, present work proposes to use CNN-based fusion and classification of Sentinel 1 (VV and VH polarization) and Sentinel 2 datasets acquired over an agricultural area near Hisar (India). For image fusion, three CNN-based approaches are used to fuse Sentinel 2 (10 m) data with VV and VH bands of Sentinel 1 data. After fusion, classification was performed using 2D-CNN classifier to judge the performance of fused images in terms of classification accuracy. Results suggest that out of the three fusion approaches, only infrared image fusion (IVF) approach performed well with the considered dataset in terms of fusion indicators and classification accuracy. Keeping in view of its better performance, this study proposes a modified IVF approach by using different image pyramid methods. Comparison of results suggests an improved performance by modified IVF approach for the fusion of Sentinel 2 and Sentinel 1 data in comparison with the original IVF approach.",Remote Sensing; Imaging Science & Photographic Technology,no,Sentinel-2,Land cover classification,Vegetation,Classification,no,89.18,,,,,,,87,,,,,,,
J,"Sanchez, AMS; Gonzalez-Piqueras, J; de la Ossa, L; Calera, A",Convolutional Neural Networks for Agricultural Land Use Classification from Sentinel-2 Image Time Series,REMOTE SENSING,2022,deep learning; remote sensing; land use classification; sentinel; time series,"Land use classification (LUC) is the process of providing information on land cover and the types of human activity involved in land use. In this study, we perform agricultural LUC using sequences of multispectral reflectance Sentinel-2 images taken in 2018. LUC can be carried out using machine or deep learning techniques. Some existing models process data at the pixel level, performing LUC successfully with a reduced number of images. Part of the pixel information corresponds to multispectral temporal patterns that, despite not being especially complex, might remain undetected by models such as random forests or multilayer perceptrons. Thus, we propose to arrange pixel information as 2D yearly fingerprints so as to render such patterns explicit and make use of a CNN to model and capture them. The results show that our proposal reaches a 91% weighted accuracy in classifying pixels among 19 classes, outperforming random forest by 8%, or a specifically tuned multilayer perceptron by 4%. Furthermore, models were also used to perform a ternary classification in order to detect irrigated fields, reaching a 97% global accuracy. We can conclude that this is a promising operational tool for monitoring crops and water use over large areas.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Agricultural land use classification,Agriculture,Classification,no,91,91,91,91,,,,,,,,,,,
J,Corbane C.; Syrris V.; Sabo F.; Politis P.; Melchiorri M.; Pesaresi M.; Soille P.; Kemper T.,Convolutional neural networks for global human settlements mapping from Sentinel-2 satellite imagery,Neural Computing and Applications,2021,Built-up areas; Convolutional neural networks; Human settlements; Image segmentation; Remote sensing,"Spatially consistent and up-to-date maps of human settlements are crucial for addressing policies related to urbanization and sustainability, especially in the era of an increasingly urbanized world. The availability of open and free Sentinel-2 data of the Copernicus Earth Observation program offers a new opportunity for wall-to-wall mapping of human settlements at a global scale. This paper presents a deep-learning-based framework for a fully automated extraction of built-up areas at a spatial resolution of 10 m from a global composite of Sentinel-2 imagery. A multi-neuro modeling methodology building on a simple Convolution Neural Networks architecture for pixel-wise image classification of built-up areas is developed. The core features of the proposed model are the image patch of size 5 × 5 pixels adequate for describing built-up areas from Sentinel-2 imagery and the lightweight topology with a total number of 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened layers. The deployment of the model on the global Sentinel-2 image composite provides the most detailed and complete map reporting about built-up areas for reference year 2018. The validation of the results with an independent reference dataset of building footprints covering 277 sites across the world establishes the reliability of the built-up layer produced by the proposed framework and the model robustness. The results of this study contribute to cutting-edge research in the field of automated built-up areas mapping from remote sensing data and establish a new reference layer for the analysis of the spatial distribution of human settlements across the rural–urban continuum. © 2020, The Author(s).",,yes,Sentinel-2,Built-up area classification,Urban,Classification,yes,85,,,,,,,65,,,,,,,
J,"Kou, Wenqi; Shen, Zhanfeng; Liu, Diyou; Liu, Zhe; Li, Junli; Chang, Wanqiu; Wang, Haoyu; Huang, Lan; Jiao, Shuhui; Lei, Yating; Zhang, Chi",Crop classification methods and influencing factors of reusing historical samples based on 2D-CNN,INTERNATIONAL JOURNAL OF REMOTE SENSING,2023,reusing historical samples; crop classification; irregular satellite image time series; 2D-CNN; influencing factors,"Crop classification is a crucial task in agricultural remote sensing, with the accuracy of such classification heavily relies on field sampling. Reusing historical samples can minimize the reliance on annual field sampling for crop classification. Although previous research primarily focused on the classification accuracy based on the reused historical samples, the underlying factors that influence the accuracy have not been adequately investigated. In this study, we employed a two-dimensional convolutional neural network (2D-CNN) model for crop classification reusing historical samples and investigated the factors influencing the classification accuracy. First, we calculated a normalized difference vegetation index (NDVI) time series from historical data to characterize crop growth patterns. Secondly, we assessed three different time-series construction methods. Subsequently, we used the 2D-CNN model to automatically extract abstract features of crop growth patterns. Finally, we designed various strategies for reusing historical samples to explore the influencing factors. Experiments were conducted in Kuitun, Xinjiang Uygur Autonomous Region, China, from 2016 to 2020, employing a long time series of Sentinel-2 images as remote sensing data. Our results indicated that optimal 2D-CNN models using an irregular satellite image time series (irSITS) outperformed random forest models in inter-annual classifications (the overall accuracies for 2016-2020 were 0.78, 0.61, 0.89, 0.89, and 0.70, respectively). In addition, we identified that the primary factors affecting classification accuracy were 1) the time-series construction method used; 2) the crop growth patterns; and 3) the sample diversity. By reusing historical samples and considering the factors influencing classification accuracy, this study provides valuable insights into high-quality crop classification mapping under limited field sample conditions.",Remote Sensing; Imaging Science & Photographic Technology,no,Sentinel-2,Crop classification,Agriculture,Classification,no,77.4,,,,,,,,,,,,,,
J,"Kanwal, Rida; Rafaqat, Warda; Iqbal, Mansoor; Weiguo, Song",Data-Driven Approaches for Wildfire Mapping and Prediction Assessment Using a Convolutional Neural Network (CNN),REMOTE SENSING,2023,machine learning; wildfire assessment; CNN; random forest; fire occurrence,"As wildfires become increasingly perilous amidst Pakistan's expanding population and evolving environmental conditions, their global significance necessitates urgent attention and concerted efforts toward proactive measures and international cooperation. This research strives to comprehensively enhance wildfire prediction and management by implementing various measures to contribute to proactive mitigation in Pakistan. Additionally, the objective of this research was to acquire an extensive understanding of the factors that influence fire patterns in the country. For this purpose, we looked at the spatiotemporal patterns and causes of wildfires between 2000 and 2023 using descriptive analysis. The data analysis included a discussion on density-based clustering as well as the distribution of the data across four seasons over a period of six years. Factors that could indicate the probability of a fire occurrence such as weather conditions, terrain characteristics, and fuel availability encompass details about the soil, economy, and vegetation. We used a convolutional neural network (CNN) to extract features, and different machine learning (ML) techniques were implemented to obtain the best model for wildfire prediction. The majority of fires in the past six years have primarily occurred during the winter months in coastal locations. The occurrence of fires was accurately predicted by ML models such as random forest (RF), which outperformed competing models. Meanwhile, a CNN with 1D and 2D was used for more improvement in prediction by ML models. The accuracy increased from an 86.48 to 91.34 accuracy score by just using a CNN 1D. For more feature extraction, a CNN 2D was used on the same dataset, which led to state-of-the-art prediction results. A 96.91 accuracy score was achieved by further tuning the RF model on the total data. Data division by spatial and temporal changes was also used for the better prediction of fire, which can further be helpful for understanding the different prospects of wildfire. This research aims to advance wildfire prediction methodologies by leveraging ML techniques to explore the benefits and limitations of capturing complex patterns and relationships in large datasets. Policymakers, environmentalists, and scholars studying climate change can benefit greatly from the study's analytical approach, which may assist Pakistan in better managing and reducing wildfires.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,CBERS-4A,Wildfire prediction,Geohazards,Classification,yes,96.56,96.95,95.24,,,,,,,,,,,,
J,"Hussain, Muhammad Afaq; Chen, Zhanlong; Zheng, Ying; Zhou, Yulong; Daud, Hamza",Deep Learning and Machine Learning Models for Landslide Susceptibility Mapping with Remote Sensing Data,REMOTE SENSING,2023,convolutional neural network; recurrent neural networks; landslide susceptibility mapping; extreme gradient boosting; random forest,"Karakoram Highway (KKH) is an international route connecting South Asia with Central Asia and China that holds socio-economic and strategic significance. However, KKH has extreme geological conditions that make it prone and vulnerable to natural disasters, primarily landslides, posing a threat to its routine activities. In this context, the study provides an updated inventory of landslides in the area with precisely measured slope deformation (Vslope), utilizing the SBAS-InSAR (small baseline subset interferometric synthetic aperture radar) and PS-InSAR (persistent scatterer interferometric synthetic aperture radar) technology. By processing Sentinel-1 data from June 2021 to June 2023, utilizing the InSAR technique, a total of 571 landslides were identified and classified based on government reports and field investigations. A total of 24 new prospective landslides were identified, and some existing landslides were redefined. This updated landslide inventory was then utilized to create a landslide susceptibility model, which investigated the link between landslide occurrences and the causal variables. Deep learning (DL) and machine learning (ML) models, including convolutional neural networks (CNN 2D), recurrent neural networks (RNNs), random forest (RF), and extreme gradient boosting (XGBoost), are employed. The inventory was split into 70% for training and 30% for testing the models, and fifteen landslide causative factors were used for the susceptibility mapping. To compare the accuracy of the models, the area under the curve (AUC) of the receiver operating characteristic (ROC) was used. The CNN 2D technique demonstrated superior performance in creating the landslide susceptibility map (LSM) for KKH. The enhanced LSM provides a prospective modeling approach for hazard prevention and serves as a conceptual reference for routine management of the KKH for risk assessment and mitigation.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Landslide Susceptibility Mapping,Geohazards,Classification,no,,,83.61,,,,,,,,,,,,
J,"Li, Qianjing; Tian, Jia; Tian, Qingjiu",Deep Learning Application for Crop Classification via Multi-Temporal Remote Sensing Images,AGRICULTURE-BASEL,2023,crop type classification; deep learning; multi-temporal; remote sensing,"The combination of multi-temporal images and deep learning is an efficient way to obtain accurate crop distributions and so has drawn increasing attention. However, few studies have compared deep learning models with different architectures, so it remains unclear how a deep learning model should be selected for multi-temporal crop classification, and the best possible accuracy is. To address this issue, the present work compares and analyzes a crop classification application based on deep learning models and different time-series data to exploit the possibility of improving crop classification accuracy. Using Multi-temporal Sentinel-2 images as source data, time-series classification datasets are constructed based on vegetation indexes (VIs) and spectral stacking, respectively, following which we compare and evaluate the crop classification application based on time-series datasets and five deep learning architectures: (1) one-dimensional convolutional neural networks (1D-CNNs), (2) long short-term memory (LSTM), (3) two-dimensional-CNNs (2D-CNNs), (4) three-dimensional-CNNs (3D-CNNs), and (5) two-dimensional convolutional LSTM (ConvLSTM2D). The results show that the accuracy of both 1D-CNN (92.5%) and LSTM (93.25%) is higher than that of random forest (similar to 91%) when using a single temporal feature as input. The 2D-CNN model integrates temporal and spatial information and is slightly more accurate (94.76%), but fails to fully utilize its multi-spectral features. The accuracy of 1D-CNN and LSTM models integrated with temporal and multi-spectral features is 96.94% and 96.84%, respectively. However, neither model can extract spatial information. The accuracy of 3D-CNN and ConvLSTM2D models is 97.43% and 97.25%, respectively. The experimental results show limited accuracy for crop classification based on single temporal features, whereas the combination of temporal features with multi-spectral or spatial information significantly improves classification accuracy. The 3D-CNN and ConvLSTM2D models are thus the best deep learning architectures for multi-temporal crop classification. However, the ConvLSTM architecture combining recurrent neural networks and CNNs should be further developed for multi-temporal image crop classification.",Agriculture,yes,Sentinel-2,Crop classification,Agriculture,Classification,no,94.75,,,,,,,93.4,,,,,,,
J,"Chelali, M; Kurtz, C; Puissant, A; Vincent, N",Deep-STaR: Classification of image time series based on spatio-temporal representations,COMPUTER VISION AND IMAGE UNDERSTANDING,2021,Image time series; Spatio-temporal features; Planar image representation; Space filling curves; Random walk; CNN; Remote sensing,"Image time series (ITS) represent complex 3D (2D+t in practice) data that are now daily produced in various domains, from medical imaging to remote sensing. They contain rich spatio-temporal information allowing the observation of the evolution of a sensed scene over time. In this work, we focus on the classification task of ITS, as often available in remote sensing tasks. An underlying problem here is to consider jointly the spatial and the temporal dimensions of the data. We present Deep-STaR, a method to learn such features from ITS data to proceed to their classification. Instead of reasoning in the original 2D+t space, we investigate novel 2D planar data representations, containing both temporal and spatial information. Such representations are a novel way to structure the ITS, compatible with deep learning architectures. They are used to feed a convolutional neural network to learn spatio-temporal features with 2D convolutions, leading ultimately to classification decision. To enhance the explainability of the results, we also propose a post-hoc attention mechanism, enabled by this new approach, providing a semantic map giving some insights for the taken decision. Deep-STaR is evaluated on a remote sensing application, for the classification of agricultural crops from satellite ITS. The results highlight the benefice of this method, compared to the literature, and its interest to make easier the interpretation of ITS to understand spatio-temporal phenomena.",Computer Science; Engineering,no,Sentinel-2,Agricultural crops classification,Agriculture,Classification,yes,,,,,,,,,,,,,,,
J,"Aldabbagh, YAN; Shafri, HZM; Mansor, S; Ismail, MH","Desertification prediction with an integrated 3D convolutional neural network and cellular automata in Al-Muthanna, Iraq",ENVIRONMENTAL MONITORING AND ASSESSMENT,2022,Desertification prediction; Convolutional neural networks; Cellular automata; Al-Muthanna,"Desertification is a major environmental issue all over the world, and Al-Khidhir district, Al-Muthanna, in the south of Iraq is no exception. In mapping, assessing, and predicting desertification, remote sensing and geospatial solutions (spatial analysis, machine learning) are crucial. During 1998-2018, this study employed satellite images from Landsat TM, ETM +, and OLI to map and predict desertification in the Al-Khidhir district. The year 2028 was chosen as the target date. Prediction models were constructed using a 3D convolutional neural network (3D CNN) and cellular automata (CA) techniques. In addition to the historical land cover maps, the model incorporated desertification indicators identified as important in the study, including geology, soil type, distance from waterways, elevation, population density, and Normalized Difference Vegetation Index (NDVI). Several accuracy metrics were used to evaluate the models, including overall accuracy (OA), average accuracy (AA), and the Kappa index (K). The simulated and actual land cover maps from 1998 and 2008 were used to evaluate the desertification prediction models. The 3D CNN model outperforms the typical 2D CNN for both the 2008 and 2018 images, according to the results. For the 2008 image, the 3D CNN model achieved 89.675 OA, 69.946 AA, and 0.781 K, while the 2018 image achieved 91.494 OA, 75.138 AA, and 0.770 K. The 2D CNN model performed a little worse than the 3D CNN model. The results of the change assessment showed that between 1998 and 2008, agricultural land was the dominant class (39%, 47.4%, respectively). The bare land, however, was the most dominant class in 2018, accounting for 46.6% of the total, compared to 26.2% for agricultural land. The spatial distribution characteristics of desertification in the Al-Khidhir, in the year 1998, were prevalent in the area's south (25.9%). In the following 10 years, desertification has spread to the surrounding territories. In the year 2008, desertification increased in the north of the study area (50.8%). Unless the local administration of Al-Khidhir district establishes desertification control strategies, this study suggests that the extent of bare land could expand in 2028 (54.1%).",Environmental Sciences & Ecology,no,Landsat-7,Desertification prediction,Geohazards,Classification,no,89.32,,,,,,,76.9,,,,,,,
J,"Aldabbagh, YAN; Shafri, HZM; Mansor, S; Ismail, MH","Desertification prediction with an integrated 3D convolutional neural network and cellular automata in Al-Muthanna, Iraq",ENVIRONMENTAL MONITORING AND ASSESSMENT,2022,Desertification prediction; Convolutional neural networks; Cellular automata; Al-Muthanna,"Desertification is a major environmental issue all over the world, and Al-Khidhir district, Al-Muthanna, in the south of Iraq is no exception. In mapping, assessing, and predicting desertification, remote sensing and geospatial solutions (spatial analysis, machine learning) are crucial. During 1998-2018, this study employed satellite images from Landsat TM, ETM +, and OLI to map and predict desertification in the Al-Khidhir district. The year 2028 was chosen as the target date. Prediction models were constructed using a 3D convolutional neural network (3D CNN) and cellular automata (CA) techniques. In addition to the historical land cover maps, the model incorporated desertification indicators identified as important in the study, including geology, soil type, distance from waterways, elevation, population density, and Normalized Difference Vegetation Index (NDVI). Several accuracy metrics were used to evaluate the models, including overall accuracy (OA), average accuracy (AA), and the Kappa index (K). The simulated and actual land cover maps from 1998 and 2008 were used to evaluate the desertification prediction models. The 3D CNN model outperforms the typical 2D CNN for both the 2008 and 2018 images, according to the results. For the 2008 image, the 3D CNN model achieved 89.675 OA, 69.946 AA, and 0.781 K, while the 2018 image achieved 91.494 OA, 75.138 AA, and 0.770 K. The 2D CNN model performed a little worse than the 3D CNN model. The results of the change assessment showed that between 1998 and 2008, agricultural land was the dominant class (39%, 47.4%, respectively). The bare land, however, was the most dominant class in 2018, accounting for 46.6% of the total, compared to 26.2% for agricultural land. The spatial distribution characteristics of desertification in the Al-Khidhir, in the year 1998, were prevalent in the area's south (25.9%). In the following 10 years, desertification has spread to the surrounding territories. In the year 2008, desertification increased in the north of the study area (50.8%). Unless the local administration of Al-Khidhir district establishes desertification control strategies, this study suggests that the extent of bare land could expand in 2028 (54.1%).",Environmental Sciences & Ecology,no,Landsat-8,Desertification prediction,Geohazards,Classification,no,91.38,,,,,,,76.3,,,,,,,
C,Park S.; Park N.-W.,Effects of class purity of training data on crop classification using 2D-CNn,"40th Asian Conference on Remote Sensing, ACRS 2019: Progress of Remote Sensing Technology for Smart Future",2020,Deep learning; Patch-based classification; Training data,"Proper collection of training data is an important factor in supervised classification for crop mapping. Each pixel in remote sensing imagery represents an area with various characteristics of surface objects and may have different spectral values for the same crop type. This mixed pixel effect in training data may greatly affect classification results. Although much effort has been made for the proper selection of training data in pixel-based classification, few studies have been conducted in patch-based classification with deep learning. In this study, we analyze the effect of class purity within the patch of training data on a patch-based 2D convolutional neural network (2D-CNN) model for crop classification. The classification performance of 2D-CNN was evaluated from two case study areas with different spatial characteristics of crops and input images with different spatial resolutions. In the area which consists of crop parcels with similar shapes and uniform patterns, the classification accuracy could be improved by collecting training samples with high class purity in the high spatial resolution imagery. On the contrary, using training samples with lower class purity in crop classification with Landsat images led to the improvement in the classification accuracy in the classification of areas where crop parcels had various shapes and sizes. These experimental results indicate that training data in the patch-based crop classification should be selected by taking into account the characteristics of the area to be classified. © 2020 40th Asian Conference on Remote Sensing, ACRS 2019: ""Progress of Remote Sensing Technology for Smart Future"". All rights reserved.",,yes,Landsat-8,Crop classification,Agriculture,Classification,no,88,,,,,,,,,,,,,,
J,Park S.; Park N.-W.,Effects of class purity of training patch on classification performance of crop classification with convolutional neural network,Applied Sciences (Switzerland),2020,Class purity; Convolutional neural network; Patch-based classification; Training samples,"As the performance of supervised classification using convolutional neural networks (CNNs) are affected significantly by training patches, it is necessary to analyze the effects of the information content of training patches in patch-based classification. The objective of this study is to quantitatively investigate the effects of class purity of a training patch on performance of crop classification. Here, class purity that refers to a degree of compositional homogeneity of classes within a training patch is considered as a primary factor for the quantification of information conveyed by training patches. New quantitative indices for class homogeneity and variations of local class homogeneity over the study area are presented to characterize the spatial homogeneity of the study area. Crop classification using 2D-CNN was conducted in two regions (Anbandegi in Korea and Illinois in United States) with distinctive spatial distributions of crops and class homogeneity over the area to highlight the effect of class purity of a training patch. In the Anbandegi region with high class homogeneity, superior classification accuracy was obtained when using large size training patches with high class purity (7.1%p improvement in overall accuracy over classification with the smallest patch size and the lowest class purity). Training patches with high class purity could yield a better identification of homogenous crop parcels. In contrast, using small size training patches with low class purity yielded the highest classification accuracy in the Illinois region with low class homogeneity (19.8%p improvement in overall accuracy over classification with the largest patch size and the highest class purity). Training patches with low class purity could provide useful information for the identification of diverse crop parcels. The results indicate that training samples in patch-based classification should be selected based on the class purity that reflects the local class homogeneity of the study area. © 2020 by the authors.",,yes,Landsat-8,Crop classification,Agriculture,Classification,no,86.56,,,,,,,,,,,,,,
J,"Nguyen, Chi; Tan, Chang Wei; Daly, Edoardo; Pauwels, Valentijn R. N.",Efficient analysis of hydrological connectivity using 1D and 2D Convolutional Neural Networks,ADVANCES IN WATER RESOURCES,2023,Convolutional neural network; Functional connectivity; Potential connection length,"Understanding hydrological connectivity is essential to investigate ecological processes in river catchments and floodplains. Assessing flooding behavior, including flooded areas and connection times, is required to analyze hydrological connectivity in river floodplains. Deep learning, especially Convolutional Neural Networks (CNNs), is an attractive alternative to hydrodynamic modeling, which is more computationally expensive. This paper aims to develop a methodology to analyze the functional connectivity in remote and field measurement data-scarce areas using remote sensing data, CNN models, and connectivity metrics. The northern Lakes of the Narran River catchment, located in the Condamine-Balonne River floodplain in New South Wales, Australia, is the showcase for this method. One-dimensional CNN and two-dimensional U-Net configurations were applied and yielded comparable flood extents to the satellite images with Hit Rate values of 0.853 and 0.873, respectively. Two algorithms for determining hydrological connectivity were investigated, including the geostatistical Connectivity Function (CF) and the newly proposed Potential Connection Length (PCL). It was found that the connection along the main Narran River stream was more substantial than between the river and the floodplain lakes. The analysis using the PCL shows that the connectivity patterns in different stages of a flood event can vary depending on the initial condition of the floodplain. The overall conclusion from this work is that hydrological connectivity can be assessed computationally efficiently using only remote sensing, discharge data, and CNN models.",Water Resources,yes,Sentinel-2,Hydrological connectivity,Water,Classification,no,,,84.275,,,,,,,,,,,,
J,Moreno-Revelo M.Y.; Guachi-Guachi L.; Gómez-Mendoza J.B.; Revelo-Fuelagán J.; Peluffo-Ordóñez D.H.,Enhanced convolutional-neural-network architecture for crop classification,Applied Sciences (Switzerland),2021,Convolutional neural network (CNN); Crop classification; Post-processing; Satellite images,"Automatic crop identification and monitoring is a key element in enhancing food production processes as well as diminishing the related environmental impact. Although several efficient deep learning techniques have emerged in the field of multispectral imagery analysis, the crop classification problem still needs more accurate solutions. This work introduces a competitive methodology for crop classification from multispectral satellite imagery mainly using an enhanced 2D convolutional neural network (2D-CNN) designed at a smaller-scale architecture, as well as a novel post-processing step. The proposed methodology contains four steps: image stacking, patch extraction, classification model design (based on a 2D-CNN architecture), and post-processing. First, the images are stacked to increase the number of features. Second, the input images are split into patches and fed into the 2D-CNN model. Then, the 2D-CNN model is constructed within a small-scale framework, and properly trained to recognize 10 different types of crops. Finally, a post-processing step is performed in order to reduce the classification error caused by lower-spatial-resolution images. Experiments were carried over the so-named Campo Verde database, which consists of a set of satellite images captured by Landsat and Sentinel satellites from the municipality of Campo Verde, Brazil. In contrast to the maximum accuracy values reached by remarkable works reported in the literature (amounting to an overall accuracy of about 81%, a f1 score of 75.89%, and average accuracy of 73.35%), the proposed methodology achieves a competitive overall accuracy of 81.20%, a f1 score of 75.89%, and an average accuracy of 88.72% when classifying 10 different crops, while ensuring an adequate trade-off between the number of multiply-accumulate operations (MACs) and accuracy. Furthermore, given its ability to effectively classify patches from two image sequences, this methodology may result appealing for other real-world applications, such as the classification of urban materials. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,yes,Landsat-8,Crop classification,Agriculture,Classification,no,81.2,75.89,,,,,,,,,,,,,
C,Ding C.; Zhang X.; Ma S.; Han W.; Lu Y.; Yin J.,"Estuary water quality classification through deep learning image segmentation, an example of Hangzhou Bay","2021 9th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2021",2021,convolutional neural networks; image segmentation; Suspended sediment concentration; UNet,"Suspended sediment plays a dual role in the aquatic ecosystems. It serves an important part of the aquatic nutrient cycle but is also a pollutant in aquatic ecosystems. Traditional methods of measuring suspended sediment concentration (SSC) are mainly in-situ sampling and laboratory analysis. Through optical sensors carried by satellites or airplanes, large-scale SSC is possible to be acquired, which helps to improve our understanding of connections of source, sink and pathway. Although lots of efforts have been done in water quality modeling, methods utilized are mainly multi-layer perceptrons, machine learning algorithms or physio-chemical models, few tries on convolutional neural networks have been done. Here we take the Landsat image of Hangzhou Bay as an example. Based on the inversion algorithm the water quality classification is conducted through by 1D-CNN and 2D-UNet model. Best match between the band and reflectance of SSC received by the sensor are carefully selected as input. Both models prove feasible with convincing accuracy and precision, despite some discrepancies in details. Water quality classification could be furtherly completed if taking multi-index of water into account. © 2021 IEEE.",,yes,Landsat-8,Estuary water quality classification,Water,Classification,yes,91.38,,91.51,,,,,,,,97.16,,,,
J,"Sagan, V; Maimaitijiang, M; Bhadra, S; Maimaitiyiming, M; Brown, DR; Sidike, P; Fritschi, FB",Field-scale crop yield prediction using multi-temporal WorldView-3 and PlanetScope satellite data and deep learning,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2021,PlanetScope; WorldView-3; Deep learning; Convolutionneural network; ResNet; Artificial intelligence; Food security,"Agricultural management at field-scale is critical for improving yield to address global food security, as providing enough food for the world's growing population has become a wicked problem for both scientists and policy-makers. County- or regional-scale data do not provide meaningful information to farmers who are interested in field-scale yield forecasting for effective and timely field management. No studies directly utilized raw satellite imagery for field-scale yield prediction using deep learning. The objectives of this paper were twofold: (1) to develop a raw imagery-based deep learning approach for field-scale yield prediction, (2) investigate the contribution of in-season multitemporal imagery for grain yield prediction with hand-crafted features and WorldView-3 (WV) and PlanetScope (PS) imagery as the direct input, respectively. Four WV-3 and 25 PS imagery collected during the growing season of soybean were utilized. Both 2-dimensional (2D) and 3-dimensional (3D) convolution neural network (CNN) architectures were developed that integrated spectral, spatial, temporal information contained in the satellite data. For comparison, hundreds of carefully selected spectral, spatial, textural, and temporal features that are optimal for crop growth monitoring were extracted and fed into the same deep learning model. Our results demonstrated that (1) deep learning was able to predict yield directly using raw satellite imagery to the extent that was comparable to feature-fed deep learning approaches; (2) both 2D and 3D CNN models were able to explain nearly 90% variance in field-scale yield; (3) limited number of WV-3 outperformed multi-temporal PS data collected during entire growing season mainly attributed to RedEdge and SWIR bands available with WV-3; and (4) 3D CNN increased the prediction power of PS data compared to 2D CNN due to its ability to digest temporal features extracted from PS data.",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,"WorldView-3, PlanetScope",Crop yield prediction,Agriculture,Regression,yes,,,,,,,,,,,,,,,
J,"Shakya, A; Biswas, M; Pal, M",Fusion and classification of multi-temporal SAR and optical imagery using convolutional neural network,INTERNATIONAL JOURNAL OF IMAGE AND DATA FUSION,2022,Fusion; Convolutional Neural Network (CNN); Support Vector Machine (SVM); Bayesian Optimisation,"Remote sensing image classification is difficult, especially for agricultural crops with identical phenological growth periods. In this context, multi-sensor image fusion allows a comprehensive representation of biophysical and structural information. Recently, Convolutional Neural Network (CNN)-based methods are used for several applications due to their spatial-spectral interpretability. Hence, this study explores the potential of fused multi-temporal Sentinel 1 (S1) and Sentinel 2 (S2) images for Land Use/Land Cover classification over an agricultural area in India. For classification, Bayesian optimised 2D CNN-based DL and pixel-based SVM classifiers were used. For fusion, a CNN-based siamese network with Ratio-of-Laplacian pyramid method was used for the images acquired over the entire winter cropping period. This fusion strategy leads to better interpretability of results and also found that 2D CNN-based DL classifier performed well in terms of classification accuracy for both single-month (95.14% and 96.11%) as well as multi-temporal (99.87% and 99.91%) fusion in comparison to the SVM with classification accuracy for single-month (80.02% and 81.36%) and multi-temporal fusion (95.69% and 95.84%). Results indicate better performance by Vertical-Vertical polarised fused images than Vertical-Horizontal polarised fused images. Thus, implying the need to analyse classified images obtained by DL classifiers along with the classification accuracy.",Remote Sensing,no,Sentinel-2,Land-use and land-cover,Vegetation,Classification,yes,,,,,,,,,,,,,,,
J,Z. Zhu; Y. Tao; X. Luo,HCNNet: A Hybrid Convolutional Neural Network for Spatiotemporal Image Fusion,IEEE Transactions on Geoscience and Remote Sensing,2022,Feature fusion;hybrid convolution (Conv);spatiotemporal fusion (STF);spectral correlation,"In recent years, leaps and bounds have developed spatiotemporal fusion (STF) methods for remote sensing (RS) images based on deep learning. However, most existing methods use 2-D convolution (Conv) to explore features. 3-D Conv can explore time-dimensional features, but it requires more memory footprint and is rarely used. In addition, the current STF methods based on convolutional neural networks (CNNs) are mainly the following two: 1) use 2-D Conv to extract features from multiple bands of the input image together and fuse the features to predict the multiband image directly and 2) use 2-D Conv to extract features from individual bands of the image, predict the reflectance data of individual bands, and finally stack the predicted individual bands directly to synthesize the multiband image. The former method does not sufficiently consider the spectral and reflectance differences between different bands, and the latter does not consider the similarity of spatial structures between adjacent bands and the spectral correlation. To solve these problems, we propose a 2-D/3-D hybrid CNN called HCNNet, in which the 2D-CNN branch extracts the spatial information features of single-band image, and the 3D-CNN branch extracts spatiotemporal features of single-band images. After fusing the features of the dual branches, we introduce neighboring band features to share spatial information so that the information is complementary to obtain single-band features and images, and finally stack each single-band image to generate multiband images. Visual assessment and metric evaluation of the three publicly available datasets showed that our method predicted better images compared with the five methods.",,yes,"Landsat-8, MODIS",Spatiotemporal Image Fusion,Others,Regression,yes,,,,,,,,,,,,0.9906,0.0258,,Reflectance
C,"Chelali, M; Kurtz, C; Puissant, A; Vincent, N",Image Time Series Classification based on a Planar Spatio-temporal Data Representation,"VISAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4: VISAPP",2020,Satellite Image Time Series; Spatio-temporal Features; Space-filling Curves; Convolutional Neural Networks,"Image time series such as MRI functional sequences or Satellite Image Time Series (SITS) provide valuable information for the automatic analysis of complex patterns through time. A major issue when analyzing such data is to consider at the same time their temporal and spatial dimensions. In this article we present a novel data representation that makes image times series compatible with classical deep learning model, such as Convolutional Neural Networks (CNN). The proposed approach is based on a novel planar representation of image time series that converts 2D + t data as 2D images without loosing too much spatial or temporal information. Doing so, CNN can learn at the same time the parameters of 2D filters involving temporal and spatial knowledge. Preliminary results in the remote sensing domain highlight the ability of our approach to discriminate complex agricultural land-cover classes from a SITS.",Computer Science,yes,Sentinel-2,Agricultural crops classification,Agriculture,Classification,yes,85.37,,,,,,,,,,,,,,
C,G. S. Phartiyal; L. S. Khangarot; D. Singh,Impact of Permuted Spectral Neighborhood of High-Dimensional Msts Rs Data on Crop Classification Performance with DNN Models,IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium,2023,localized spectral information;CNNs;multi-sensor;crop classification;time-series,"It is still a challenge for existing DNN based models to synergistically exploit the spatial, temporal, and especially spectral information of a crop present in multi-sensor time series (MSTS) remote sensing (RS) images and provide accurate crop classification while keeping the generalization ability of DNN models high. This imbalance requires investigation and demands novel CNN and RNN model-based approaches that can address the issue. The novel models proposed in this study involve the concepts of permuted localized spectral convolutions, localized spatial convolutions, and bi-directional recurrent units. The permuted spectral band stacking strategy is explored in this study to strengthen the influence of the spectral information. Overall, 6 models are proposed namely; Perm-1D-CNN, Perm-3D-CNN, Perm-RNN, Perm-1D-CRNN, Perm-2D-CRNN, and Perm-3D-CRNN. The qualitative and quantitative assessments reflect the higher generalization ability of the Perm-3D-CRNN along with its high classification accuracy. Also, the impact of spectral band permutations and localized spectral convolutions on the performance of DNN models is significant toward improved generalization.",Time series analysis;Stacking;Crops;Bidirectional control;Data models;Remote sensing,yes,Sentinel-2,Crop classification,Agriculture,Classification,no,94.2,,,,,,,,,,,,,,
J,"Yang, ZW; Zhang, HB; Lyu, XX; Du, WB","Improving Typical Urban Land-Use Classification with Active-Passive Remote Sensing and Multi-Attention Modules Hybrid Network: A Case Study of Qibin District, Henan, China",SUSTAINABILITY,2022,land-use classification; convolutional neural networks (CNN); active and passive remote sensing; data fusion,"The study of high-precision land-use classification is essential for the sustainable development of land resources. This study addresses the problem of classification errors in optical remote-sensing images under high surface humidity, cloud cover, and hazy weather. The synthetic aperture radar (SAR) images are sensitive to soil moisture, and the microwave can penetrate clouds, haze, and smoke. By using both the active and passive remote-sensing data, the Sentinel-1A SAR and Sentinel-2B multispectral (MS) images are combined synergistically. The full-band data combining the SAR + MS + spectral indexes is thus constructed. Based on the high dimensionality and heterogeneity of this data set, a new framework (MAM-HybridNet) based on two-dimensional (2D) and three-dimensional (3D) hybrid convolutional neural networks combined with multi-attention modules (MAMs) is proposed for improving the accuracy of land-use classification in cities with high surface humidity. In addition, the same training samples supported by All bands data (SAR + MS + spectral index) are selected and compared with k-Nearest Neighbors (KNN), support vector machine (SVM), 2D convolutional neural networks, 3D convolutional neural networks, and hybridSN classification models to verify the accuracy of the proposed classification model. The results show that (1) fusion classification based on Sentinel-2B MSI and Sentinel-1A SAR data produce an overall accuracy (OA) of 95.10%, a kappa coefficient (KC) of 0.93, and an average accuracy (AA) of 92.86%, which is better than the classification results using Sentinel-2B MSI and Sentinel-1A SAR images separately. (2) The classification accuracy improves upon adding the spectral index, and the OA, KC, and AA improve by 3.77%, 0.05, and 5.5%, respectively. (3) With the support of full-band data, the algorithm proposed herein produces better results than other classification algorithms, with an OA of 98.87%, a KC of 0.98, and an AA of 98.36%. These results indicate that the synergistic effect of active-passive remote-sensing data improves land-use classification. Additionally, the results verify the effectiveness of the proposed deep-learning classification model for land-use classification.",Science & Technology - Other Topics; Environmental Sciences & Ecology,yes,Sentinel-2,Urban land-use classification,Urban,Classification,no,94.18,,,,,,,92.32,,,,,,,
J,"Luo, X; Tong, XH; Hu, ZW; Wu, GF",Improving Urban Land Cover/Use Mapping by Integrating A Hybrid Convolutional Neural Network and An Automatic Training Sample Expanding Strategy,REMOTE SENSING,2020,remote sensing; land cover classification; spectral feature; context feature; convolutional neural networks,"Moderate spatial resolution (MSR) satellite images, which hold a trade-off among radiometric, spectral, spatial and temporal characteristics, are extremely popular data for acquiring land cover information. However, the low accuracy of existing classification methods for MSR images is still a fundamental issue restricting their capability in urban land cover mapping. In this study, we proposed a hybrid convolutional neural network (H-ConvNet) for improving urban land cover mapping with MSR Sentinel-2 images. The H-ConvNet was structured with two streams: one lightweight 1D ConvNet for deep spectral feature extraction and one lightweight 2D ConvNet for deep context feature extraction. To obtain a well-trained 2D ConvNet, a training sample expansion strategy was introduced to assist context feature learning. The H-ConvNet was tested in six highly heterogeneous urban regions around the world, and it was compared with support vector machine (SVM), object-based image analysis (OBIA), Markov random field model (MRF) and a newly proposed patch-based ConvNet system. The results showed that the H-ConvNet performed best. We hope that the proposed H-ConvNet would benefit for the land cover mapping with MSR images in highly heterogeneous urban regions.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Urban land cover/use mapping,Urban,Classification,no,77.86,,,,,,,,,,,,,,
C,"Voelsen, M; Teimouri, M; Rottensteiner, F; Heipke, C",INVESTIGATING 2D AND 3D CONVOLUTIONS FOR MULTITEMPORAL LAND COVER CLASSIFICATION USING REMOTE SENSING IMAGES,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",2022,land cover classification; remote sensing; FCN; multi-temporal images; 3D-CNN,"With the availability of large amounts of satellite image time series (SITS), the identification of different materials of the Earth's surface is possible with a high temporal resolution. One of the basic tasks is the pixel-wise classification of land cover, i.e. the task of identifying the physical material of the Earth's surface in an image. Fully convolutional neural networks (FCN) are successfully used for this task. In this paper, we investigate different FCN variants, using different methods for the computation of spatial, spectral, and temporal features. We investigate the impact of 3D convolutions in the spatial-temporal as well as in the spatial-spectral dimensions in comparison to 2D convolutions in the spatial dimensions only. Additionally, we introduce a new method to generate multitemporal input patches by using time intervals instead of fixed acquisition dates. We then choose the image that is closest in time to the middle of the corresponding time interval, which makes our approach more flexible with respect to the requirements for the acquisition of new data. Using these multi-temporal input patches, generated from Sentinel-2 images, we improve the classification of land cover by 4% in the mean F1-score and 1.3% in the overall accuracy compared to a classification using mono-temporal input patches. Furthermore, the usage of 3D convolutions instead of 2D convolutions improves the classification performance by a small amount of 0.4% in the mean F1-score and 1.2% in the overall accuracy.",Physical Geography; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Land cover classification,Urban,Classification,yes,87.7,79.9,,,,,,,,,,,,,
J,"Sabo, Filip; Meroni, Michele; Waldner, Francois; Rembold, Felix",Is deeper always better? Evaluating deep learning models for yield forecasting with small data,ENVIRONMENTAL MONITORING AND ASSESSMENT,2023,Convolutional neural networks; Agriculture; Remote sensing; Food security,"Predicting crop yields, and especially anomalously low yields, is of special importance for food insecure countries. In this study, we investigate a flexible deep learning approach to forecast crop yield at the provincial administrative level based on deep 1D and 2D convolutional neural networks using limited data. This approach meets the operational requirements-public and global records of satellite data in an application ready format with near real time updates-and can be transferred to any country with reliable yield statistics. Three-dimensional histograms of normalized difference vegetation index (NDVI) and climate data are used as input to the 2D model, while simple administrative-level time series averages of NDVI and climate data to the 1D model. The best model architecture is automatically identified during efficient and extensive hyperparameter optimization. To demonstrate the relevance of this approach, we hindcast (2002-2018) the yields of Algeria's three main crops (barley, durum and soft wheat) and contrast the model's performance with machine learning algorithms and conventional benchmark models used in a previous study. Simple benchmarks such as peak NDVI remained challenging to outperform while machine learning models were superior to deep learning models for all forecasting months and all tested crops. We attribute the poor performance of deep learning to the small size of the dataset available.",Environmental Sciences & Ecology,yes,MODIS,Crop yield prediction,Agriculture,Regression,no,,,,,,,,,,,,,,,
J,"Li, R; Zheng, SY; Duan, CX; Wang, LB; Zhang, C",Land cover classification from remote sensing images based on multi-scale fully convolutional network,GEO-SPATIAL INFORMATION SCIENCE,2022,Spatio-temporal remote sensing images; Multi-Scale Fully Convolutional Network; land cover classification,"Although the Convolutional Neural Network (CNN) has shown great potential for land cover classification, the frequently used single-scale convolution kernel limits the scope of information extraction. Therefore, we propose a Multi-Scale Fully Convolutional Network (MSFCN) with a multi-scale convolutional kernel as well as a Channel Attention Block (CAB) and a Global Pooling Module (GPM) in this paper to exploit discriminative representations from two-dimensional (2D) satellite images. Meanwhile, to explore the ability of the proposed MSFCN for spatio-temporal images, we expand our MSFCN to three-dimension using three-dimensional (3D) CNN, capable of harnessing each land cover category's time series interaction from the reshaped spatio-temporal remote sensing images. To verify the effectiveness of the proposed MSFCN, we conduct experiments on two spatial datasets and two spatiotemporal datasets. The proposed MSFCN achieves 60.366% on the WHDLD dataset and 75.127% on the GID dataset in terms of mIoU index while the figures for two spatiotemporal datasets are 87.753% and 77.156%. Extensive comparative experiments and ablation studies demonstrate the effectiveness of the proposed MSFCN. Code will be available at https://github.com/lironui/MSFCN.",Remote Sensing,yes,Gaofen-2,Land cover classification,Vegetation,Segmentation,yes,94.13,82.78,,,,,,88.86,,73,,,,,
J,"Fan, Xiangsuo; Chen, Lin; Xu, Xinggui; Yan, Chuan; Fan, Jinlong; Li, Xuyang",Land Cover Classification of Remote Sensing Images Based on Hierarchical Convolutional Recurrent Neural Network,FORESTS,2023,pixel classification; CNN; RNN; RS image classification,"Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have gained improved results in remote sensing image data classification. Multispectral image classification can benefit from the rich spectral information extracted by these models for land cover classification. This paper proposes a classification model called a hierarchical convolutional recurrent neural network (HCRNN) to combine the CNN and RNN modules for pixel-level classification of multispectral remote sensing images. In the HCRNN model, the original 13-band information from Sentinel-2 is transformed into a 1D multispectral sequence using a fully connected layer. It is then reshaped into a 3D multispectral feature matrix. The 2D-CNN features are extracted and used as inputs to the corresponding hierarchical RNN. The feature information at each level is adapted to the same convolution size. This network structure fully leverages the advantages of CNNs and RNNs to extract temporal and spatial features from the spectral data, leading to high-precision pixel-level multispectral remote sensing image classification. The experimental results demonstrate that the overall accuracy of the HCRNN model on the Sentinel-2 dataset reaches 97.62%, which improves the performance by 1.78% compared to the RNN model. Furthermore, this study focused on the changes in forest cover in the study area of Laibin City, Guangxi Zhuang Autonomous Region, which was 7997.1016 km2, 8990.4149 km2, and 8103.0020 km2 in 2017, 2019, and 2021, respectively, with an overall trend of a small increase in the area covered.",Forestry,yes,Sentinel-2,Land cover classification,Urban,Classification,yes,97.62,,,,,,,96.81,,,,,,,
J,Patel T.; Jones M.W.; Redfern T.,Manifold Explorer: Satellite Image Labelling and Clustering Tool with Using Deep Convolutional Autoencoders,Algorithms - MDPI,2023,dimension reduction; labelling samples; manifold exploration; remote sensing data,"We present a novel approach to providing greater insight into the characteristics of an unlabelled dataset, increasing the efficiency with which labelled datasets can be created. We leverage dimension-reduction techniques in combination with autoencoders to create an efficient feature representation for image tiles derived from remote sensing satellite imagery. The proposed methodology consists of two main stages. Firstly, an autoencoder network is utilised to reduce the high-dimensional image tile data into a compact and expressive latentfeature representation. Subsequently, features are further reduced to a two-dimensional embedding space using the manifold learning algorithm Uniform Manifold Approximation and Projection (UMAP) and t-distributed Stochastic Neighbour Embedding (t-SNE). This step enables the visualization of the image tile clusters in a 2D plot, providing an intuitive and interactive representation that can be used to aid rapid and geographically distributed image labelling. To facilitate the labelling process, our approach allows users to interact with the 2D visualization and label clusters based on their domain knowledge. In cases where certain classes are not effectively separated, users can re-apply dimension reduction to interactively refine subsets of clusters and achieve better class separation, enabling a comprehensively labelled dataset. We evaluate the proposed approach on real-world remote sensing satellite image datasets and demonstrate its effectiveness in achieving accurate and efficient image tile clustering and labelling. Users actively participate in the labelling process through our interactive approach, leading to enhanced relevance of the labelled data, by allowing domain experts to contribute their expertise and enrich the dataset for improved downstream analysis and applications. © 2023 by the authors.",Computer Science,yes,Sentinel-2,Clustering and labelling,Others,Classification,yes,,,,,,,,,,,,,,,
J,"Zhang, WC; Liu, HB; Wu, W; Zhan, LQ; Wei, J",Mapping Rice Paddy Based on Machine Learning with Sentinel-2 Multi-Temporal Data: Model Comparison and Transferability,REMOTE SENSING,2020,rice; convolutional neural network; F1 score; sentinel-2; transfer,"Rice is an important agricultural crop in the Southwest Hilly Area, China, but there has been a lack of efficient and accurate monitoring methods in the region. Recently, convolutional neural networks (CNNs) have obtained considerable achievements in the remote sensing community. However, it has not been widely used in mapping a rice paddy, and most studies lack the comparison of classification effectiveness and efficiency between CNNs and other classic machine learning models and their transferability. This study aims to develop various machine learning classification models with remote sensing data for comparing the local accuracy of classifiers and evaluating the transferability of pretrained classifiers. Therefore, two types of experiments were designed: local classification experiments and model transferability experiments. These experiments were conducted using cloud-free Sentinel-2 multi-temporal data in Banan District and Zhongxian County, typical hilly areas of Southwestern China. A pure pixel extraction algorithm was designed based on land-use vector data and a Google Earth Online image. Four convolutional neural network (CNN) algorithms (one-dimensional (Conv-1D), two-dimensional (Conv-2D) and three-dimensional (Conv-3D_1 and Conv-3D_2) convolutional neural networks) were developed and compared with four widely used classifiers (random forest (RF), extreme gradient boosting (XGBoost), support vector machine (SVM) and multilayer perceptron (MLP)). Recall, precision, overall accuracy (OA) and F1 score were applied to evaluate classification accuracy. The results showed that Conv-2D performed best in local classification experiments with OA of 93.14% and F1 score of 0.8552 in Banan District, OA of 92.53% and F1 score of 0.8399 in Zhongxian County. CNN-based models except Conv-1D provided more desirable performance than non-CNN classifiers. Besides, among the non-CNN classifiers, XGBoost received the best result with OA of 89.73% and F1 score of 0.7742 in Banan District, SVM received the best result with OA of 88.57% and F1 score of 0.7538 in Zhongxian County. In model transferability experiments, almost all CNN classifiers had low transferability. RF and XGBoost models have achieved acceptable F1 scores for transfer (RF = 0.6673 and 0.6469, XGBoost = 0.7171 and 0.6709, respectively).",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Mapping Rice Paddy,Vegetation,Classification,no,93.14,85.52,88.61,82.64,,,,,,,,,,,
J,"Debella-Gilo, M; Gjertsen, AK",Mapping Seasonal Agricultural Land Use Types Using Deep Learning on Sentinel-2 Image Time Series,REMOTE SENSING,2021,multilayer perceptron; CNN; hyperparameter tuning; cereal; grass,"The size and location of agricultural fields that are in active use and the type of use during the growing season are among the vital information that is needed for the careful planning and forecasting of agricultural production at national and regional scales. In areas where such data are not readily available, an independent seasonal monitoring method is needed. Remote sensing is a widely used tool to map land use types, although there are some limitations that can partly be circumvented by using, among others, multiple observations, careful feature selection and appropriate analysis methods. Here, we used Sentinel-2 satellite image time series (SITS) over the land area of Norway to map three agricultural land use classes: cereal crops, fodder crops (grass) and unused areas. The Multilayer Perceptron (MLP) and two variants of the Convolutional Neural Network (CNN), are implemented on SITS data of four different temporal resolutions. These enabled us to compare twelve model-dataset combinations to identify the model-dataset combination that results in the most accurate predictions. The CNN is implemented in the spectral and temporal dimensions instead of the conventional spatial dimension. Rather than using existing deep learning architectures, an autotuning procedure is implemented so that the model hyperparameters are empirically optimized during the training. The results obtained on held-out test data show that up to 94 % overall accuracy and 90% Cohen's Kappa can be obtained when the 2D CNN is applied on the SITS data with a temporal resolution of 7 days. This is closely followed by the 1D CNN on the same dataset. However, the latter performs better than the former in predicting data outside the training set. It is further observed that cereal is predicted with the highest accuracy, followed by grass. Predicting the unused areas has been found to be difficult as there is no distinct surface condition that is common for all unused areas.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Mapping Seasonal Agricultural Land Use Types,Agriculture,Classification,no,94,93.67,94,93.33,,,,90,,,,,,,
J,"Zhang, E; Fu, YH; Wang, J; Liu, L; Yu, K; Peng, JY",MSAC-Net: 3D Multi-Scale Attention Convolutional Network for Multi-Spectral Imagery Pansharpening,REMOTE SENSING,2022,deep learning; multi-spectral image; 3D convolutional; multi-scale cost,"Pansharpening fuses spectral information from the multi-spectral image and spatial information from the panchromatic image, generating super-resolution multi-spectral images with high spatial resolution. In this paper, we proposed a novel 3D multi-scale attention convolutional network (MSAC-Net) based on the typical U-Net framework for multi-spectral imagery pansharpening. MSAC-Net is designed via 3D convolution, and the attention mechanism replaces the skip connection between the contraction and expansion pathways. Multiple pansharpening layers at the expansion pathway are designed to calculate the reconstruction results for preserving multi-scale spatial information. The MSAC-Net performance is verified on the IKONOS and QuickBird satellites' datasets, proving that MSAC-Net achieves comparable or superior performance to the state-of-the-art methods. Additionally, 2D and 3D convolution are compared, and the influences of the number of convolutions in the convolution block, the weight of multi-scale information, and the network's depth on the network performance are analyzed.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,QuickBird,MS imagery pansharpening,Others,Regression,no,,,,,,,,,,,,0.78,1.059,,Similarity of spectral features
C,Y. Xie; J. Tian,Multimodal Co-learning: A Domain Adaptation Method for Building Extraction from Optical Remote Sensing Imagery,2023 Joint Urban Remote Sensing Event (JURSE),2023,building extraction;multimodal data;co-learning;domain adaptation;transfer learning,"In this paper, we aim to improve the transfer learning ability of 2D convolutional neural networks (CNNs) for building extraction from optical imagery and digital surface models (DSMs) using a 2D-3D co-learning framework. Unlabeled target domain data are incorporated as unlabeled training data pairs to optimize the training procedure. Our framework adaptively transfers unsupervised mutual information between the 2D and 3D modality (i.e., DSM-derived point clouds) during the training phase via a soft connection, utilizing a predefined loss function. Experimental results from a spaceborne-to-airborne cross-domain case demonstrate that the framework we present can quantitatively and qualitatively improve the testing results for building extraction from single-modality optical images.",Point cloud compression;Training;Three-dimensional displays;Buildings;Transfer learning;Training data;Optical imaging,yes,WorldView-2,Building extraction,Urban,Classification,yes,,79.47,,,,,,,65.93,,,,,,
J,S. M. M. Nejad; D. Abbasi-Moghadam; A. Sharifi; N. Farmonov; K. Amankulova; M. Lászlź,Multispectral Crop Yield Prediction Using 3D-Convolutional Neural Networks and Attention Convolutional LSTM Approaches,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2023,3D-CNN;ConvLSTM;forecasting;LSTM attention;skip connection,"In recent years, national economies are highly affected by crop yield predictions. By early prediction, the market price can be predicted, importing, and exporting plan can be provided, social, and economic effects of waste products can be minimized, and a program can be presented for humanitarian food aid. In addition, agricultural fields are constantly growing to generate products required. The use of machine learning (ML) methods in this sector can lead to the efficient production and high-quality agricultural products. Traditional predictive machine models were unable to find nonlinear relationships between data. Recently, there has been a revolution in prediction systems via the advancement of ML, which can be used to achieve highly accurate decision-making networks. Thus far, many strategies have been used to evaluate agricultural products, such as DeepYield, CNN-LSTM, and ConvLSTM. However, preferable prediction accuracy is required. In this study, two architectures have been proposed. The first model includes 2D-CNN, skip connections, and LSTM-Attentions. The second model comprises 3D-CNN, skip connections, and ConvLSTM Attention. The Input data given from MODIS products such as Land-Cover, Surface-Temperature, and MODIS-Land-surface from 2003 to 2018 on the county level over 1800 counties, where soybean is mainly cultivated in the USA. The proposed methods have been compared with the most recent models. Then, the results showed that the second proposed method notably outperformed the other techniques. In case of MAE, the second proposed method, DeepYield, ConvLSTM, 3DCNN, and CNN-LSTM obtained 4.3, 6.003, 6.05, 6.3, and 7.002, respectively.",,yes,MODIS,Crop yield prediction,Agriculture,Regression,yes,,,,,,,,,,,,0.73,6.25,5.09,Prediction performances of crop yield forecasting at the county level
J,"Zhong, J; Sun, J; Lai, ZL; Song, Y",Nearshore Bathymetry from ICESat-2 LiDAR and Sentinel-2 Imagery Datasets Using Deep Learning Approach,REMOTE SENSING,2022,nearshore bathymetry; ICESat-2; Sentinel-2; 2D CNN; deep learning,"Accurate bathymetric data is crucial for marine and coastal ecosystems. A lot of studies have been carried out for nearshore bathymetry using satellite data. The approach adopted extensively in shallow water depths estimation has recently been one of empirical models. However, the linear empirical model is simple and only takes limited band information at each bathymetric point into consideration. It may be not suitable for complex environments. In this paper, a deep learning framework was proposed for nearshore bathymetry (DL-NB) from ICESat-2 LiDAR and Sentinel-2 Imagery datasets. The bathymetric points from the spaceborne ICESat-2 LiDAR were extracted instead of in situ measurements. By virtue of the two-dimensional convolutional neural network (2D CNN), DL-NB can make full use of the initial multi-spectral information of Sentinel-2 at each bathymetric point and its adjacent areas during the training. Based on the trained model, the bathymetric maps of several study areas were produced including the Appalachian Bay (AB), Virgin Islands (VI), and Cat Island (CI) of the United States. The performance of DL-NB was evaluated by empirical method, machine learning method and multilayer perceptron (MLP). The results indicate that the accuracy of the DL-NB is better than comparative methods can in nearshore bathymetry. After quantitative analysis, the RMSE of DL-NB could achieve 1.01 m, 1.80 m and 0.28 m in AB, VI and CI respectively. Given the same data conditions, the proposed method can be applied for high precise global scale and multitemporal nearshore bathymetric maps generation, which are beneficial to marine environmental change assessment and conservation.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Bathymetry,Water,Regression,no,,,,,,,,,,,,0.9,1.03,,Bathymetry (m)
C,M. Pal; Akshay; H. Rohilla; B. C. Teja,"Patch Based Land Cover Classification: A Comparison of Deep Learning, SVM and NN Classifiers",IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium,2020,2D-CNN;SVM;neural network;patch size;classification accuracy,"Pixel based algorithms including back propagation neural networks (NN) and support vector machines (SVM) have been widely used for remotely sensed image classifications. Within last few years, deep learning based image classifier like convolution neural networks (2-CNN) are becoming popular alternatives to these classifiers. In this paper, we compare performance of patch based SVM and NN with that of a deep learning algorithms comprising of 2-D CNN and fully connected layers. Similar to CNN which utilise image patches to derive features for further classification, we propose to use patches as an input in place of individual pixel with both SVM and NN classifiers. Two datasets, one multispectral and other hyperspectral data was used to compare the performance of different classifiers. Results with both datasets suggest the effectiveness of patch based SVM and NN classifiers in comparison to state of art 2D-CNN classifier.",,yes,Landsat-7,Land cover classification,Vegetation,Classification,no,99.21,,,,,,,,,,,,,,
J,"Wang, CY; Zhang, YJ; Wu, XF; Yang, W; Qiang, HY; Lu, BB; Wang, JL",R-IMNet: Spatial-Temporal Evolution Analysis of Resource-Exhausted Urban Land Based on Residual-Intelligent Module Network,REMOTE SENSING,2022,remote sensing image; convolutional neural network; land use; driving force; resource depletion,"The transformation of resource-exhausted urban land is an urgent problem for sustainable urban development in the world today. Obtaining the urban land use type and analyzing the changes in their land use can lead to better management of the relationship between economic development and resource utilization. In this paper, a residual-intelligent module network was proposed to solve the problems of low classification accuracy and missing objects edge information in traditional computer classification methods. The classification of four Landsat-TM/OLI images from 1993-2020 for Jiaozuo city (the first batch of resource-exhausted cities in China) was realized by this method. The results (overall accuracy was 98.61%, in 2020 images) were better than the comparison models (support vector machine, 2D-convolutional neural network, hybrid convolution networks; overall accuracy was 87.12%, 96.16%, 98.46%, respectively) and effectively reduced the loss of information on the edge of the ground objects. On this basis, six main land use types were constructed by combining field surveys and other methods. The characteristics and driving forces of spatial-temporal change in land use were explored from the aspect of social, economic and policy factors. The results showed that from 1993 to 2020 the cultivated land, forest land, water body and other land types in the study area decreased by 690.97 km(2), 57.54 km(2), 47.04 km(2) and 59.43 km(2), respectively. The construction land and bare land increased by 839.38 km(2) and 15.57 km(2), respectively. The transfer of land use types was mainly from cultivated land to construction land, with a cumulative conversion of 920.95 km(2) within 27 years. The driving forces of land use in the study area were analyzed by principal component analysis (PCA) and regression analysis. The spatial-temporal evolution of land use types was affected by policy changes, the level of social development and the adjustment in the economy, industry and agriculture structure. The investment in fixed assets and per capita net income in rural areas were the top two influencing factors and their cumulative contribution rate was 94.62%. The findings of this study can provide scientific reference and theoretical support for land use planning, land reclamation in mining areas, ecological protection and sustainable development in Jiaozuo and other resource-exhausted cities in the world.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"Landsat-5, Landsat-8",Urban land use,Urban,Classification,no,96.16,,,,,,,94,,,,,,,
J,"Wang, Chunyang; Yang, Kui; Yang, Wei; Qiang, Haiyang; Xue, Huiyuan; Lu, Bibo; Zhou, Peng",R-MFNet: Analysis of Urban Carbon Stock Change against the Background of Land-Use Change Based on a Residual Multi-Module Fusion Network,REMOTE SENSING,2023,residual connection; attention mechanism; carbon density; spatio-temporal change; deep learning,"Regional land-use change is the leading cause of ecosystem carbon stock change; it is essential to investigate the response of LUCC to carbon stock to achieve the strategic goal of double carbon in a region. This paper proposes a residual network algorithm, the Residual Multi-module Fusion Network (R-MFNet), to address the problems of blurred feature boundary information, low classification accuracy, and high noise, which are often encountered in traditional classification methods. The network algorithm uses an R-ASPP module to expand the receptive field of the feature map to extract sufficient and multi-scale target features; it uses the attention mechanism to assign weights to the multi-scale information of each channel and space. It can fully preserve the remote sensing image features extracted by the convolutional layer through the residual connection. Using this classification network method, the classification of three Landsat-TM/OLI images of Zhengzhou City (the capital of Henan Province) from 2001 to 2020 was realized (the years that the three images were taken are 2001, 2009, and 2020). Compared with SVM, 2D-CNN, and deep residual networks (ResNet), the overall accuracy of the test dataset is increased by 10.07%, 3.96%, and 1.33%, respectively. The classification achieved using this method is closer to the real land surface, and its accuracy is higher than that of the finished product data obtained using the traditional classification method, providing high-precision land-use classification data for the subsequent carbon storage estimation research. Based on the land-use classification data and the carbon density data corrected by meteorological data (temperature and precipitation data), the InVEST model is used to analyze the land-use change and its impact on carbon storage in the region. The results showed that, from 2001 to 2020, the carbon stock in the study area showed a downward trend, with a total decrease of 1.48 x 107 t. Over the course of this 19-year period, the farmland area in Zhengzhou decreased by 1101.72 km2, and the built land area increased sharply by 936.16 km2. The area of land transfer accounted for 29.26% of the total area of Zhengzhou City from 2001 to 2009, and 31.20% from 2009 to 2020. The conversion of farmland to built land is the primary type of land transfer and the most important reason for decreasing carbon stock. The research results can provide support, in the form of scientific data, for land-use management decisions and carbon storage function protections in Zhengzhou and other cities around the world undergoing rapid urbanization.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"Landsat-5, Landsat-8",Land use,Urban,Classification,no,95.23,,,,,,,93,,,,,,,
J,"Ghandorh, H; Boulila, W; Masood, S; Koubaa, A; Ahmed, F; Ahmad, J",Semantic Segmentation and Edge Detection-Approach to Road Detection in Very High Resolution Satellite Images,REMOTE SENSING,2022,deep learning; convolutional neural networks; 2D attention; satellite images; road segmentation; edge detection,"Road detection technology plays an essential role in a variety of applications, such as urban planning, map updating, traffic monitoring and automatic vehicle navigation. Recently, there has been much development in detecting roads in high-resolution (HR) satellite images based on semantic segmentation. However, the objects being segmented in such images are of small size, and not all the information in the images is equally important when making a decision. This paper proposes a novel approach to road detection based on semantic segmentation and edge detection. Our approach aims to combine these two techniques to improve road detection, and it produces sharp-pixel segmentation maps, using the segmented masks to generate road edges. In addition, some well-known architectures, such as SegNet, used multi-scale features without refinement; thus, using attention blocks in the encoder to predict fine segmentation masks resulted in finer edges. A combination of weighted cross-entropy loss and the focal Tversky loss as the loss function is also used to deal with the highly imbalanced dataset. We conducted various experiments on two datasets describing real-world datasets covering the three largest regions in Saudi Arabia and Massachusetts. The results demonstrated that the proposed method of encoding HR feature maps effectively predicts sharp segmentation masks to facilitate accurate edge detection, even against a harsh and complicated background.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Spot-7,Road Detection,Urban,Segmentation,yes,,,,,,,,,,79.51,88.65,,,,
J,"Saralioglu, E; Gungor, O",Semantic segmentation of land cover from high resolution multispectral satellite images by spectral-spatial convolutional neural network,GEOCARTO INTERNATIONAL,2022,Remote sensing; classification; deep learning; semantic segmentation,"Research to improve the accuracy of very high-resolution satellite image classification algorithms is still one of the hot topics in the field of remote sensing. Successful results of deep learning methods in areas such as image classification and object detection have led to the application of these methods to remote sensing problems. Recently, Convolutional Neural Networks (CNNs) are among the most common deep learning methods used in image classification, however, the use of CNN's in satellite image classification is relatively new. Due to the high computational complexity of 3D CNNs, which aim to extract both spatial and spectral information, 2D CNNs focussing on the extraction of spatial information are often preferred. High-resolution satellite images, however, contain crucial spectral information as well as spatial information. In this study, a 3D-2D CNN model using both spectral and spatial information was applied to extract more accurate land cover information from very high-resolution satellite images. The model was applied on a Worldview-2 satellite image including agricultural product areas such as tea, hazelnut groves and land use classes such as buildings and roads. The results of the CNN based model were also compared against those of the Support Vector Machine (SVM) and Random Forest (RF) algorithms. The post-classification accuracies were obtained using 800 control points generated by a web interface created for crowdsourcing purposes. The classification accuracy was 95.6% for the 3D-2D CNN model, 89.2% for the RF and 86.4% for the SVM.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,Worldview-2,Semantic segmentation of land cover,Vegetation,Segmentation,yes,95.6,,,,,,,,,,,,,,
J,"Adrian, J; Sagan, V; Maimaitijiang, M",Sentinel SAR-optical fusion for crop type mapping using deep learning and Google Earth Engine,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2021,3D U-Net; Denoising neural networks; Sentinel-1; Sentinel-2; Data fusion,"Accurate crop type mapping provides numerous benefits for a deeper understanding of food systems and yield prediction. Ever-increasing big data, easy access to high-resolution imagery, and cloud-based analytics platforms like Google Earth Engine have drastically improved the ability for scientists to advance data-driven agriculture with improved algorithms for crop type mapping using remote sensing, computer vision, and machine learning. Crop type mapping techniques mainly relied on standalone SAR and optical imagery, few studies investigated the potential of SAR-optical data fusion, coupled with virtual constellation, and 3-dimensional (3D) deep learning networks. To this extent, we use a deep learning approach that utilizes the denoised backscatter and texture information from multi-temporal Sentinel-1 SAR data and the spectral information from multi-temporal optical Sentinel-2 data for mapping ten different crop types, as well as water, soil and urban area. Multi-temporal Sentinel-1 data was fused with multi-temporal optical Sentinel-2 data in an effort to improve classification accuracies for crop types. We compared the results of the 3D U-Net to the state-of-the-art deep learning networks, including SegNet and 2D U-Net, as well as commonly used machine learning method such as Random Forest. The results showed (1) fusing multi-temporal SAR and optical data yields higher training overall accuracies (OA) (3D U-Net 0.992, 2D U-Net 0.943, SegNet 0.871) and testing OA (3D U-Net 0.941, 2D U-Net 0.847, SegNet 0.643) for crop type mapping compared to standalone multi-temporal SAR or optical data (2) optical data fused with denoised SAR data via a denoising convolution neural network (OA 0.912) performed better for crop type mapping compared to optical data fused with boxcar (OA 0.880), Lee (OA 0.881), and median (OA 0.887) filtered SAR data and (3) 3D convolutional neural networks perform better than 2D convolutional neural networks for crop type mapping (SAR OA 0.912, optical OA 0.937, fused OA 0.992).",Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology,no,Sentinel-2,Crop type mapping,Agriculture,Classification,yes,84.7,,,,,,,,,,,,,,
J,J. Fang; G. He; Z. Zhu; B. I. M. Attaher; J. Xue,Spatial–Spectral Decoupling Interaction Network for Multispectral Imagery Change Detection,IEEE Geoscience and Remote Sensing Letters,2022,Multispectral imagery change detection;spatial–spectral decoupling interaction network,"We present a spatial–spectral decoupling interaction network for multispectral imagery change detection, which can exploit the underlying information of the multispectral imagery adequately through simultaneously considering the discriminative attribute of each pixel and robust spatial structure of the corresponding patch. Specifically, a 1-D convolutional neural network (1D-CNN) is applied to the spectral vector of each pixel to extract its discriminative feature, while a 2D-CNN is applied to the patch centering on the corresponding pixel to explore the spatial structure information. In addition, an interaction mechanism is incorporated into the feature fusion module to enhance the spatial–spectral consistency.",,yes,"Gaofen-1, Landsat-7",Change detection,Others,Classification,yes,95.94,,,,,,,94.19,,,,,,,
J,"Wang, Zhiyuan; Fang, Shuai; Zhang, Jing",Spatiotemporal Fusion Model of Remote Sensing Images Combining Single-Band and Multi-Band Prediction,REMOTE SENSING,2023,spatiotemporal fusion; remote sensing; deep learning; ConvNeXt; convolutional neural network (CNN),"In recent years, convolutional neural network (CNN)-based spatiotemporal fusion (STF) models for remote sensing images have made significant progress. However, existing STF models may suffer from two main drawbacks. Firstly, multi-band prediction often generates a hybrid feature representation that includes information from all bands. This blending of features can lead to the loss or blurring of high-frequency details, making it challenging to reconstruct multi-spectral remote sensing images with significant spectral differences between bands. Another challenge in many STF models is the limited preservation of spectral information during 2D convolution operations. Combining all input channels' convolution results into a single-channel output feature map can lead to the degradation of spectral dimension information. To address these issues and to strike a balance between avoiding hybrid features and fully utilizing spectral information, we propose a remote sensing image STF model that combines single-band and multi-band prediction (SMSTFM). The SMSTFM initially performs single-band prediction, generating separate predicted images for each band, which are then stacked together to form a preliminary fused image. Subsequently, the multi-band prediction module leverages the spectral dimension information of the input images to further enhance the preliminary predictions. We employ the modern ConvNeXt convolutional module as the primary feature extraction component. During the multi-band prediction phase, we enhance the spatial and channel information captures by replacing the 2D convolutions within ConvNeXt with 3D convolutions. In the experimental section, we evaluate our proposed algorithm on two public datasets with 16x resolution differences and one dataset with a 3x resolution difference. The results demonstrate that our SMSTFM achieves state-of-the-art performance on these datasets and is proven effective and reasonable through ablation studies.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"MODIS, Sentinel-2, Landsat-8",Fusion model of remote sensing images,Others,Regression,yes,,,,,,,,,,,,,,,
J,"Wang, L; Wu, YX; Xu, JP; Zhang, HY; Wang, XY; Yu, JB; Sun, Q; Zhao, ZY",STATUS PREDICTION BY 3D FRACTAL NET CNN BASED ON REMOTE SENSING IMAGES,FRACTALS-COMPLEX GEOMETRY PATTERNS AND SCALING IN NATURE AND SOCIETY,2020,Fractal Net; 3D CNN; Status Prediction; Remote Sensing Images; Eutrophication,"The contradiction between the supply and demand of water resources is becoming increasingly prominent, whose main reason is the eutrophication of rivers and lakes. However, limited and inaccurate data makes it impossible to establish a precise model to successfully predict eutrophication levels. Moreover, it is incompetent to distinguish the degree of eutrophication status of lakes by manual calculation and processing. Focusing on these inconveniences, this study proposes 3D fractal net CNN to extract features in remote sensing images automatically, aiming at achieving scientific forecasting on eutrophication status of lakes. In order to certificate the effectiveness of the proposed method, we predict the state of the water body based on remote sensing images of natural lake. The images in natural lake were accessed by MODIS satellite, cloud-free chlorophyll inversion picture of 2009 was resized into 273 x 273 patches, which were collected as training and testing samples. In the total of 162 pictures, our study makes three consecutive pictures as a set of data so as to attain 120 group of training and 40 testing data. Taking one set of data as input of the neural network and the next day's eutrophication level as labels, CNNs act considerable efficiency. Through the experimental results of 2D CNN, 3D CNN and 3D fractal net CNN, 3D fractal net CNN has more outstanding performance than the other two, with the prediction accuracy of 67.5% better than 47.5% and 62.5%, respectively.",Mathematics; Science & Technology - Other Topics,yes,MODIS,Eutrophication status of lakes,Water,Classification,no,47.5,,,,,,,,,,,,,,
J,"Zhang, Kaixin; Yuan, Da; Yang, Huijin; Zhao, Jianhui; Li, Ning",Synergy of Sentinel-1 and Sentinel-2 Imagery for Crop Classification Based on DC-CNN,REMOTE SENSING,2023,convolutional neural network; synthetic aperture radar (SAR); multispectral imagery; feature fusion; crop classification; polarimetric decomposition,"Over the years, remote sensing technology has become an important means to obtain accurate agricultural production information, such as crop type distribution, due to its advantages of large coverage and a short observation period. Nowadays, the cooperative use of multi-source remote sensing imagery has become a new development trend in the field of crop classification. In this paper, the polarimetric components of Sentinel-1 (S-1) decomposed by a new model-based decomposition method adapted to dual-polarized SAR data were introduced into crop classification for the first time. Furthermore, a Dual-Channel Convolutional Neural Network (DC-CNN) with feature extraction, feature fusion, and encoder-decoder modules for crop classification based on S-1 and Sentinel-2 (S-2) was constructed. The two branches can learn from each other by sharing parameters so as to effectively integrate the features extracted from multi-source data and obtain a high-precision crop classification map. In the proposed method, firstly, the backscattering components (VV, VH) and polarimetric components (volume scattering, remaining scattering) were obtained from S-1, and the multispectral feature was extracted from S-2. Four candidate combinations of multi-source features were formed with the above features. Following that, the optimal one was found on a trial. Next, the characteristics of optimal combinations were input into the corresponding network branches. In the feature extraction module, the features with strong collaboration ability in multi-source data were learned by parameter sharing, and they were deeply fused in the feature fusion module and encoder-decoder module to obtain more accurate classification results. The experimental results showed that the polarimetric components, which increased the difference between crop categories and reduced the misclassification rate, played an important role in crop classification. Among the four candidate feature combinations, the combination of S-1 and S-2 features had a higher classification accuracy than using a single data source, and the classification accuracy was the highest when two polarimetric components were utilized simultaneously. On the basis of the optimal combination of features, the effectiveness of the proposed method was verified. The classification accuracy of DC-CNN reached 98.40%, with Kappa scoring 0.98 and Macro-F1 scoring 0.98, compared to 2D-CNN (OA reached 94.87%, Kappa scored 0.92, and Macro-F1 scored 0.95), FCN (OA reached 96.27%, Kappa scored 0.94, and Macro-F1 scored 0.96), and SegNet (OA reached 96.90%, Kappa scored 0.95, and Macro-F1 scored 0.97). The results of this study demonstrated that the proposed method had significant potential for crop classification.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,Sentinel-2,Crop classification,Agriculture,Classification,no,94.87,95,,,,,,92,,,,,,,
J,"Lee, J; Im, J; Cha, DH; Park, H; Sim, S",Tropical Cyclone Intensity Estimation Using Multi-Dimensional Convolutional Neural Networks from Geostationary Satellite Data,REMOTE SENSING,2020,tropical cyclones; multispectral imaging; 2D; 3D convolutional neural networks,"For a long time, researchers have tried to find a way to analyze tropical cyclone (TC) intensity in real-time. Since there is no standardized method for estimating TC intensity and the most widely used method is a manual algorithm using satellite-based cloud images, there is a bias that varies depending on the TC center and shape. In this study, we adopted convolutional neural networks (CNNs) which are part of a state-of-art approach that analyzes image patterns to estimate TC intensity by mimicking human cloud pattern recognition. Both two dimensional-CNN (2D-CNN) and three-dimensional-CNN (3D-CNN) were used to analyze the relationship between multi-spectral geostationary satellite images and TC intensity. Our best-optimized model produced a root mean squared error (RMSE) of 8.32 kts, resulting in better performance (35%) than the existing model using the CNN-based approach with a single channel image. Moreover, we analyzed the characteristics of multi-spectral satellite-based TC images according to intensity using a heat map, which is one of the visualization means of CNNs. It shows that the stronger the intensity of the TC, the greater the influence of the TC center in the lower atmosphere. This is consistent with the results from the existing TC initialization method with numerical simulations based on dynamical TC models. Our study suggests the possibility that a deep learning approach can be used to interpret the behavior characteristics of TCs.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,COMS,Tropical Cyclone Intensity,Geohazards,Regression,no,,,,,,,,,,,,0.94,8.32,6.09,Maximum sustained wind speed (kts)
J,"Papadomanolaki, M; Christodoulidis, S; Karantzalos, K; Vakalopoulou, M",Unsupervised Multistep Deformable Registration of Remote Sensing Imagery Based on Deep Learning,REMOTE SENSING,2021,spatial gradients; deformation; satellite; very high resolution imagery; urban and periurban; learning-based registration; dense displacements; alignment,"Image registration is among the most popular and important problems of remote sensing. In this paper we propose a fully unsupervised, deep learning based multistep deformable registration scheme for aligning pairs of satellite imagery. The presented method is based on the expression power of deep fully convolutional networks, regressing directly the spatial gradients of the deformation and employing a 2D transformer layer to efficiently warp one image to the other, in an end-to-end fashion. The displacements are calculated with an iterative way, utilizing different time steps to refine and regress them. Our formulation can be integrated into any kind of fully convolutional architecture, providing at the same time fast inference performances. The developed methodology has been evaluated in two different datasets depicting urban and periurban areas; i.e., the very high-resolution dataset of the East Prefecture of Attica, Greece, as well as the high resolution ISPRS Ikonos dataset. Quantitative and qualitative results demonstrated the high potentials of our method.",Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology,yes,"WorldView-2, IKONOS",Image registration,Others,Regression,yes,,,,,,,,,,,,,,,
J,"Ball, JGC; Petrova, K; Coomes, DA; Flaxman, S",Using deep convolutional neural networks to forecast spatial patterns of Amazonian deforestation,METHODS IN ECOLOGY AND EVOLUTION,2022,Amazon; artificial intelligence; convolutional neural networks; deep learning; deforestation forecasting; machine learning; spatial forecasting; tropical forests,"1.Tropical forests are subject to diverse deforestation pressures while their conservation is essential to achieve global climate goals. Predicting the location of deforestation is challenging due to the complexity of the natural and human systems involved but accurate and timely forecasts could enable effective planning and on-the-ground enforcement practices to curb deforestation rates. New computer vision technologies based on deep learning can be applied to the increasing volume of Earth observation data to generate novel insights and make predictions with unprecedented accuracy. 2. Here, we demonstrate the ability of deep convolutional neural networks (CNNs) to learn spatiotemporal patterns of deforestation from a limited set of freely available global data layers, including multispectral satellite imagery, the Hansen maps of annual forest change (2001-2020) and the ALOS PALSAR digital surface model, to forecast deforestation (2021). We designed four model architectures, based on 2D CNNs, 3D CNNs, and Convolutional Long Short-Term Memory (ConvLSTM) Recurrent Neural Networks (RNNs), to produce spatial maps that indicate the risk to each forested pixel (similar to 30 m) in the landscape of becoming deforested within the next year. They were trained and tested on data from two similar to 80,000 km(2) tropical forest regions in the Southern Peruvian Amazon. 3. The networks could predict the location of future forest loss to a high degree of accuracy (F-1 = 0.58-0.71). Our best performing model (3D CNN) had the highest pixel-wise accuracy (F-1 = 0.71) when validated on 2020 forest loss (2014-2019 training). Visual interpretation of the mapped forecasts indicated that the network could automatically discern the drivers of forest loss from the input data. For example, pixels around new access routes (e.g. roads) were assigned high risk, whereas this was not the case for recent, concentrated natural loss events (e.g. remote landslides). 4. Convolutional neural networks can harness limited time-series data to predict near-future deforestation patterns, an important step in harnessing the growing volume of satellite remote sensing data to curb global deforestation. The modelling framework can be readily applied to any tropical forest location and used by governments and conservation organisations to prevent deforestation and plan protected areas.",Environmental Sciences & Ecology,yes,Landsat-7,Deforestation,Geohazards,Classification,no,,70.375,70.375,70.385,,,,,,,,,,,
J,"Luo, Dong; Zhang, Hankui K.; Houborg, Rasmus; Ndekelu, Lina M. N.; Maimaitijiang, Maitiniyazi; Tran, Khuong H.; McMaine, John",Utility of daily 3 m Planet Fusion Surface Reflectance data for tillage practice mapping with deep learning,SCIENCE OF REMOTE SENSING,2023,Planet fusion; Tillage practice; Tillage date; Deep learning classification; Deep learning interpretation,"Tillage practices alter soil surface structure that can be potentially captured by satellite images with both high spatial and temporal resolution. This study explored tillage practice mapping using the daily Planet Fusion surface reflectance (PF-SR) gap-free 3 m data generated by fusing PlanetScope with Landsat-8, Sentinel-2 and MODIS surface reflectance data. The study area is a 220 x 220 km2 agricultural area in South Dakota, USA, and the study used 3285 PF-SR images from September 1, 2020 to August 31, 2021. The PF-SR images for the sur-veyed 433 fields were sliced into 10,747 training (70%) and evaluation (30%) non-overlapping time series patches. The training and evaluation patches were from different fields for evaluation data independence. The performance of four deep learning models (i.e., 2D convolutional neural networks (CNN), 3D CNN, CNN-Long short-term memory (LSTM), and attention CNN-LSTM) in tillage practice mapping, as well as their sensitivity to different spatial (i.e., 3 m, 24 m, and 96 m) and temporal resolutions (16-day, 8-day, 4-day, 2-day and 1-day) were examined. Classification accuracy continuously increased with increases in both temporal and spatial resolutions. The optimal models (3D CNN and attention CNN-LSTM) achieved-77% accuracy using 2-day or daily 3 m resolution data as opposed to-72% accuracy using 16-day 3 m resolution data or daily 24 m resolution data. This study also analyzed the feature importance of different acquisition dates for the two optimal models. The 3D CNN model feature importances were found to agree well with the tillage practice time. High feature importance was associated with observations during the fall and spring tillage period (i.e., fresh tillage signals) whereas the crop peak growing period (i.e., tillage signals weathered and confounded by dense canopy) was characterized by a relatively low feature importance. The work provides valuable insights into the utility of deep learning for tillage mapping and change event time identification based on high resolution imagery.",Environmental Sciences & Ecology; Remote Sensing; Imaging Science & Photographic Technology,yes,"Landsat-8, Sentinel-2, MODIS",Tillage practice mapping,Agriculture,Classification,no,71,,,,,,,,,,,,,,
J,"Zhou, XY; Zhou, WZ; Li, F; Shao, ZL; Fu, XL",Vegetation Type Classification Based on 3D Convolutional Neural Network Model: A Case Study of Baishuijiang National Nature Reserve,FORESTS,2022,vegetation types; classification; 3D convolutional neural network; Baishuijiang National Nature Reserve,"Efficient and accurate vegetation type extraction from remote sensing images can provide decision makers with basic forest cover and land use information, and provides a reliable basis for long-term monitoring. With the development of deep learning, the convolutional neural network (CNN) has been used successfully to classify tree species in many studies, but CNN models have rarely been applied in the classification of vegetation types on larger scales. To evaluate the performance of CNN models in the classification of vegetation types, this paper compared the classification accuracy of nine dominant land cover types in Baishuijiang National Nature Reserve with four models: 3D-CNN, 2D-CNN, JSSAN (joint spatial-spectral attention network) and Resnet18, using sentinel-2A data. Comparing the difference in classification accuracy between the direct use of raw sentinel images and fused feature indices sentinel images, the results showed that adding feature indices can improve the overall accuracy of the model. After fusing the characteristic bands, the accuracy of the four models was improved significantly, by 5.46-19.33%. The best performing 3D-CNN model achieved the highest classification accuracy with an overall accuracy of 95.82% and a kappa coefficient of 95.07%. In comparison, 2D-CNN achieved an overall accuracy of 79.07% and a kappa coefficient of 75.44%, JSSAN achieved an overall accuracy of 81.67% and a kappa coefficient of 78.56%, and Resnet18 achieved an overall accuracy of 93.61% and a kappa coefficient of 92.45%. The results showed that the 3D-CNN model can effectively capture vegetation type cover changes from broad-leaved forests at lower elevation, to shrublands and grasslands at higher elevation, across a range spanning 542-4007 m. In experiments using a small amount of sample data, 3D-CNN can better incorporate spatial-spectral information and is more effective in distinguishing the performance of spectrally similar vegetation types, providing an efficient and novel approach to classifying vegetation types in nature reserves with complex conditions.",Forestry,yes,Sentinel-2,Vegetation type classification,Vegetation,Classification,no,79.07,,,,,,,75.44,,,,,,,
